{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled data...\n",
      "elapsed time: 156.45 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME\n",
    "\n",
    "print \"loading pickled data...\"\n",
    "tic = time()\n",
    "with open(data_filename) as f:  \n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "f.close()\n",
    "with open(train_test_filename) as f:\n",
    "    train_data, val_data, test_data = pickle.load(f)\n",
    "f.close()\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 150, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 150, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 150, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 150, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "training...\n",
      "train data size:22853\n",
      "val data size:189\n",
      "test data size:186\n"
     ]
    }
   ],
   "source": [
    "#training parameters\n",
    "num_epochs = 2 #16\n",
    "batch_size = 32\n",
    "\n",
    "#model parameters\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 150  #TODO: tune\n",
    "kernel_sizes = range(2,6)\n",
    "learning_rate = 1e-3 \n",
    "weight_decay = 1e-5\n",
    "\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.tanh(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.avg_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\n",
    "print model\n",
    "\n",
    "#define loss and optimizer\n",
    "#criterion = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#scheduler = StepLR(optimizer, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "print \"training...\"\n",
    "print \"train data size:\" + str(len(train_data))\n",
    "print \"val data size:\" + str(len(val_data))\n",
    "print \"test data size:\" + str(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable params:  420600\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "cnn_num_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print \"number of trainable params: \", cnn_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(is_training_phase, num_epochs, data_to_load, idx_df, batch_size, number_negative_examples, model, criterion, optimizer, scheduler, model_name, use_gpu, save_model_at):\n",
    "    print \"Model invoked\"\n",
    "    loss_per_epoch = []\n",
    "    learning_rate_schedule = []\n",
    "    patience_cnt = 0\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data_to_load, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "    \n",
    "    if not is_training_phase:\n",
    "        num_epochs = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print \"epoch value: \" + str(epoch)\n",
    "        loss_over_batches = 0.0\n",
    "        \n",
    "        if is_training_phase:\n",
    "            model.train()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            query_idx = batch['query_idx']\n",
    "            query_title = Variable(batch['query_title'])\n",
    "            query_body = Variable(batch['query_body'])\n",
    "            similar_title = Variable(batch['similar_title'])\n",
    "            similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "            random_title_list = []\n",
    "            random_body_list = []\n",
    "            for ridx in range(number_negative_examples): #number of random negative examples\n",
    "                random_title_name = 'random_title_' + str(ridx)\n",
    "                random_body_name = 'random_body_' + str(ridx)\n",
    "                random_title_list.append(Variable(batch[random_title_name]))\n",
    "                random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "            if use_gpu:\n",
    "                query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "                similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "                random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "                random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "\n",
    "            if is_training_phase:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            nn_query_title = model(query_title)\n",
    "            nn_query_body = model(query_body)\n",
    "            nn_query = (nn_query_title + nn_query_body)/2.0\n",
    "\n",
    "            nn_similar_title = model(similar_title)\n",
    "            nn_similar_body = model(similar_body)\n",
    "            nn_similar = (nn_similar_title + nn_similar_body)/2.0\n",
    "\n",
    "            nn_random_list = []\n",
    "            for ridx in range(len(random_title_list)):\n",
    "                nn_random_title = model(random_title_list[ridx])\n",
    "                nn_random_body = model(random_body_list[ridx])\n",
    "                nn_random = (nn_random_title + nn_random_body)/2.0\n",
    "                nn_random_list.append(nn_random)\n",
    "            #end for\n",
    "\n",
    "            cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            score_pos = cosine_similarity(nn_query, nn_similar)\n",
    "        \n",
    "            score_list = []\n",
    "            score_list.append(score_pos)\n",
    "            for ridx in range(len(nn_random_list)):\n",
    "                score_neg = cosine_similarity(nn_query, nn_random_list[ridx])\n",
    "                score_list.append(score_neg)\n",
    "\n",
    "            X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "            print X_scores\n",
    "            y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "            if use_gpu:\n",
    "                y_targets = y_targets.cuda()\n",
    "            loss = criterion(X_scores, y_targets) #y_target=0\n",
    "            \n",
    "            if is_training_phase:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            loss_over_batches += loss.cpu().data[0]\n",
    "            \n",
    "            #save scores to data-frame\n",
    "            nn_query_idx = query_idx.numpy()\n",
    "            nn_retrieved_scores = X_scores.data.numpy()[:,1:] #skip positive score\n",
    "            for row, qidx in enumerate(nn_query_idx):\n",
    "                idx_df.loc[idx_df['query_id'] == qidx, model_name] = \" \".join(nn_retrieved_scores[row,:].astype('str'))\n",
    "    \n",
    "        #end for-batch\n",
    "        loss_per_epoch.append(loss_over_batches)\n",
    "    \n",
    "        if is_training_phase:\n",
    "            early_stop = False\n",
    "            learning_rate_schedule.append(scheduler.get_lr())\n",
    "            print \"epoch: %4d, training loss: %.4f\" %(epoch+1, loss_over_batches)\n",
    "        \n",
    "            torch.save(model, save_model_at)\n",
    "\n",
    "            #early stopping\n",
    "            patience = 4\n",
    "            min_delta = 0.1\n",
    "            if epoch > 0 and (loss_per_epoch[epoch-1] - loss_per_epoch[epoch] > min_delta):\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "\n",
    "            if patience_cnt > patience:\n",
    "                print \"early stopping...\"\n",
    "                early_stop = True\n",
    "        \n",
    "            if early_stop:\n",
    "                if is_training_phase:\n",
    "                    return loss_per_epoch, idx_df, learning_rate_schedule\n",
    "                else:\n",
    "                    return loss_per_epoch, idx_df, []\n",
    "  \n",
    "    #end for-epoch\n",
    "    if is_training_phase:\n",
    "        return loss_per_epoch, idx_df, learning_rate_schedule\n",
    "    else:\n",
    "        return loss_per_epoch, idx_df, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss, train_idx_df, learning_rate_schedule = model_cnn(True, num_epochs, train_data, train_idx_df, batch_size, 40,  model, criterion, optimizer, scheduler, 'CNN_train', use_gpu, (SAVE_PATH + SAVE_NAME))\n",
    "\n",
    "print \"testing...\"\n",
    "running_test_loss, test_idx_df, _ = model_cnn(False, num_epochs, test_data, test_idx_df, batch_size, 20,  model, criterion, optimizer, scheduler, 'CNN_test', use_gpu, (SAVE_PATH + SAVE_NAME + \"test\"))\n",
    "\n",
    "    \n",
    "print \"total test loss: \", running_test_loss\n",
    "print \"number of NaN: \\n\", test_idx_df.isnull().sum()\n",
    "#test_idx_df = test_idx_df.dropna() #NaNs are due to restriction: range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save scored data frame\n",
    "#test_idx_df.to_csv(SAVE_PATH + '/test_idx_df_scored_cnn.csv', header=True)\n",
    "\n",
    "print \"computing ranking metrics...\"\n",
    "cnn_mrr_test = compute_mrr(test_idx_df, score_name='CNN_test')\n",
    "print \"cnn MRR (test): \", np.mean(cnn_mrr_test)\n",
    "\n",
    "cnn_pr1_test = precision_at_k(test_idx_df, K=1, score_name='CNN_test')\n",
    "print \"cnn P@1 (test): \", np.mean(cnn_pr1_test)\n",
    "\n",
    "cnn_pr5_test = precision_at_k(test_idx_df, K=5, score_name='CNN_test')\n",
    "print \"cnn P@5 (test): \", np.mean(cnn_pr5_test)\n",
    "\n",
    "cnn_map_test = compute_map(test_idx_df, score_name='CNN_test')\n",
    "print \"cnn map (test): \", np.mean(cnn_map_test)\n",
    "\n",
    "\n",
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(training_loss, label='Adam')\n",
    "plt.title(\"CNN Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_training_loss1.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate_schedule, label='learning rate')\n",
    "plt.title(\"CNN learning rate schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_learning_rate_schedule1.png')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(validation_loss, label='Adam')\n",
    "plt.title(\"CNN Model Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_validation_loss.png')\n",
    "\"\"\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
