{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled data...\n",
      "elapsed time: 20.98 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME\n",
    "\n",
    "print \"loading pickled data...\"\n",
    "tic = time()\n",
    "with open(data_filename) as f:  \n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "f.close()\n",
    "with open(train_test_filename) as f:\n",
    "    train_data, val_data, test_data = pickle.load(f)\n",
    "f.close()\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 100, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "training...\n",
      "train data size:36\n",
      "val data size:20\n",
      "test data size:20\n"
     ]
    }
   ],
   "source": [
    "#training parameters\n",
    "num_epochs = 2 #16\n",
    "batch_size = 16 \n",
    "\n",
    "#model parameters\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  #TODO: tune\n",
    "kernel_sizes = range(2,6)\n",
    "learning_rate = 1e-3 \n",
    "weight_decay = 1e-5\n",
    "\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.requires_grad = False\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\n",
    "print model\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "print \"training...\"\n",
    "print \"train data size:\" + str(len(train_data))\n",
    "print \"val data size:\" + str(len(val_data))\n",
    "print \"test data size:\" + str(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(val_data[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(is_training_phase, num_epochs, data_to_load, idx_df, batch_size, number_negative_examples, model, criterion, optimizer, scheduler, model_name, use_gpu, save_model_at):\n",
    "    print \"Model invoked\"\n",
    "    loss_per_epoch = []\n",
    "    learning_rate_schedule = []\n",
    "    patience_cnt = 0\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data_to_load, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "    \n",
    "    if not is_training_phase:\n",
    "        num_epochs = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print \"epoch value: \" + str(epoch)\n",
    "        loss_over_batches = 0.0\n",
    "        \n",
    "        if is_training_phase:\n",
    "            model.train()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            query_idx = batch['query_idx']\n",
    "            query_title = Variable(batch['query_title'])\n",
    "            query_body = Variable(batch['query_body'])\n",
    "            similar_title = Variable(batch['similar_title'])\n",
    "            similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "            random_title_list = []\n",
    "            random_body_list = []\n",
    "            for ridx in range(number_negative_examples): #number of random negative examples\n",
    "                random_title_name = 'random_title_' + str(ridx)\n",
    "                random_body_name = 'random_body_' + str(ridx)\n",
    "                random_title_list.append(Variable(batch[random_title_name]))\n",
    "                random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "            if use_gpu:\n",
    "                query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "                similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "                random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "                random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "\n",
    "            if is_training_phase:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            nn_query_title = model(query_title)\n",
    "            nn_query_body = model(query_body)\n",
    "            nn_query = (nn_query_title + nn_query_body)/2.0\n",
    "\n",
    "            nn_similar_title = model(similar_title)\n",
    "            nn_similar_body = model(similar_body)\n",
    "            nn_similar = (nn_similar_title + nn_similar_body)/2.0\n",
    "\n",
    "            nn_random_list = []\n",
    "            for ridx in range(len(random_title_list)):\n",
    "                nn_random_title = model(random_title_list[ridx])\n",
    "                nn_random_body = model(random_body_list[ridx])\n",
    "                nn_random = (nn_random_title + nn_random_body)/2.0\n",
    "                nn_random_list.append(nn_random)\n",
    "            #end for\n",
    "\n",
    "            cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            score_pos = cosine_similarity(nn_query, nn_similar)\n",
    "        \n",
    "            score_list = []\n",
    "            score_list.append(score_pos)\n",
    "            for ridx in range(len(nn_random_list)):\n",
    "                score_neg = cosine_similarity(nn_query, nn_random_list[ridx])\n",
    "                score_list.append(score_neg)\n",
    "\n",
    "            X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "            print X_scores\n",
    "            y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "            if use_gpu:\n",
    "                y_targets = y_targets.cuda()\n",
    "            loss = criterion(X_scores, y_targets) #y_target=0\n",
    "            \n",
    "            if is_training_phase:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            loss_over_batches += loss.cpu().data[0]\n",
    "            \n",
    "            #save scores to data-frame\n",
    "            nn_query_idx = query_idx.numpy()\n",
    "            nn_retrieved_scores = X_scores.data.numpy()[:,1:] #skip positive score\n",
    "            for row, qidx in enumerate(nn_query_idx):\n",
    "                idx_df.loc[idx_df['query_id'] == qidx, model_name] = \" \".join(nn_retrieved_scores[row,:].astype('str'))\n",
    "    \n",
    "        #end for-batch\n",
    "        loss_per_epoch.append(loss_over_batches)\n",
    "    \n",
    "        if is_training_phase:\n",
    "            early_stop = False\n",
    "            learning_rate_schedule.append(scheduler.get_lr())\n",
    "            print \"epoch: %4d, training loss: %.4f\" %(epoch+1, loss_over_batches)\n",
    "        \n",
    "            torch.save(model, save_model_at)\n",
    "\n",
    "            #early stopping\n",
    "            patience = 4\n",
    "            min_delta = 0.1\n",
    "            if epoch > 0 and (loss_per_epoch[epoch-1] - loss_per_epoch[epoch] > min_delta):\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "\n",
    "            if patience_cnt > patience:\n",
    "                print \"early stopping...\"\n",
    "                early_stop = True\n",
    "        \n",
    "            if early_stop:\n",
    "                if is_training_phase:\n",
    "                    return loss_per_epoch, idx_df, learning_rate_schedule\n",
    "                else:\n",
    "                    return loss_per_epoch, idx_df, []\n",
    "  \n",
    "    #end for-epoch\n",
    "    if is_training_phase:\n",
    "        return loss_per_epoch, idx_df, learning_rate_schedule\n",
    "    else:\n",
    "        return loss_per_epoch, idx_df, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model invoked\n",
      "epoch value: 0\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9736  0.8707  0.8590  0.8882  0.9234  0.9430  0.9152  0.9042  0.8974  0.9013\n",
      " 0.9236  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9812  0.8867  0.8608  0.9017  0.9035  0.9089  0.8521  0.9018  0.8834  0.8799\n",
      " 0.8944  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9536  0.8367  0.8264  0.8265  0.8593  0.8467  0.8536  0.8599  0.8427  0.8782\n",
      " 0.9725  0.7845  0.7676  0.7618  0.7812  0.7302  0.7873  0.7473  0.7749  0.7859\n",
      " 0.9129  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9116  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9589  0.8364  0.7804  0.8199  0.8427  0.7776  0.7440  0.7879  0.8565  0.8299\n",
      " 0.9615  0.9004  0.9467  0.9258  0.9222  0.9023  0.9113  0.9361  0.9293  0.9416\n",
      " 0.9728  0.9020  0.9255  0.9418  0.9193  0.8957  0.8910  0.9235  0.9320  0.9125\n",
      " 0.9519  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9282  0.7630  0.7479  0.7354  0.7019  0.8110  0.8447  0.7390  0.6661  0.7679\n",
      " 0.9814  0.9187  0.9030  0.9154  0.9114  0.9155  0.9426  0.9310  0.8793  0.8951\n",
      " 0.9824  0.8642  0.8668  0.7926  0.8816  0.8890  0.8714  0.8912  0.8784  0.9097\n",
      " 0.9672  0.8909  0.8905  0.8814  0.8814  0.8848  0.8697  0.8669  0.8845  0.8748\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8840  0.8929  0.8465  0.9237  0.8493  0.8270  0.9191  0.8751  0.8684  0.8676\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.8618  0.8933  0.8832  0.8766  0.8841  0.9022  0.8871  0.8874  0.8867  0.8839\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.8741  0.8050  0.8755  0.8433  0.8345  0.8694  0.8858  0.8627  0.8970  0.8754\n",
      " 0.7351  0.7619  0.7826  0.7525  0.7583  0.7359  0.7739  0.7598  0.7477  0.7903\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.8508  0.8116  0.8045  0.8383  0.7640  0.7913  0.8028  0.7440  0.7998  0.7874\n",
      " 0.9405  0.9359  0.8995  0.9448  0.9245  0.9308  0.9135  0.9377  0.9272  0.9423\n",
      " 0.9041  0.9478  0.9384  0.9127  0.9025  0.9179  0.9127  0.8933  0.9405  0.8852\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.7862  0.8156  0.7098  0.7743  0.8410  0.8065  0.8083  0.7273  0.7570  0.7303\n",
      " 0.9248  0.9306  0.9077  0.9190  0.9284  0.9350  0.9161  0.8993  0.9106  0.9151\n",
      " 0.8398  0.8845  0.8385  0.8955  0.8855  0.8771  0.8697  0.8603  0.8869  0.8810\n",
      " 0.8864  0.7505  0.8790  0.8734  0.8702  0.8877  0.8832  0.8801  0.8842  0.8729\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.9366  0.9483  0.8214  0.9256  0.9238  0.9075  0.8948  0.9386  0.8564  0.9149\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.8875  0.8944  0.8876  0.8771  0.8980  0.8537  0.8618  0.8719  0.9023  0.9042\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.8492  0.8223  0.8802  0.8475  0.8354  0.8194  0.8318  0.8233  0.8285  0.8618\n",
      " 0.7397  0.7587  0.7704  0.7734  0.7504  0.7729  0.7695  0.7727  0.7616  0.7609\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.8636  0.7891  0.8244  0.7757  0.8057  0.7974  0.8304  0.8318  0.8152  0.7971\n",
      " 0.9332  0.9489  0.9091  0.9302  0.9116  0.9306  0.9124  0.9298  0.9417  0.9343\n",
      " 0.9106  0.9117  0.9243  0.9130  0.9386  0.9190  0.9152  0.9074  0.8623  0.9355\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.7853  0.8303  0.7491  0.7631  0.6898  0.8164  0.7814  0.7280  0.7793  0.7980\n",
      " 0.9341  0.9169  0.9055  0.9267  0.8966  0.9288  0.9146  0.9074  0.9349  0.9214\n",
      " 0.9160  0.9264  0.8675  0.8897  0.8965  0.8897  0.8805  0.8957  0.8898  0.8655\n",
      " 0.8689  0.8695  0.8681  0.8865  0.8882  0.8789  0.8819  0.8689  0.8957  0.8794\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.8703  0.8922  0.9131  0.8946  0.8536  0.8988  0.9342  0.9030  0.8936  0.9167\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.8981  0.9162  0.8530  0.8942  0.8877  0.8714  0.8985  0.9049  0.8886  0.9029\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.8289  0.8522  0.8185  0.8634  0.8258  0.8750  0.8453  0.8266  0.8741  0.8585\n",
      " 0.7445  0.7970  0.7573  0.7669  0.7516  0.7286  0.7337  0.7676  0.8638  0.7773\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.7928  0.8215  0.8045  0.8154  0.7647  0.8165  0.7999  0.7941  0.8230  0.8336\n",
      " 0.9293  0.9238  0.9394  0.9354  0.9297  0.9273  0.9046  0.9399  0.8856  0.9427\n",
      " 0.8976  0.9150  0.9227  0.9167  0.9221  0.8889  0.9159  0.9248  0.9152  0.9216\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.6958  0.7576  0.7771  0.7515  0.7411  0.7145  0.7735  0.7283  0.7521  0.7362\n",
      " 0.9222  0.9127  0.8938  0.9091  0.9195  0.9058  0.9393  0.9296  0.9385  0.9345\n",
      " 0.8779  0.8953  0.8844  0.9057  0.8700  0.8793  0.8818  0.8411  0.8988  0.8304\n",
      " 0.8677  0.8706  0.8897  0.8971  0.8739  0.8604  0.8802  0.8681  0.8746  0.8563\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.8816\n",
      " 0.7736\n",
      " 0.8828\n",
      " 0.7736\n",
      " 0.8616\n",
      " 0.7531\n",
      " 0.7736\n",
      " 0.7736\n",
      " 0.8215\n",
      " 0.9406\n",
      " 0.8733\n",
      " 0.7736\n",
      " 0.7736\n",
      " 0.9116\n",
      " 0.8522\n",
      " 0.8943\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:15<00:15, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9554  0.8156  0.8404  0.7984  0.7835  0.8119  0.8098  0.7864  0.8209  0.8565\n",
      " 0.9463  0.6938  0.6687  0.6589  0.6200  0.7592  0.8015  0.6620  0.5722  0.6977\n",
      " 0.9729  0.9077  0.8565  0.8517  0.8726  0.8732  0.8547  0.8529  0.8462  0.8130\n",
      " 0.9824  0.8461  0.8261  0.8629  0.9060  0.9359  0.8940  0.8864  0.8741  0.8802\n",
      " 0.9764  0.8868  0.9305  0.8048  0.8816  0.9298  0.8696  0.8881  0.8588  0.8411\n",
      " 0.9778  0.8863  0.8492  0.8796  0.8928  0.8560  0.8694  0.8869  0.8347  0.8817\n",
      " 0.9624  0.8868  0.9305  0.8048  0.8816  0.9298  0.8696  0.8881  0.8588  0.8411\n",
      " 0.9685  0.7696  0.8304  0.8240  0.8301  0.8087  0.8305  0.7464  0.7961  0.8198\n",
      " 0.9396  0.7823  0.7673  0.7661  0.8143  0.7917  0.8054  0.8163  0.7907  0.8357\n",
      " 0.9302  0.6938  0.6687  0.6589  0.6200  0.7592  0.8015  0.6620  0.5722  0.6977\n",
      " 0.9468  0.8969  0.9135  0.8840  0.8857  0.9054  0.8905  0.8926  0.8867  0.9014\n",
      " 0.9634  0.7241  0.7019  0.6951  0.7208  0.6598  0.7328  0.6802  0.7198  0.7286\n",
      " 0.9528  0.6938  0.6687  0.6589  0.6200  0.7592  0.8015  0.6620  0.5722  0.6977\n",
      " 0.9134  0.6938  0.6687  0.6589  0.6200  0.7592  0.8015  0.6620  0.5722  0.6977\n",
      " 0.9189  0.6938  0.6687  0.6589  0.6200  0.7592  0.8015  0.6620  0.5722  0.6977\n",
      " 0.9664  0.8868  0.9305  0.8048  0.8816  0.9298  0.8696  0.8881  0.8588  0.8411\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8216  0.8026  0.8392  0.8057  0.7388  0.7865  0.8221  0.7882  0.7632  0.7731\n",
      " 0.7223  0.7596  0.6287  0.7050  0.7927  0.7476  0.7526  0.6477  0.6820  0.6481\n",
      " 0.8811  0.8810  0.8377  0.8553  0.8805  0.8919  0.8751  0.8520  0.8734  0.8453\n",
      " 0.8545  0.8686  0.8109  0.9079  0.8085  0.7879  0.9017  0.8434  0.8364  0.8365\n",
      " 0.7894  0.8473  0.8979  0.8968  0.8636  0.8717  0.8568  0.8810  0.8759  0.8902\n",
      " 0.8805  0.8767  0.8817  0.8663  0.8903  0.8848  0.8244  0.8781  0.8946  0.8632\n",
      " 0.7894  0.8473  0.8979  0.8968  0.8636  0.8717  0.8568  0.8810  0.8759  0.8902\n",
      " 0.8275  0.8253  0.8296  0.8330  0.8599  0.7939  0.7884  0.7837  0.8221  0.8510\n",
      " 0.8355  0.7377  0.8316  0.7890  0.7765  0.8353  0.8497  0.8142  0.8647  0.8328\n",
      " 0.7223  0.7596  0.6287  0.7050  0.7927  0.7476  0.7526  0.6477  0.6820  0.6481\n",
      " 0.8763  0.8830  0.8896  0.8975  0.8932  0.8954  0.8397  0.8664  0.8786  0.9174\n",
      " 0.6668  0.6986  0.7239  0.6845  0.6993  0.6687  0.7141  0.6973  0.6757  0.7323\n",
      " 0.7223  0.7596  0.6287  0.7050  0.7927  0.7476  0.7526  0.6477  0.6820  0.6481\n",
      " 0.7223  0.7596  0.6287  0.7050  0.7927  0.7476  0.7526  0.6477  0.6820  0.6481\n",
      " 0.7223  0.7596  0.6287  0.7050  0.7927  0.7476  0.7526  0.6477  0.6820  0.6481\n",
      " 0.7894  0.8473  0.8979  0.8968  0.8636  0.8717  0.8568  0.8810  0.8759  0.8902\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.7637  0.7801  0.8048  0.7885  0.8575  0.8268  0.8276  0.8137  0.8065  0.8278\n",
      " 0.7196  0.7807  0.6733  0.6917  0.6003  0.7603  0.7153  0.6451  0.7127  0.7366\n",
      " 0.9420  0.8925  0.8357  0.8934  0.8688  0.8837  0.8546  0.8948  0.8203  0.8845\n",
      " 0.9252  0.9395  0.7826  0.9107  0.9079  0.8874  0.8702  0.9268  0.8213  0.8951\n",
      " 0.8963  0.8667  0.7405  0.8783  0.8530  0.8866  0.8685  0.8262  0.8992  0.8755\n",
      " 0.8579  0.8678  0.8894  0.8580  0.8979  0.8312  0.8833  0.8312  0.8764  0.8710\n",
      " 0.8963  0.8667  0.7405  0.8783  0.8530  0.8866  0.8685  0.8262  0.8992  0.8755\n",
      " 0.8389  0.8710  0.8324  0.8413  0.8148  0.8234  0.8498  0.7951  0.8180  0.8190\n",
      " 0.7962  0.7634  0.8370  0.8005  0.7782  0.7576  0.7721  0.7616  0.7732  0.8173\n",
      " 0.7196  0.7807  0.6733  0.6917  0.6003  0.7603  0.7153  0.6451  0.7127  0.7366\n",
      " 0.8679  0.9036  0.8669  0.8694  0.9009  0.9059  0.9067  0.9076  0.8931  0.8707\n",
      " 0.6706  0.6924  0.7062  0.7086  0.6795  0.7133  0.7070  0.7129  0.6988  0.6950\n",
      " 0.7196  0.7807  0.6733  0.6917  0.6003  0.7603  0.7153  0.6451  0.7127  0.7366\n",
      " 0.7196  0.7807  0.6733  0.6917  0.6003  0.7603  0.7153  0.6451  0.7127  0.7366\n",
      " 0.7196  0.7807  0.6733  0.6917  0.6003  0.7603  0.7153  0.6451  0.7127  0.7366\n",
      " 0.8963  0.8667  0.7405  0.8783  0.8530  0.8866  0.8685  0.8262  0.8992  0.8755\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.8327  0.8313  0.8083  0.7842  0.7687  0.7732  0.8276  0.8317  0.7922  0.8420\n",
      " 0.6052  0.6848  0.7119  0.6809  0.6682  0.6319  0.7080  0.6490  0.6800  0.6585\n",
      " 0.8960  0.8542  0.8452  0.8431  0.8718  0.8482  0.8497  0.8647  0.8576  0.8944\n",
      " 0.8385  0.8625  0.8939  0.8680  0.8190  0.8726  0.9207  0.8800  0.8721  0.8968\n",
      " 0.8617  0.8448  0.8956  0.9006  0.8716  0.9059  0.8193  0.8863  0.8743  0.8645\n",
      " 0.8770  0.8724  0.8756  0.9009  0.8579  0.8298  0.8166  0.8777  0.8584  0.8967\n",
      " 0.8617  0.8448  0.8956  0.9006  0.8716  0.9059  0.8193  0.8863  0.8743  0.8645\n",
      " 0.8190  0.8162  0.8216  0.8245  0.8612  0.8431  0.7780  0.8602  0.7972  0.8272\n",
      " 0.7672  0.8001  0.7576  0.8184  0.7685  0.8350  0.7937  0.7702  0.8331  0.8132\n",
      " 0.6052  0.6848  0.7119  0.6809  0.6682  0.6319  0.7080  0.6490  0.6800  0.6585\n",
      " 0.8770  0.9003  0.9182  0.9244  0.9020  0.9003  0.8937  0.8825  0.8823  0.9259\n",
      " 0.6761  0.7472  0.6857  0.7017  0.6844  0.6583  0.6591  0.7066  0.8316  0.7231\n",
      " 0.6052  0.6848  0.7119  0.6809  0.6682  0.6319  0.7080  0.6490  0.6800  0.6585\n",
      " 0.6052  0.6848  0.7119  0.6809  0.6682  0.6319  0.7080  0.6490  0.6800  0.6585\n",
      " 0.6052  0.6848  0.7119  0.6809  0.6682  0.6319  0.7080  0.6490  0.6800  0.6585\n",
      " 0.8617  0.8448  0.8956  0.9006  0.8716  0.9059  0.8193  0.8863  0.8743  0.8645\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.8239\n",
      " 0.7040\n",
      " 0.8692\n",
      " 0.8521\n",
      " 0.8753\n",
      " 0.8799\n",
      " 0.8753\n",
      " 0.7952\n",
      " 0.8166\n",
      " 0.7040\n",
      " 0.8872\n",
      " 0.6834\n",
      " 0.7040\n",
      " 0.7040\n",
      " 0.7040\n",
      " 0.8753\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:32<00:00, 15.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, training loss: 0.5042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch value: 1\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9660  0.7064  0.7873  0.7794  0.7890  0.7566  0.7875  0.6757  0.7441  0.7754\n",
      " 0.9555  0.8588  0.9113  0.7502  0.8477  0.9141  0.8341  0.8611  0.8187  0.8003\n",
      " 0.8622  0.6171  0.5772  0.5704  0.5325  0.7031  0.7523  0.5785  0.4708  0.6153\n",
      " 0.9747  0.8271  0.7860  0.8429  0.8442  0.8551  0.7653  0.8426  0.8046  0.8091\n",
      " 0.9735  0.6566  0.6277  0.6216  0.6525  0.5836  0.6724  0.6044  0.6576  0.6623\n",
      " 0.9059  0.6171  0.5772  0.5704  0.5325  0.7031  0.7523  0.5785  0.4708  0.6153\n",
      " 0.9785  0.8565  0.8073  0.8517  0.8688  0.8180  0.8359  0.8563  0.7945  0.8523\n",
      " 0.9676  0.8333  0.7726  0.7679  0.8124  0.8041  0.8262  0.8283  0.7721  0.8049\n",
      " 0.9730  0.8588  0.9113  0.7502  0.8477  0.9141  0.8341  0.8611  0.8187  0.8003\n",
      " 0.9459  0.7422  0.6489  0.7215  0.7583  0.6411  0.6029  0.6610  0.7797  0.7301\n",
      " 0.9489  0.8713  0.8892  0.8499  0.8530  0.8773  0.8564  0.8623  0.8535  0.8733\n",
      " 0.9739  0.8563  0.8939  0.9156  0.8755  0.8484  0.8372  0.8832  0.8998  0.8744\n",
      " 0.9173  0.7166  0.6930  0.6910  0.7566  0.7233  0.7418  0.7629  0.7245  0.7795\n",
      " 0.9463  0.6171  0.5772  0.5704  0.5325  0.7031  0.7523  0.5785  0.4708  0.6153\n",
      " 0.9805  0.8168  0.7840  0.8306  0.8816  0.9269  0.8662  0.8634  0.8458  0.8530\n",
      " 0.9621  0.8924  0.9007  0.8893  0.8171  0.7810  0.8501  0.9269  0.8753  0.8435\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.7830  0.7792  0.7863  0.7941  0.8266  0.7400  0.7367  0.7232  0.7775  0.8143\n",
      " 0.7333  0.8050  0.8720  0.8685  0.8265  0.8354  0.8140  0.8479  0.8407  0.8603\n",
      " 0.6487  0.6943  0.5393  0.6255  0.7374  0.6793  0.6908  0.5605  0.5976  0.5534\n",
      " 0.7926  0.8296  0.8181  0.8018  0.8208  0.8443  0.8216  0.8216  0.8206  0.8160\n",
      " 0.5946  0.6277  0.6565  0.6115  0.6343  0.5956  0.6460  0.6270  0.5939  0.6711\n",
      " 0.6487  0.6943  0.5393  0.6255  0.7374  0.6793  0.6908  0.5605  0.5976  0.5534\n",
      " 0.8448  0.8452  0.8471  0.8309  0.8625  0.8566  0.7735  0.8427  0.8665  0.8260\n",
      " 0.8247  0.8451  0.8208  0.8194  0.8132  0.8187  0.8000  0.8230  0.8127  0.7537\n",
      " 0.7333  0.8050  0.8720  0.8685  0.8265  0.8354  0.8140  0.8479  0.8407  0.8603\n",
      " 0.7666  0.7010  0.6947  0.7416  0.6304  0.6676  0.6916  0.6029  0.6788  0.6562\n",
      " 0.8429  0.8533  0.8577  0.8721  0.8648  0.8660  0.8019  0.8289  0.8444  0.8945\n",
      " 0.8539  0.9301  0.9156  0.8742  0.8551  0.8771  0.8716  0.8417  0.9144  0.8306\n",
      " 0.7856  0.6559  0.7764  0.7176  0.7061  0.7940  0.8033  0.7525  0.8229  0.7806\n",
      " 0.6487  0.6943  0.5393  0.6255  0.7374  0.6793  0.6908  0.5605  0.5976  0.5534\n",
      " 0.8187  0.8376  0.7639  0.8854  0.7597  0.7379  0.8775  0.8045  0.7955  0.7991\n",
      " 0.8683  0.8453  0.8544  0.8428  0.8735  0.8916  0.8513  0.8740  0.9160  0.8766\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.7984  0.8377  0.7955  0.8023  0.7606  0.7761  0.8190  0.7385  0.7697  0.7699\n",
      " 0.8686  0.8325  0.6830  0.8393  0.8070  0.8552  0.8366  0.7813  0.8724  0.8417\n",
      " 0.6440  0.7233  0.5862  0.6088  0.5005  0.6972  0.6403  0.5526  0.6364  0.6631\n",
      " 0.8266  0.8353  0.8216  0.8106  0.8419  0.7716  0.7833  0.7941  0.8419  0.8472\n",
      " 0.5945  0.6219  0.6361  0.6348  0.5993  0.6478  0.6387  0.6477  0.6309  0.6219\n",
      " 0.6440  0.7233  0.5862  0.6088  0.5005  0.6972  0.6403  0.5526  0.6364  0.6631\n",
      " 0.8195  0.8330  0.8613  0.8203  0.8699  0.7882  0.8508  0.7856  0.8431  0.8352\n",
      " 0.7870  0.8046  0.8177  0.8313  0.8125  0.8079  0.8515  0.8419  0.8009  0.8317\n",
      " 0.8686  0.8325  0.6830  0.8393  0.8070  0.8552  0.8366  0.7813  0.8724  0.8417\n",
      " 0.7904  0.6736  0.7244  0.6417  0.6911  0.6741  0.7308  0.7382  0.7042  0.6868\n",
      " 0.8314  0.8721  0.8280  0.8359  0.8745  0.8767  0.8784  0.8841  0.8639  0.8341\n",
      " 0.8654  0.8696  0.8894  0.8762  0.9042  0.8804  0.8807  0.8667  0.7870  0.9142\n",
      " 0.7311  0.6916  0.7807  0.7369  0.7076  0.6828  0.7007  0.6840  0.7042  0.7629\n",
      " 0.6440  0.7233  0.5862  0.6088  0.5005  0.6972  0.6403  0.5526  0.6364  0.6631\n",
      " 0.9113  0.9278  0.7344  0.8901  0.8871  0.8591  0.8400  0.9118  0.7794  0.8688\n",
      " 0.9080  0.8740  0.8392  0.8395  0.7904  0.8672  0.8593  0.8692  0.9175  0.8524\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.7721  0.7700  0.7730  0.7828  0.8260  0.8048  0.7193  0.8260  0.7444  0.7861\n",
      " 0.8210  0.7937  0.8689  0.8729  0.8336  0.8802  0.7708  0.8572  0.8392  0.8270\n",
      " 0.5044  0.6001  0.6409  0.6031  0.5845  0.5387  0.6352  0.5597  0.6017  0.5719\n",
      " 0.8359  0.8713  0.7751  0.8312  0.8177  0.7980  0.8316  0.8467  0.8167  0.8480\n",
      " 0.5989  0.6952  0.6046  0.6300  0.6093  0.5809  0.5757  0.6395  0.7981  0.6665\n",
      " 0.5044  0.6001  0.6409  0.6031  0.5845  0.5387  0.6352  0.5597  0.6017  0.5719\n",
      " 0.8433  0.8418  0.8434  0.8777  0.8185  0.7814  0.7641  0.8420  0.8238  0.8706\n",
      " 0.7873  0.8347  0.8335  0.7898  0.7657  0.7617  0.8109  0.8081  0.8182  0.7729\n",
      " 0.8210  0.7937  0.8689  0.8729  0.8336  0.8802  0.7708  0.8572  0.8392  0.8270\n",
      " 0.6735  0.7178  0.6917  0.7078  0.6244  0.7145  0.6826  0.6711  0.7211  0.7353\n",
      " 0.8388  0.8742  0.8970  0.9073  0.8771  0.8730  0.8633  0.8497  0.8530  0.9044\n",
      " 0.8507  0.8721  0.8841  0.8764  0.8850  0.8378  0.8731  0.8925  0.8803  0.8811\n",
      " 0.6906  0.7342  0.6841  0.7618  0.6973  0.7836  0.7297  0.7008  0.7798  0.7537\n",
      " 0.5044  0.6001  0.6409  0.6031  0.5845  0.5387  0.6352  0.5597  0.6017  0.5719\n",
      " 0.7988  0.8256  0.8673  0.8341  0.7763  0.8397  0.9001  0.8505  0.8440  0.8699\n",
      " 0.8208  0.8408  0.8612  0.8638  0.8382  0.8977  0.8595  0.8525  0.8332  0.8939\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.7449\n",
      " 0.8380\n",
      " 0.6252\n",
      " 0.8128\n",
      " 0.6033\n",
      " 0.6252\n",
      " 0.8472\n",
      " 0.8018\n",
      " 0.8380\n",
      " 0.7099\n",
      " 0.8587\n",
      " 0.8009\n",
      " 0.7596\n",
      " 0.6252\n",
      " 0.8152\n",
      " 0.7985\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:15<00:15, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9295  0.6396  0.6081  0.6053  0.6884  0.6440  0.6652  0.7016  0.6492  0.7121\n",
      " 0.9591  0.8133  0.8987  0.8548  0.8553  0.8343  0.8424  0.8771  0.8605  0.8862\n",
      " 0.9539  0.7178  0.7693  0.6987  0.6761  0.7276  0.7235  0.6715  0.7340  0.7968\n",
      " 0.9577  0.7791  0.7269  0.7877  0.8480  0.9157  0.8264  0.8332  0.8069  0.8154\n",
      " 0.9415  0.7723  0.7890  0.7745  0.8057  0.7245  0.7466  0.8117  0.7740  0.8089\n",
      " 0.9593  0.5790  0.5450  0.5394  0.5748  0.5007  0.6040  0.5201  0.5910  0.5871\n",
      " 0.9151  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      " 0.9777  0.7511  0.7611  0.6226  0.7827  0.7951  0.7706  0.8067  0.7708  0.8426\n",
      " 0.9654  0.8473  0.7632  0.7485  0.7930  0.7809  0.7731  0.7612  0.7519  0.6951\n",
      " 0.8739  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      " 0.9491  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      " 0.9709  0.8435  0.8218  0.8489  0.8421  0.8482  0.8972  0.8795  0.7873  0.7862\n",
      " 0.8680  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      " 0.9566  0.8246  0.8869  0.6810  0.8050  0.8962  0.7881  0.8265  0.7682  0.7463\n",
      " 0.9583  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      " 0.8860  0.5418  0.4863  0.4867  0.4516  0.6487  0.7027  0.5010  0.3816  0.5332\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.7273  0.5649  0.7133  0.6320  0.6252  0.7464  0.7467  0.6777  0.7749  0.7172\n",
      " 0.8894  0.8807  0.8107  0.8953  0.8477  0.8758  0.8344  0.8760  0.8598  0.8805\n",
      " 0.7401  0.7036  0.7787  0.7052  0.6033  0.6889  0.7380  0.6873  0.6591  0.6561\n",
      " 0.7714  0.7951  0.7004  0.8544  0.6960  0.6765  0.8427  0.7539  0.7402  0.7502\n",
      " 0.7680  0.8218  0.7822  0.7642  0.8252  0.7576  0.7091  0.7715  0.7858  0.7966\n",
      " 0.5161  0.5482  0.5808  0.5300  0.5628  0.5157  0.5688  0.5489  0.5035  0.6034\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      " 0.7006  0.7846  0.7116  0.8045  0.7931  0.7885  0.7526  0.7298  0.8013  0.7748\n",
      " 0.8195  0.8071  0.7368  0.7744  0.8060  0.8284  0.7874  0.7480  0.7938  0.7556\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      " 0.8626  0.8610  0.8143  0.8542  0.8610  0.8775  0.8504  0.8212  0.8415  0.8431\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      " 0.6640  0.7493  0.8395  0.8334  0.7784  0.7901  0.7561  0.8058  0.7936  0.8216\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      " 0.5748  0.6296  0.4552  0.5470  0.6840  0.6096  0.6303  0.4796  0.5155  0.4603\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.6541  0.6113  0.7136  0.6582  0.6275  0.5969  0.6190  0.5968  0.6231  0.7010\n",
      " 0.8829  0.8984  0.8162  0.8655  0.8471  0.8669  0.8246  0.8666  0.8952  0.8764\n",
      " 0.6496  0.6728  0.7159  0.6896  0.8006  0.7375  0.7456  0.7265  0.7088  0.7459\n",
      " 0.8934  0.9124  0.6715  0.8622  0.8574  0.8213  0.7998  0.8913  0.7244  0.8327\n",
      " 0.7201  0.7600  0.8124  0.7861  0.7461  0.7684  0.7968  0.7865  0.7635  0.7742\n",
      " 0.5075  0.5444  0.5574  0.5526  0.5084  0.5759  0.5627  0.5740  0.5566  0.5408\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      " 0.8616  0.8840  0.7567  0.8073  0.8127  0.7982  0.7819  0.8042  0.8012  0.7610\n",
      " 0.9054  0.8263  0.7331  0.8240  0.7870  0.8089  0.7618  0.8293  0.7150  0.8037\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      " 0.8784  0.8428  0.8240  0.8652  0.7979  0.8584  0.8401  0.8246  0.8817  0.8618\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      " 0.8333  0.7873  0.6096  0.7888  0.7476  0.8154  0.7935  0.7238  0.8381  0.7978\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      " 0.5699  0.6655  0.5014  0.5265  0.4117  0.6318  0.5676  0.4659  0.5637  0.5896\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.6034  0.6595  0.5982  0.6956  0.6138  0.7175  0.6534  0.6204  0.7172  0.6830\n",
      " 0.8736  0.8586  0.8917  0.8747  0.8630  0.8652  0.8243  0.8796  0.8045  0.9043\n",
      " 0.7493  0.7482  0.7260  0.6720  0.6554  0.6676  0.7418  0.7528  0.6781  0.7761\n",
      " 0.7457  0.7787  0.8298  0.7893  0.7197  0.7945  0.8705  0.8092  0.8064  0.8334\n",
      " 0.7615  0.7962  0.7954  0.7907  0.7841  0.7281  0.8108  0.8077  0.8268  0.7451\n",
      " 0.5101  0.6420  0.5138  0.5523  0.5244  0.4931  0.4825  0.5640  0.7634  0.6070\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      " 0.7731  0.8171  0.7954  0.8347  0.7514  0.7784  0.7890  0.7086  0.8152  0.7060\n",
      " 0.8311  0.7646  0.7452  0.7460  0.7837  0.7534  0.7566  0.7692  0.7652  0.8329\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      " 0.8627  0.8394  0.8015  0.8295  0.8442  0.8283  0.8923  0.8674  0.8828  0.8828\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      " 0.7665  0.7276  0.8343  0.8370  0.7821  0.8482  0.7100  0.8216  0.7923  0.7777\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      " 0.4111  0.5162  0.5727  0.5303  0.5009  0.4516  0.5641  0.4724  0.5276  0.4874\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.6912\n",
      " 0.8792\n",
      " 0.7406\n",
      " 0.7684\n",
      " 0.7698\n",
      " 0.5135\n",
      " 0.5481\n",
      " 0.7311\n",
      " 0.7803\n",
      " 0.5481\n",
      " 0.5481\n",
      " 0.8330\n",
      " 0.5481\n",
      " 0.7891\n",
      " 0.5481\n",
      " 0.5481\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:36<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    2, training loss: 0.3483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing...\n",
      "Model invoked\n",
      "epoch value: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:03<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.7789  0.8584  0.7789  0.8240  0.8274  0.7922  0.8320  0.8275  0.8332  0.7893\n",
      " 0.9031  0.9031  0.8858  0.8846  0.8807  0.8743  0.8837  0.8809  0.8835  0.8362\n",
      " 0.8788  0.8676  0.8706  0.8468  0.8788  0.6917  0.8502  0.8643  0.8668  0.8276\n",
      " 0.8494  0.9180  0.8750  0.8810  0.8757  0.8383  0.8771  0.9265  0.8735  0.9041\n",
      " 0.9122  0.9122  0.9013  0.9091  0.8980  0.9177  0.8993  0.9161  0.9264  0.8153\n",
      " 0.8810  0.8810  0.8610  0.8901  0.8876  0.8571  0.7584  0.7137  0.8894  0.8644\n",
      " 0.8806  0.8817  0.8806  0.8790  0.8701  0.8650  0.9105  0.9009  0.8948  0.8594\n",
      " 0.9313  0.9313  0.9047  0.9179  0.9077  0.9405  0.9223  0.8931  0.9194  0.8988\n",
      " 0.9037  0.8946  0.8726  0.8385  0.9037  0.7448  0.8987  0.8341  0.8635  0.8294\n",
      " 0.9078  0.9078  0.9113  0.9185  0.9027  0.9292  0.9241  0.8992  0.9423  0.9123\n",
      " 0.8808  0.8895  0.8808  0.9119  0.8919  0.7728  0.8759  0.8764  0.8263  0.9091\n",
      " 0.8220  0.7868  0.8057  0.7760  0.8174  0.8115  0.8220  0.8189  0.7808  0.7993\n",
      " 0.9181  0.8981  0.9143  0.9163  0.9181  0.8954  0.9090  0.8764  0.8878  0.9173\n",
      " 0.9079  0.9079  0.9118  0.8873  0.9279  0.8823  0.9020  0.9345  0.9189  0.8717\n",
      " 0.8979  0.8979  0.9070  0.8942  0.9235  0.9114  0.9003  0.8963  0.8877  0.8910\n",
      " 0.8974  0.8947  0.8760  0.8396  0.9169  0.9105  0.8273  0.9043  0.8974  0.8986\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8011  0.7780  0.8064  0.7681  0.8080  0.8126  0.7650  0.8254  0.7990  0.7885\n",
      " 0.8912  0.9089  0.8576  0.8894  0.7446  0.8653  0.8697  0.8804  0.8733  0.8694\n",
      " 0.8425  0.8574  0.8041  0.8636  0.8434  0.8553  0.8584  0.8639  0.8708  0.8299\n",
      " 0.8986  0.9211  0.8982  0.8592  0.8494  0.8492  0.9159  0.8734  0.9010  0.9029\n",
      " 0.8891  0.8698  0.9273  0.9081  0.9036  0.9004  0.9028  0.8786  0.9019  0.9215\n",
      " 0.8606  0.9003  0.8369  0.8704  0.8849  0.8740  0.8714  0.8645  0.8088  0.8828\n",
      " 0.8973  0.9007  0.8004  0.9303  0.8864  0.9053  0.8907  0.8724  0.8606  0.8851\n",
      " 0.9291  0.9353  0.9244  0.9204  0.9411  0.9110  0.8980  0.9089  0.8777  0.9101\n",
      " 0.8627  0.8794  0.8871  0.8726  0.8713  0.9052  0.7827  0.8823  0.8833  0.8894\n",
      " 0.9146  0.9278  0.9021  0.9123  0.8895  0.9469  0.8914  0.8974  0.9174  0.9025\n",
      " 0.8916  0.9032  0.8826  0.8718  0.9034  0.9001  0.8813  0.9021  0.8602  0.8999\n",
      " 0.8147  0.8302  0.8094  0.8024  0.7887  0.7896  0.8297  0.8151  0.8111  0.8440\n",
      " 0.8859  0.8879  0.9081  0.8931  0.9111  0.8800  0.8929  0.8830  0.8902  0.9185\n",
      " 0.9090  0.8859  0.9183  0.8797  0.9095  0.9114  0.8962  0.9035  0.9005  0.8742\n",
      " 0.9157  0.8691  0.8669  0.9254  0.8399  0.9101  0.8387  0.8510  0.9186  0.8638\n",
      " 0.9242  0.8940  0.9114  0.9165  0.9229  0.8960  0.8798  0.9093  0.9160  0.8564\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.7842\n",
      " 0.8593\n",
      " 0.8481\n",
      " 0.9021\n",
      " 0.9119\n",
      " 0.8287\n",
      " 0.8668\n",
      " 0.9022\n",
      " 0.8667\n",
      " 0.8775\n",
      " 0.8424\n",
      " 0.7994\n",
      " 0.9080\n",
      " 0.8887\n",
      " 0.8293\n",
      " 0.8839\n",
      "[torch.FloatTensor of size 16x21]\n",
      "\n",
      "total test loss:  [0.37206700444221497]\n",
      "number of NaN: \n",
      "index           0\n",
      "query_id        0\n",
      "similar_id      0\n",
      "random_id       0\n",
      "bm25_score      0\n",
      "CNN_test      168\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss, train_idx_df, learning_rate_schedule = model_cnn(True, num_epochs, train_data, train_idx_df, batch_size, 40,  model, criterion, optimizer, scheduler, 'CNN_train', use_gpu, (SAVE_PATH + SAVE_NAME))\n",
    "\n",
    "print \"testing...\"\n",
    "running_test_loss, test_idx_df, _ = model_cnn(False, num_epochs, test_data, test_idx_df, batch_size, 20,  model, criterion, optimizer, scheduler, 'CNN_test', use_gpu, (SAVE_PATH + SAVE_NAME + \"test\"))\n",
    "\n",
    "    \n",
    "print \"total test loss: \", running_test_loss\n",
    "print \"number of NaN: \\n\", test_idx_df.isnull().sum()\n",
    "#test_idx_df = test_idx_df.dropna() #NaNs are due to restriction: range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_retrieved_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-25313ed6dee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mnn_retrieved_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_retrieved_scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ranking metrics...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f4a511bd72c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"computing ranking metrics...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcnn_mrr_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mrr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_idx_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CNN_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"cnn MRR (test): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_mrr_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sf_vmsharefolder/courses/6.864_term_project/ipynb/ranking_metrics.pyc\u001b[0m in \u001b[0;36mcompute_mrr\u001b[0;34m(data_frame, score_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mretrieved_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrelevant_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'similar_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mretrieved_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#sort according to scores (higher score is better, i.e. ranked higher)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#save scored data frame\n",
    "#test_idx_df.to_csv(SAVE_PATH + '/test_idx_df_scored_cnn.csv', header=True)\n",
    "\n",
    "print \"computing ranking metrics...\"\n",
    "cnn_mrr_test = compute_mrr(test_idx_df, score_name='CNN_test')\n",
    "print \"cnn MRR (test): \", np.mean(cnn_mrr_test)\n",
    "\n",
    "cnn_pr1_test = precision_at_k(test_idx_df, K=1, score_name='CNN_test')\n",
    "print \"cnn P@1 (test): \", np.mean(cnn_pr1_test)\n",
    "\n",
    "cnn_pr5_test = precision_at_k(test_idx_df, K=5, score_name='CNN_test')\n",
    "print \"cnn P@5 (test): \", np.mean(cnn_pr5_test)\n",
    "\n",
    "cnn_map_test = compute_map(test_idx_df, score_name='CNN_test')\n",
    "print \"cnn map (test): \", np.mean(cnn_map_test)\n",
    "\n",
    "\n",
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(training_loss, label='Adam')\n",
    "plt.title(\"CNN Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_training_loss1.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate_schedule, label='learning rate')\n",
    "plt.title(\"CNN learning rate schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_learning_rate_schedule1.png')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(validation_loss, label='Adam')\n",
    "plt.title(\"CNN Model Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_validation_loss.png')\n",
    "\"\"\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
