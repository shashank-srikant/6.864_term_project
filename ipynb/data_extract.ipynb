{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np.random.seed(0)\n",
    "from operator import itemgetter, attrgetter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from meter import AUCMeter\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "DATA_PATH_TARGET = config.get('paths', 'data_path_target')\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled data...\n",
      "elapsed time: 17.94 sec\n"
     ]
    }
   ],
   "source": [
    "print \"loading pickled data...\"\n",
    "tic = time()\n",
    "with open(data_filename) as f:  \n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "f.close()\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df['title_body'] = train_text_df['title'] + \" \" + train_text_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=None,\n",
    "                                 min_df=2, stop_words='english', strip_accents = 'ascii',\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_obj = vectorizer.fit(train_text_df['title_body'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text_file = DATA_PATH_TARGET + 'corpus.txt'\n",
    "target_text_df = pd.read_table(target_text_file, sep='\\t', header=None)\n",
    "target_text_df.columns = ['id', 'title', 'body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(embed1, embed2):\n",
    "    # embed1, embed2 could be tf-idf vectors, word embeddings, anything.\n",
    "    return cosine_similarity(embed1, embed2)\n",
    "\n",
    "def process_file(query_id_path, text_pd, vectorizer, ground_truth):\n",
    "    similarity_vector = []\n",
    "    ground_truth_arr = []\n",
    "    \n",
    "    data_frame = pd.read_table(query_id_path, sep=' ', header=None)\n",
    "    data_frame.columns = ['query_id', 'candidate_id']\n",
    "    \n",
    "    #num_samples = min(100,data_frame.shape[0])\n",
    "    num_samples = data_frame.shape[0]\n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        #try:\n",
    "            ind1 = np.where(text_pd['idz'] == data_frame.loc[idx,'query_id'])\n",
    "            ind2 = np.where(text_pd['idz'] == data_frame.loc[idx,'candidate_id'])\n",
    "            ind1 = int(ind1[0])\n",
    "            ind2 = int(ind2[0])\n",
    "            q1 = text_pd.loc[ind1,'body']\n",
    "            q2 = text_pd.loc[ind2,'body']\n",
    "            s = get_similarity(vectorizer.transform([q1]),vectorizer.transform([q2]))\n",
    "            similarity_vector.append(float(s[0][0]))\n",
    "            ground_truth_arr.append(ground_truth)\n",
    "        #except:\n",
    "         #    print \"oopsie1\" \n",
    "            \n",
    "        \n",
    "    return similarity_vector, ground_truth_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pd = pd.read_table(DATA_PATH_TARGET + 'corpus.txt', sep='\\t', header=None)\n",
    "text_pd.columns = ['idz', 'text','body']\n",
    "text_pd['body'] = text_pd['text'] + \" \" + text_pd['body']\n",
    "text_pd = text_pd.dropna()\n",
    "text_pd = text_pd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118500/118500 [06:12<00:00, 318.02it/s]\n",
      "100%|██████████| 1185/1185 [00:03<00:00, 324.62it/s]\n"
     ]
    }
   ],
   "source": [
    "auc_obj = AUCMeter()\n",
    "target_dev_neg = DATA_PATH_TARGET + 'test.neg.txt'\n",
    "sim_dev_neg, ground_truth_neg = process_file(target_dev_neg, text_pd, vectorizer, 0)\n",
    "\n",
    "target_dev_pos = DATA_PATH_TARGET + 'test.pos.txt'\n",
    "sim_dev_pos, ground_truth_pos = process_file(target_dev_pos, text_pd, vectorizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185\n",
      "0.595444462248\n"
     ]
    }
   ],
   "source": [
    "print len(sim_dev_pos)\n",
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array(sim_dev_pos), np.array(ground_truth_pos))\n",
    "auc_meter.add(np.array(sim_dev_neg), np.array(ground_truth_neg))\n",
    "print auc_meter.value(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array([0.4,0.2,0.4,0.2]),np.array([1,1,1,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "xy = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "sorted_x = sorted(xy.items(), key= itemgetter(1))\n",
    "xxx = pd.DataFrame(sorted_x)\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "xxx.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
