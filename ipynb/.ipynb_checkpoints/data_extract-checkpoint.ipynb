{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cPickle as pickle\n",
    "from tqdm import tqdm\n",
    "import ConfigParser\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "filename = SAVE_PATH + DATA_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time to retrieve extracted data from file: 23.44 sec\n"
     ]
    }
   ],
   "source": [
    "tic1 = time()\n",
    "with open(filename) as f:  # Python 3: open(..., 'rb')\n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "toc1 = time()\n",
    "print \"elapsed time to retrieve extracted data from file: %.2f sec\" %(toc1 - tic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>167763.000000</td>\n",
       "      <td>167763.000000</td>\n",
       "      <td>167763.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>277233.059864</td>\n",
       "      <td>6.077097</td>\n",
       "      <td>92.816032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>154615.224025</td>\n",
       "      <td>2.462824</td>\n",
       "      <td>210.555275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>140659.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>289429.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>412834.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>523750.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5634.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id      title_len       body_len\n",
       "count  167763.000000  167763.000000  167763.000000\n",
       "mean   277233.059864       6.077097      92.816032\n",
       "std    154615.224025       2.462824     210.555275\n",
       "min         1.000000       1.000000       0.000000\n",
       "25%    140659.500000       4.000000      28.000000\n",
       "50%    289429.000000       6.000000      47.000000\n",
       "75%    412834.000000       7.000000      85.000000\n",
       "max    523750.000000      26.000000    5634.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "MAX_TITLE_LEN = 20\n",
    "MAX_BODY_LEN = 100 \n",
    "#visualize data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\n",
    "sns.distplot(train_text_df['title_len'], hist=True, kde=True, color='b', label='Title length', ax=ax1)\n",
    "sns.distplot(train_text_df[train_text_df['body_len'] < 256]['body_len'], hist=True, kde=True, color='r', label='Body length', ax=ax2)\n",
    "\n",
    "ax1.axvline(x=MAX_TITLE_LEN, color='k', linestyle='--', label='20 words cutoff')\n",
    "ax2.axvline(x=MAX_BODY_LEN, color='k', linestyle='--', label='100 words cutoff')\n",
    "ax1.set_title('Title length histogram'); ax1.legend(loc=1); \n",
    "ax2.set_title('Body length histogram'); ax2.legend(loc=1);\n",
    "ax1.set_xlabel(\"# of words\")\n",
    "ax2.set_xlabel(\"# of words\")\n",
    "ax1.set_ylabel(\"Kernel Density Estimate\")\n",
    "ax2.set_ylabel(\"Kernel Density Estimate\")\n",
    "plt.show()\n",
    "#plt.savefig('./figures/question_len_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File /media/sf_vmsharefolder/courses/6.864_term_project/data/android//android/corpus.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f2e61602520c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDATA_PATH_TARGET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_path_target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtarget_text_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_PATH_TARGET\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/android/corpus.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtarget_text_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtarget_text_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /media/sf_vmsharefolder/courses/6.864_term_project/data/android//android/corpus.txt does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print \"loading data...\"\n",
    "tic = time()\n",
    "DATA_PATH_TARGET = config.get('paths', 'data_path_target')\n",
    "target_text_file = DATA_PATH_TARGET + '/corpus.txt'\n",
    "target_text_df = pd.read_table(target_text_file, sep='\\t', header=None)\n",
    "print target_text_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id     title_len      body_len  title_len_or   body_len_or\n",
      "count   42970.000000  42970.000000  42970.000000  42970.000000  42970.000000\n",
      "mean    85314.823551      6.912753     42.172795      9.491832     68.795648\n",
      "std     52573.279439      2.597669     15.287447      3.945501     24.103445\n",
      "min         1.000000      1.000000      2.000000      1.000000      3.000000\n",
      "25%     38321.500000      5.000000     30.000000      7.000000     49.000000\n",
      "50%     83155.000000      7.000000     46.000000      9.000000     77.000000\n",
      "75%    133725.500000      8.000000     55.000000     12.000000     90.000000\n",
      "max    170992.000000     23.000000    200.000000     34.000000    217.000000\n",
      "                id_1           id_2\n",
      "count    1185.000000    1185.000000\n",
      "mean   108000.967089   50194.807595\n",
      "std     35356.505773   42224.970727\n",
      "min      2728.000000       1.000000\n",
      "25%     83468.000000   15869.000000\n",
      "50%    103983.000000   36860.000000\n",
      "75%    139172.000000   75684.000000\n",
      "max    170871.000000  170413.000000\n",
      "                id_1           id_2\n",
      "count  118500.000000  118500.000000\n",
      "mean   108000.967089   85066.364152\n",
      "std     35341.733389   52587.049017\n",
      "min      2728.000000       1.000000\n",
      "25%     83468.000000   38002.000000\n",
      "50%    103983.000000   82641.000000\n",
      "75%    139172.000000  133493.000000\n",
      "max    170871.000000  170992.000000\n"
     ]
    }
   ],
   "source": [
    "print target_text_df.describe()\n",
    "print target_pos_df.describe()\n",
    "print target_neg_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "MAX_TITLE_LEN = 20\n",
    "MAX_BODY_LEN = 100 \n",
    "#visualize data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\n",
    "sns.distplot(target_text_df['title_len'], hist=True, kde=True, color='b', label='Title length', ax=ax1)\n",
    "sns.distplot(target_text_df['body_len'], hist=True, kde=True, color='r', label='Body length', ax=ax2)\n",
    "\n",
    "ax1.axvline(x=MAX_TITLE_LEN, color='k', linestyle='--', label='20 words cutoff')\n",
    "ax2.axvline(x=MAX_BODY_LEN, color='k', linestyle='--', label='100 words cutoff')\n",
    "ax1.set_title('Title length histogram'); ax1.legend(loc=1); \n",
    "ax2.set_title('Body length histogram'); ax2.legend(loc=1);\n",
    "ax1.set_xlabel(\"# of words\")\n",
    "ax2.set_xlabel(\"# of words\")\n",
    "ax1.set_ylabel(\"Kernel Density Estimate\")\n",
    "ax2.set_ylabel(\"Kernel Density Estimate\")\n",
    "plt.show()\n",
    "#plt.savefig('./figures/question_len_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#model parameters\n",
    "embed_dim = embeddings.shape[1] #200\n",
    "hidden_size = 128 #hidden vector dim \n",
    "weight_decay = 1e-5 \n",
    "learning_rate = 1e-3\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x, flg):\n",
    "        print \"--\\n input\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        print \"--\\n after embed\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        print \"--\\n after unsqueeze\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        \n",
    "        for conv in self.convs1:\n",
    "            conv(x)\n",
    "        \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        print \"--\\n RELU\\n--\\n\"\n",
    "        for t in x:\n",
    "            print t.data.shape\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        print \"--\\n after maxpool\\n--\\n\"\n",
    "        for t in x:\n",
    "            print t.data.shape\n",
    "        x = torch.cat(x, 1)\n",
    "        print \"--\\n after concat\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        return x\n",
    "\n",
    "batch_size = 32\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  \n",
    "kernel_sizes = range(2,6)\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m = nn.AvgPool1d(3, stride=1)\n",
    "x = torch.Tensor([[[20, 30]]])\n",
    "m = [nn.AvgPool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print \"training...\"\n",
    "for epoch in range(2):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "        \n",
    "    for batch in tqdm(train_data_loader):\n",
    "    \n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])        \n",
    "        optimizer.zero_grad()\n",
    "        print query_title.data.shape\n",
    "        cnn_query_title = model(query_title, 0)\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np.random.seed(0)\n",
    "from operator import itemgetter, attrgetter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from meter import AUCMeter\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "DATA_PATH_TARGET = config.get('paths', 'data_path_target')\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "#CNN parameters\n",
    "batch_size = 32\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  \n",
    "kernel_sizes = range(2,6)\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = torch.load(\"../trained_models/cnn_baseline_full_40neg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_df['title_body'] = train_text_df['title'] + \" \" + train_text_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=None,\n",
    "                                 min_df=2, stop_words='english', strip_accents = 'ascii',\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_obj = vectorizer.fit(train_text_df['title_body'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_text_file = DATA_PATH_TARGET + 'corpus.txt'\n",
    "target_text_df = pd.read_table(target_text_file, sep='\\t', header=None)\n",
    "target_text_df.columns = ['id', 'title', 'body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(embed1, embed2):\n",
    "    # embed1, embed2 could be tf-idf vectors, word embeddings, anything.\n",
    "    return cosine_similarity(embed1, embed2)\n",
    "\n",
    "def process_file(query_id_path, text_pd, vectorizer, ground_truth):\n",
    "    similarity_vector = []\n",
    "    ground_truth_arr = []\n",
    "    \n",
    "    data_frame = pd.read_table(query_id_path, sep=' ', header=None)\n",
    "    data_frame.columns = ['query_id', 'candidate_id']\n",
    "    \n",
    "    #num_samples = min(100,data_frame.shape[0])\n",
    "    num_samples = data_frame.shape[0]\n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        #try:\n",
    "            ind1 = np.where(text_pd['idz'] == data_frame.loc[idx,'query_id'])\n",
    "            ind2 = np.where(text_pd['idz'] == data_frame.loc[idx,'candidate_id'])\n",
    "            ind1 = int(ind1[0])\n",
    "            ind2 = int(ind2[0])\n",
    "            q1 = text_pd.loc[ind1,'body']\n",
    "            q2 = text_pd.loc[ind2,'body']\n",
    "            s = get_similarity(vectorizer.transform([q1]),vectorizer.transform([q2]))\n",
    "            similarity_vector.append(float(s[0][0]))\n",
    "            ground_truth_arr.append(ground_truth)\n",
    "        #except:\n",
    "         #    print \"oopsie1\" \n",
    "            \n",
    "        \n",
    "    return similarity_vector, ground_truth_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_pd = pd.read_table(DATA_PATH_TARGET + 'corpus.txt', sep='\\t', header=None)\n",
    "text_pd.columns = ['idz', 'text','body']\n",
    "text_pd['body'] = text_pd['text'] + \" \" + text_pd['body']\n",
    "text_pd = text_pd.dropna()\n",
    "text_pd = text_pd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_obj = AUCMeter()\n",
    "target_dev_neg = DATA_PATH_TARGET + 'test.neg.txt'\n",
    "sim_dev_neg, ground_truth_neg = process_file(target_dev_neg, text_pd, vectorizer, 0)\n",
    "\n",
    "target_dev_pos = DATA_PATH_TARGET + 'test.pos.txt'\n",
    "sim_dev_pos, ground_truth_pos = process_file(target_dev_pos, text_pd, vectorizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(sim_dev_pos)\n",
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array(sim_dev_pos), np.array(ground_truth_pos))\n",
    "auc_meter.add(np.array(sim_dev_neg), np.array(ground_truth_neg))\n",
    "print auc_meter.value(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array([0.4,0.2,0.4,0.2]),np.array([1,1,1,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "xy = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "sorted_x = sorted(xy.items(), key= itemgetter(1))\n",
    "xxx = pd.DataFrame(sorted_x)\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "xxx.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
