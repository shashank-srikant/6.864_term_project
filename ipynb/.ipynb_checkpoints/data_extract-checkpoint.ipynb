{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled data...\n",
      "elapsed time: 53.29 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('rnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME\n",
    "\n",
    "print \"loading pickled data...\"\n",
    "tic = time()\n",
    "with open(data_filename) as f:  \n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "f.close()\n",
    "with open(train_test_filename) as f:\n",
    "    train_data, val_data, test_data = pickle.load(f)\n",
    "f.close()\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "#visualize data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sns.distplot(train_text_df['title_len'], hist=True, kde=True, color='b', label='title len', ax=ax1)\n",
    "sns.distplot(train_text_df[train_text_df['body_len'] < 256]['body_len'], hist=True, kde=True, color='r', label='body len', ax=ax2)\n",
    "ax1.axvline(x=MAX_TITLE_LEN, color='k', linestyle='--', label='max len')\n",
    "ax2.axvline(x=MAX_BODY_LEN, color='k', linestyle='--', label='max len')\n",
    "ax1.set_title('title length histogram'); ax1.legend(loc=1); \n",
    "ax2.set_title('body length histogram'); ax2.legend(loc=1);\n",
    "plt.savefig('../figures/question_len_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 100, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model parameters\n",
    "embed_dim = embeddings.shape[1] #200\n",
    "hidden_size = 128 #hidden vector dim \n",
    "weight_decay = 1e-5 \n",
    "learning_rate = 1e-3\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        print \"--\\n input\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        print \"--\\n after embed\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        print \"--\\n after unsqueeze\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        print \"--\\n RELU\\n--\\n\"\n",
    "        for t in x:\n",
    "            print t.data.shape\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        print \"--\\n after maxpool\\n--\\n\"\n",
    "        for t in x:\n",
    "            print t.data.shapex = torch.cat(x, 1)\n",
    "        print \"--\\n after concat\\n--\\n\"\n",
    "        print x.data.shape\n",
    "        return x\n",
    "\n",
    "batch_size = 32\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  \n",
    "kernel_sizes = range(2,6)\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: dimension 2 out of range of 2D tensor at /opt/conda/conda-bld/pytorch_1503961620703/work/torch/lib/TH/generic/THTensor.c:24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-82f8ef69cadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#m = nn.AvgPool1d(3, stride=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAvgPool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: dimension 2 out of range of 2D tensor at /opt/conda/conda-bld/pytorch_1503961620703/work/torch/lib/TH/generic/THTensor.c:24"
     ]
    }
   ],
   "source": [
    "#m = nn.AvgPool1d(3, stride=1)\n",
    "x = torch.Tensor([[[20, 30]]])\n",
    "m = [nn.AvgPool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n",
      "--\n",
      " input\n",
      "--\n",
      "\n",
      "torch.Size([32, 20])\n",
      "--\n",
      " after embed\n",
      "--\n",
      "\n",
      "torch.Size([32, 20, 200])\n",
      "--\n",
      " after unsqueeze\n",
      "--\n",
      "\n",
      "torch.Size([32, 1, 20, 200])\n",
      "--\n",
      " RELU\n",
      "--\n",
      "\n",
      "4\n",
      "torch.Size([32, 100, 19])\n",
      "--\n",
      " after maxpool\n",
      "--\n",
      "\n",
      "4\n",
      "torch.Size([32, 100])\n",
      "--\n",
      " after concat\n",
      "--\n",
      "\n",
      "torch.Size([32, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-90d5a605eef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mquery_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcnn_query_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"training...\"\n",
    "for epoch in range(2):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "        \n",
    "    for batch in tqdm(train_data_loader):\n",
    "    \n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])        \n",
    "        optimizer.zero_grad()\n",
    "        print query_title.data.shape\n",
    "        cnn_query_title = model(query_title)\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np.random.seed(0)\n",
    "from operator import itemgetter, attrgetter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from meter import AUCMeter\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "DATA_PATH_TARGET = config.get('paths', 'data_path_target')\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "#CNN parameters\n",
    "batch_size = 32\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  \n",
    "kernel_sizes = range(2,6)\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load(\"../trained_models/cnn_baseline_full_40neg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_df['title_body'] = train_text_df['title'] + \" \" + train_text_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=None,\n",
    "                                 min_df=2, stop_words='english', strip_accents = 'ascii',\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_obj = vectorizer.fit(train_text_df['title_body'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_text_file = DATA_PATH_TARGET + 'corpus.txt'\n",
    "target_text_df = pd.read_table(target_text_file, sep='\\t', header=None)\n",
    "target_text_df.columns = ['id', 'title', 'body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(embed1, embed2):\n",
    "    # embed1, embed2 could be tf-idf vectors, word embeddings, anything.\n",
    "    return cosine_similarity(embed1, embed2)\n",
    "\n",
    "def process_file(query_id_path, text_pd, vectorizer, ground_truth):\n",
    "    similarity_vector = []\n",
    "    ground_truth_arr = []\n",
    "    \n",
    "    data_frame = pd.read_table(query_id_path, sep=' ', header=None)\n",
    "    data_frame.columns = ['query_id', 'candidate_id']\n",
    "    \n",
    "    #num_samples = min(100,data_frame.shape[0])\n",
    "    num_samples = data_frame.shape[0]\n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        #try:\n",
    "            ind1 = np.where(text_pd['idz'] == data_frame.loc[idx,'query_id'])\n",
    "            ind2 = np.where(text_pd['idz'] == data_frame.loc[idx,'candidate_id'])\n",
    "            ind1 = int(ind1[0])\n",
    "            ind2 = int(ind2[0])\n",
    "            q1 = text_pd.loc[ind1,'body']\n",
    "            q2 = text_pd.loc[ind2,'body']\n",
    "            s = get_similarity(vectorizer.transform([q1]),vectorizer.transform([q2]))\n",
    "            similarity_vector.append(float(s[0][0]))\n",
    "            ground_truth_arr.append(ground_truth)\n",
    "        #except:\n",
    "         #    print \"oopsie1\" \n",
    "            \n",
    "        \n",
    "    return similarity_vector, ground_truth_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_pd = pd.read_table(DATA_PATH_TARGET + 'corpus.txt', sep='\\t', header=None)\n",
    "text_pd.columns = ['idz', 'text','body']\n",
    "text_pd['body'] = text_pd['text'] + \" \" + text_pd['body']\n",
    "text_pd = text_pd.dropna()\n",
    "text_pd = text_pd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_obj = AUCMeter()\n",
    "target_dev_neg = DATA_PATH_TARGET + 'test.neg.txt'\n",
    "sim_dev_neg, ground_truth_neg = process_file(target_dev_neg, text_pd, vectorizer, 0)\n",
    "\n",
    "target_dev_pos = DATA_PATH_TARGET + 'test.pos.txt'\n",
    "sim_dev_pos, ground_truth_pos = process_file(target_dev_pos, text_pd, vectorizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(sim_dev_pos)\n",
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array(sim_dev_pos), np.array(ground_truth_pos))\n",
    "auc_meter.add(np.array(sim_dev_neg), np.array(ground_truth_neg))\n",
    "print auc_meter.value(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(np.array([0.4,0.2,0.4,0.2]),np.array([1,1,1,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "xy = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "sorted_x = sorted(xy.items(), key= itemgetter(1))\n",
    "xxx = pd.DataFrame(sorted_x)\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "xxx.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
