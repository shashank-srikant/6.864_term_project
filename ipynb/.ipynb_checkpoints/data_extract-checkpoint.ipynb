{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "####\n",
    "import os\n",
    "from os.path import dirname, realpath\n",
    "import sys\n",
    "sys.path.append(dirname(os.getcwd()))\n",
    "import nltk as nk\n",
    "from nltk.corpus import stopwords\n",
    "import dill\n",
    "import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "TRAIN_TEST_FILE_NAME = \"data_train_test_100.dat\"\n",
    "filename = SAVE_PATH + TRAIN_TEST_FILE_NAME\n",
    "with open(filename) as f:  # Python 3: open(..., 'rb')\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "filename = SAVE_PATH + DATA_FILE_NAME\n",
    "\n",
    "tic1 = time()\n",
    "with open(filename) as f:  # Python 3: open(..., 'rb')\n",
    "    train_text_df, train_idx_df, dev_idx_df, embeddings, word_to_idx = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_text_df\n",
    "del train_idx_df\n",
    "del dev_idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print \"begin: \"+str(x.size())\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        #print \"after embedding: \"+ str(x.size())\n",
    "        #if self.args.static:\n",
    "        #    x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        #print \"after unsqueeze: \"+ str(x.size())\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        #print x\n",
    "        #print \"after relu: \"+ str(len(x)) + \"::\" + str(x[0].size())+ \"::\" + str(x[1].size())\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        #print x\n",
    "        #print \"after max_pool1d: \"+ str(len(x)) + \"::\" + str(x[0].size())+ \"::\" + str(x[1].size())\n",
    "        \n",
    "        x = torch.cat(x, 1)\n",
    "        #print \"after torch cat final step: \"+ str(x.size())\n",
    "        #print \"---\"\n",
    "        #sys.exit(0)\n",
    "        #x = self.dropout(x) # (N,len(Ks)*Co)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 100, kernel_size=(6, 200), stride=(1, 1))\n",
      "    (4): Conv2d(1, 100, kernel_size=(7, 200), stride=(1, 1))\n",
      "    (5): Conv2d(1, 100, kernel_size=(8, 200), stride=(1, 1))\n",
      "    (6): Conv2d(1, 100, kernel_size=(9, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100\n",
    "kernel_sizes = range(3,10)\n",
    "batch_size = 50\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e2\n",
    "print model\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=2, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-201b2d92e8aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcount_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcount_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"::batch begin::\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mforking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0m_current_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/miniconda2/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'random'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"training...\"\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    count_batch = 0    \n",
    "    for batch in train_data_loader:\n",
    "        count_batch += 1\n",
    "        print \"::batch begin::\"\n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])\n",
    "        similar_title = Variable(batch['similar_title'])\n",
    "        similar_body = Variable(batch['similar_body'])\n",
    "        \n",
    "        random_title_list = []\n",
    "        random_body_list = []\n",
    "        for ridx in range(10):  #range(100)\n",
    "            random_title_name = 'random_title_' + str(ridx)\n",
    "            random_body_name = 'random_body_' + str(ridx)\n",
    "            random_title_list.append(Variable(batch[random_title_name]))\n",
    "            random_body_list.append(Variable(batch[random_body_name]))\n",
    "        \n",
    "        if count_batch  == 1:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        print \"::query title::\" \n",
    "        lstm_query_title = model(query_title)\n",
    "        print \"::query body::\" \n",
    "        lstm_query_body = model(query_body)\n",
    "        lstm_query = (lstm_query_title + lstm_query_body)/2.0\n",
    "        \n",
    "        print \"::query similar title::\" \n",
    "        lstm_similar_title = model(similar_title)\n",
    "        print \"::query similar body::\" \n",
    "        lstm_similar_body = model(similar_body)\n",
    "        lstm_similar = (lstm_similar_title + lstm_similar_body)/2.0\n",
    "        \n",
    "        \n",
    "        lstm_random_list = []\n",
    "        print \"::random title body process::\" \n",
    "        \n",
    "        for ridx in range(len(random_title_list)):\n",
    "            lstm_random_title = model(random_title_list[ridx])\n",
    "            lstm_random_body = model(random_body_list[ridx])\n",
    "            lstm_random = (lstm_random_title + lstm_random_body)/2.0\n",
    "            lstm_random_list.append(lstm_random)\n",
    "        \n",
    "        print \"done random processing..\"\n",
    "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score_pos = cosine_similarity(lstm_query, lstm_similar)\n",
    "\n",
    "        score_list = []\n",
    "        score_list1 = []\n",
    "        \n",
    "        score_list.append(score_pos)\n",
    "        score_list1.append(score_pos.data.numpy())\n",
    "        print \"::query random title body::\" \n",
    "        for ridx in range(len(lstm_random_list)):\n",
    "            score_neg = cosine_similarity(lstm_query, lstm_random_list[ridx])\n",
    "            score_list.append(score_neg)\n",
    "            score_list1.append(score_neg.data.numpy())\n",
    "\n",
    "        print \"::done scoring::\"\n",
    "        '''\n",
    "        diff = score_list1[0] - np.median(score_list1[1:])\n",
    "        plt.plot(diff)\n",
    "        plt.show()\n",
    "        '''\n",
    "        '''\n",
    "        X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "        y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "        loss = criterion(X_scores, y_targets) #y_target=0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        print \"::batch end::\"\n",
    "        #running_train_loss += loss.cpu().data[0]        \n",
    "        \n",
    "    #end for\n",
    "    #training_loss.append(running_train_loss)\n",
    "    #print \"epoch: %4d, training loss: %.4f\" %(epoch+1, running_train_loss)\n",
    "    \n",
    "    #torch.save(model, SAVE_PATH)\n",
    "#end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "    531     13     24  ...       0      0      0\n",
       "   1793   1328    575  ...       0      0      0\n",
       "   2652    275    969  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "    146    259    614  ...       0      0      0\n",
       "   2030    764   4917  ...       0      0      0\n",
       "    795    982    637  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "  26678   6722  77258  ...       0      0      0\n",
       "    831   1039    479  ...       0      0      0\n",
       "    446   2252   1429  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   1793   2103   1328  ...       0      0      0\n",
       "    113    938    207  ...       0      0      0\n",
       "   2338   1793    707  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "  21746   5038   3242  ...       0      0      0\n",
       "   1736  40012  13132  ...       0      0      0\n",
       "    231   1779   4491  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   5215  30486   1641  ...       0      0      0\n",
       "   1170     75   1243  ...       0      0      0\n",
       "     34  10120   4504  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "    532    976  11071  ...       0      0      0\n",
       "    191   1793   2000  ...       0      0      0\n",
       "   2261   1793   1018  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "     34    761     71  ...       0      0      0\n",
       "   2406   8454   6916  ...       0      0      0\n",
       "   1793   1328   8454  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   2155   2139    127  ...       0      0      0\n",
       "   5254   1645   7468  ...       0      0      0\n",
       "   1569    944    580  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   1793   2406   8454  ...       0      0      0\n",
       "   1328   8454    493  ...       0      0      0\n",
       "    913  15109   1793  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   1332    220   1328  ...       0      0      0\n",
       "  26223  22813   3157  ...       0      0      0\n",
       "  11217   1397   2171  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "  20049   4830    321  ...       0      0      0\n",
       "   3517      0   1645  ...       0      0      0\n",
       "   1793   1468    831  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   8639   1412    271  ...       0      0      0\n",
       "    259    805    841  ...       0      0      0\n",
       "    231   1793   5346  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   7764  17930   1776  ...       0      0      0\n",
       "   1793    656   7524  ...       0      0      0\n",
       "    900   2097   1452  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   1898  10911   1578  ...       0      0      0\n",
       "     35    841   2352  ...       0      0      0\n",
       "    152   2261  21262  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   1736   2782   1859  ...       0      0      0\n",
       "   1569    599   5038  ...       0      0      0\n",
       "   5866  16730    259  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   1044   4586    599  ...       0      0      0\n",
       "    531  27556    625  ...       0      0      0\n",
       "  10841    235    534  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "    359   4830    959  ...       0      0      0\n",
       "   2610   4140   7662  ...       0      0      0\n",
       "   2352   1793   1328  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20], Variable containing:\n",
       "   3616    321    367  ...       0      0      0\n",
       "   2261   1793   5607  ...       0      0      0\n",
       "     65   1170    963  ...       0      0      0\n",
       "         ...            ⋱           ...         \n",
       "   1793   2957   2338  ...       0      0      0\n",
       "   3616    321    367  ...       0      0      0\n",
       "  13717  10815   3403  ...       0      0      0\n",
       " [torch.LongTensor of size 64x20]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
