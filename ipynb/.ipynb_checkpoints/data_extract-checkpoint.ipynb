{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "####\n",
    "import os\n",
    "from os.path import dirname, realpath\n",
    "import sys\n",
    "sys.path.append(dirname(os.getcwd()))\n",
    "import nltk as nk\n",
    "from nltk.corpus import stopwords\n",
    "import dill\n",
    "import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sf_vmsharefolder/courses/6.864_term_project/\n",
      "data\n",
      "docs\n",
      "20\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'config.ini'))\n",
    "ROOT_PATH = config.get('paths', 'root_path')\n",
    "DATA_PATH = config.get('paths', 'data_path')\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "print ROOT_PATH\n",
    "print DATA_PATH\n",
    "print SAVE_PATH\n",
    "print MAX_TITLE_LEN\n",
    "print MAX_BODY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "DATA_PATH = '../data/askubuntu/'\n",
    "\n",
    "SAVE_PATH = './cnn_baseline.pt' \n",
    "EMBEDDINGS_FILE = DATA_PATH + '/vector/vectors_pruned.200.txt'\n",
    "MAX_TITLE_LEN = 20\n",
    "MAX_BODY_LEN = 100  #max number of words per sentence\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_embeddings():\n",
    "    lines = []\n",
    "    with open(EMBEDDINGS_FILE, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    embedding_tensor = []\n",
    "    word_to_idx = {}\n",
    "    \n",
    "    for idx, l in enumerate(lines):\n",
    "        word, emb = l.split()[0], l.split()[1:]\n",
    "        vector = [float(x) for x in emb]\n",
    "        if idx == 0: #reserved\n",
    "            embedding_tensor.append(np.zeros(len(vector)))\n",
    "        embedding_tensor.append(vector)\n",
    "        word_to_idx[word] = idx+1\n",
    "    #end for\n",
    "    embedding_tensor = np.array(embedding_tensor, dtype=np.float32)    \n",
    "    return embedding_tensor, word_to_idx\n",
    "        \n",
    "def get_tensor_idx(text, word_to_idx, max_len):\n",
    "    null_idx = 0  #idx if word is not in the embeddings dictionary\n",
    "    text_idx = [word_to_idx[x] if x in word_to_idx else null_idx for x in text][:max_len]\n",
    "    if len(text_idx) < max_len:\n",
    "        text_idx.extend([null_idx for _ in range(max_len - len(text_idx))])    \n",
    "    x = torch.LongTensor(text_idx)  #64-bit integer\n",
    "    return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "print \"loading data...\"\n",
    "print os.getcwd()\n",
    "stop = stopwords.words('english')\n",
    "tic = time()\n",
    "train_text_file = DATA_PATH + 'texts_raw_fixed.txt'\n",
    "train_text_df = pd.read_table(train_text_file, sep='\\t', header=None)\n",
    "train_text_df.columns = ['id', 'title', 'body']\n",
    "train_text_df = train_text_df.dropna()\n",
    "train_text_df['title_len'] = train_text_df['title'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "train_text_df['body_len'] = train_text_df['body'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "train_text_df['title'] = train_text_df['title'].apply(lambda words:' '.join(filter(lambda x: x not in stop,  words.split())))\n",
    "train_text_df['body'] = train_text_df['body'].apply(lambda words: ' '.join(filter(lambda x: x not in stop,  words.split())))\n",
    "train_text_df['title_no_stpwrds_len'] = train_text_df['title'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "train_text_df['body_no_stpwrds_len'] = train_text_df['body'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "\n",
    "train_idx_file = DATA_PATH + '/train_random.txt' \n",
    "train_idx_df = pd.read_table(train_idx_file, sep='\\t', header=None)\n",
    "train_idx_df.columns = ['query_id', 'similar_id', 'random_id']\n",
    "\n",
    "dev_idx_file = DATA_PATH + '/dev.txt'\n",
    "dev_idx_df = pd.read_table(dev_idx_file, sep='\\t', header=None)\n",
    "dev_idx_df.columns = ['query_id', 'similar_id', 'retrieved_id', 'bm25_score']\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "\n",
    "print \"loading embeddings...\"\n",
    "tic = time()\n",
    "embeddings, word_to_idx = get_embeddings()\n",
    "print \"vocab size (embeddings): \", len(word_to_idx)\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stop = stopwords.words('english')\n",
    "print stop\n",
    "def gg(words, stop):\n",
    "    return ' '.join(filter(lambda x: x not in stop,  words.split()))\n",
    "\n",
    "ass = \"how to get the your battery is broken message to go away ? everytime i turn on my computer , i see a message saying something like : your battery may be old or broken . etc ... i know my battery is not working . how can i make the message go away ?3\thow can i set the software center to install software for non-root users ?\thow can i set the software center to allow non-root users to install stuff from the ubuntu repos without having to type in their password ? i 'm fully aware of the security implications , and i am willing to take the risk . fedora 12 shipped with something like this . ( by modifying the policykit configuration , i believe )\"\n",
    "ass1 = \"how to get the your battery is broken message to go away ? everytime i turn\"\n",
    "print gg(ass1,stop) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "all_words1 = []\n",
    "count = 0\n",
    "for sentence in train_text_df['title']:\n",
    "    count += 1\n",
    "    all_words += [word for word in tokenizer.tokenize(sentence) if word not in word_to_idx]\n",
    "\n",
    "for sentence1 in train_text_df['body']:\n",
    "    count += 1\n",
    "    all_words1 += [word1 for word1 in tokenizer.tokenize(sentence) if word1 not in word_to_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train_text_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize data\n",
    "f, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2)\n",
    "sns.distplot(train_text_df['title_len'], hist=True, kde=True, color='b', label='title len', ax=ax1)\n",
    "sns.distplot(train_text_df[train_text_df['body_len'] < 256]['body_len'], hist=True, kde=True, color='r', label='body len', ax=ax2)\n",
    "sns.distplot(train_text_df['title_stopwords'], hist=True, kde=True, color='b', label='title len', ax=ax3)\n",
    "sns.distplot(train_text_df['body_stopwords'], hist=True, kde=True, color='b', label='title len', ax=ax4)\n",
    "ax1.axvline(x=MAX_TITLE_LEN, color='k', linestyle='--', label='max len')\n",
    "ax2.axvline(x=MAX_BODY_LEN, color='k', linestyle='--', label='max len')\n",
    "ax1.set_title('title length histogram'); ax1.legend(loc=1); \n",
    "ax2.set_title('body length histogram'); ax2.legend(loc=1);\n",
    "plt.show()\n",
    "#plt.savefig('question_len_hist.png')\n",
    "print np.mean(train_text_df['body_len'])\n",
    "print np.std(train_text_df['body_len'])\n",
    "print np.mean(train_text_df['title_len'])\n",
    "print np.std(train_text_df['title_len'])\n",
    "\n",
    "print train_text_df.describe()\n",
    "\n",
    "\"\"\"\n",
    "print \"fitting tf-idf vectorizer...\"\n",
    "tic = time()\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer.tokenize, analyzer='word', ngram_range=(1,1))\n",
    "tfidf.fit(train_text_df['title'].tolist() + train_text_df['body'].tolist())\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "\n",
    "vocab = tfidf.vocabulary_\n",
    "print \"vocab size: \", len(vocab)\n",
    "print \"embeddings size: \", embeddings.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.03 sec\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "from time import time\n",
    "tic1 = time()\n",
    "filename = \"../data/data_raw.dat\"\n",
    "dill.load_session(filename)\n",
    "toc1 = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc1 - tic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n",
      "<type 'int'>\n"
     ]
    }
   ],
   "source": [
    "print type(MAX_TITLE_LEN)\n",
    "print type(MAX_BODY_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "[211039]\n",
      "[227387, 413633, 113297, 356390, 256881, 145638, 296272, 318659, 86529, 48563, 53080, 65996, 334032, 517236, 470002, 177348, 502185, 248772, 457062, 339049, 265060, 264524, 15971, 40468, 422021, 177982, 513266, 437762, 318889, 163711, 491359, 72668, 523366, 513514, 119253, 167106, 395661, 411198, 284222, 75399, 352479, 250977, 476895, 384032, 379657, 47506, 214327, 295133, 165345, 278645, 40986, 147959, 498308, 328292, 49304, 518817, 418947, 155658, 522902, 316447, 487482, 100000, 436018, 235318, 314941, 187592, 439142, 26205, 180583, 418425, 41667, 188060, 147132, 68407, 191082, 455272, 486933, 229959, 153150, 101914, 254265, 314232, 384593, 366741, 227996, 384171, 302283, 335076, 452240, 144708, 395995, 221644, 392821, 197651, 404031, 319280, 257320, 38398, 287787, 83851]\n",
      "['system', 'running', 'low', 'graphic', 'mode', 'ubuntu', 'without', 'monitor']\n",
      "['ubuntu', 'desktop', '12', '10', 'want', 'desktop', 'server', 'ca', 'n', 't', 'start', 'without', 'monitor', 'tried', 'ubuntu', 'server', 'got', 'error', 'everything', 'working', 'fine', 'monitor', 'connected', 'disable', 'monitor', 'reboot', 'pc', 'ubuntu', 'start', 'says', 'system', 'running', 'low', 'graphic', 'mode', 'please', 'tell', 'solution', 'app', 'simulate', 'monitor', 'remote', 'controlling', 'thanks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nquery_title_tensor_idx = get_tensor_idx(query_title_tokens, word_to_idx, MAX_TITLE_LEN) \\nquery_body_tensor_idx = get_tensor_idx(query_body_tokens, word_to_idx, MAX_BODY_LEN) \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "query_id = train_idx_df.loc[idx, 'query_id']\n",
    "similar_id_list = map(int, train_idx_df.loc[idx, 'similar_id'].split(' '))\n",
    "random_id_list = map(int, train_idx_df.loc[idx, 'random_id'].split(' '))\n",
    "\n",
    "print query_id\n",
    "print similar_id_list\n",
    "print random_id_list\n",
    "\n",
    "query_title = train_text_df[train_text_df['id'] == query_id].title.tolist() \n",
    "query_body = train_text_df[train_text_df['id'] == query_id].body.tolist()\n",
    "query_title_tokens = tokenizer.tokenize(query_title[0])[:MAX_TITLE_LEN]\n",
    "query_body_tokens = tokenizer.tokenize(query_body[0])[:MAX_BODY_LEN]\n",
    "\n",
    "print query_title_tokens\n",
    "print query_body_tokens\n",
    "'''\n",
    "\n",
    "query_title_tensor_idx = get_tensor_idx(query_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "query_body_tensor_idx = get_tensor_idx(query_body_tokens, word_to_idx, MAX_BODY_LEN) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"generating training, validation, test datasets...\"\n",
    "MAX_TITLE_LEN = int(MAX_TITLE_LEN)\n",
    "MAX_BODY_LEN = int(MAX_BODY_LEN)\n",
    "tic = time()\n",
    "train_data = []\n",
    "for idx in tqdm(range(10)):\n",
    "#for idx in tqdm(range(train_idx_df.shape[0])):\n",
    "    query_id = train_idx_df.loc[idx, 'query_id']\n",
    "    similar_id_list = map(int, train_idx_df.loc[idx, 'similar_id'].split(' '))\n",
    "    random_id_list = map(int, train_idx_df.loc[idx, 'random_id'].split(' '))\n",
    "    \n",
    "    #query title and body tensor ids\n",
    "    query_title = train_text_df[train_text_df['id'] == query_id].title.tolist() \n",
    "    query_body = train_text_df[train_text_df['id'] == query_id].body.tolist()\n",
    "    query_title_tokens = tokenizer.tokenize(query_title[0])[:MAX_TITLE_LEN]\n",
    "    query_body_tokens = tokenizer.tokenize(query_body[0])[:MAX_BODY_LEN]\n",
    "    query_title_tensor_idx = get_tensor_idx(query_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "    query_body_tensor_idx = get_tensor_idx(query_body_tokens, word_to_idx, MAX_BODY_LEN) \n",
    "\n",
    "    for similar_id in similar_id_list:\n",
    "        sample = {}  #reset sample dictionary here\n",
    "        sample['query_title'] = query_title_tensor_idx\n",
    "        sample['query_body'] = query_body_tensor_idx\n",
    "\n",
    "        similar_title = train_text_df[train_text_df['id'] == similar_id].title.tolist() \n",
    "        similar_body = train_text_df[train_text_df['id'] == similar_id].body.tolist()\n",
    "        similar_title_tokens = tokenizer.tokenize(similar_title[0])[:MAX_TITLE_LEN]\n",
    "        similar_body_tokens = tokenizer.tokenize(similar_body[0])[:MAX_BODY_LEN]\n",
    "        similar_title_tensor_idx = get_tensor_idx(similar_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "        similar_body_tensor_idx = get_tensor_idx(similar_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "        sample['similar_title'] = similar_title_tensor_idx\n",
    "        sample['similar_body'] = similar_body_tensor_idx\n",
    "\n",
    "        for ridx, random_id in enumerate(random_id_list):\n",
    "            random_title_name = 'random_title_' + str(ridx)\n",
    "            random_body_name = 'random_body_' + str(ridx)\n",
    "        \n",
    "            random_title = train_text_df[train_text_df['id'] == random_id].title.tolist() \n",
    "            random_body = train_text_df[train_text_df['id'] == random_id].body.tolist()\n",
    "            random_title_tokens = tokenizer.tokenize(random_title[0])[:MAX_TITLE_LEN]\n",
    "            random_body_tokens = tokenizer.tokenize(random_body[0])[:MAX_BODY_LEN]\n",
    "            random_title_tensor_idx = get_tensor_idx(random_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "            random_body_tensor_idx = get_tensor_idx(random_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "            sample[random_title_name] = random_title_tensor_idx\n",
    "            sample[random_body_name] = random_body_tensor_idx\n",
    "        #end for\n",
    "        train_data.append(sample)\n",
    "    #end for\n",
    "#end for\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "num_epochs = 32 \n",
    "batch_size = 8 \n",
    "\n",
    "#model parameters\n",
    "embed_dim = embeddings.shape[1] #200\n",
    "hidden_dim = embed_dim / 2 \n",
    "weight_decay = 1e-3 \n",
    "learning_rate = 1e-3 \n",
    "\n",
    "#RNN architecture\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding_layer.weight.data = torch.from_numpy(embeddings)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=32, batch_first=False)\n",
    "        #TODO: check number of cells\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #[num_layers, minibatch_size, hidden_dim]\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, x_idx):\n",
    "        all_x = self.embedding_layer(x_idx)   \n",
    "        lstm_out, self.hidden = self.lstm(all_x.view(len(x_idx),1,-1), self.hidden)\n",
    "        return self.hidden \n",
    "        \n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = RNN(embed_dim, hidden_dim, len(word_to_idx))\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "    \n",
    "print model\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = nn.MarginRankingLoss(margin=1, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "print \"training...\"\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "        \n",
    "    for batch in tqdm(train_data_loader):\n",
    "      \n",
    "        #TODO: reshape to batches in the middle or use batch_first=True!\n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])\n",
    "        similar_title = Variable(batch['similar_title'])\n",
    "        similar_body = Variable(batch['similar_body'])\n",
    "        #TODO: for loop over random_title and random_body\n",
    "\n",
    "        if use_gpu:\n",
    "            query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "            similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "            #TODO: for loop over random_title and random_body\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        lstm_query_title = model(query_title)\n",
    "        lstm_query_body = model(query_body)\n",
    "        lstm_query = (lstm_query_title + lstm_query_body)/2.0\n",
    "\n",
    "        lstm_similar_title = model(similar_title)\n",
    "        lstm_similar_body = model(similar_body)\n",
    "        lstm_similar = (lstm_similar_title + lstm_similar_body)/2.0\n",
    "\n",
    "        #TODO: for loop over random_title and random_body\n",
    "\n",
    "        score_pos = cosine_similarity(lstm_query, lstm_similar)\n",
    "        #TODO: score_neg\n",
    "\n",
    "        loss = criterion([score_pos, score_neg], 1) #y=1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_train_loss += loss.cpu().data[0]        \n",
    "        \n",
    "    #end for\n",
    "    training_loss.append(running_train_loss)\n",
    "    print \"epoch: %4d, training loss: %.4f\" %(epoch+1, running_train_loss)\n",
    "    \n",
    "    torch.save(model, SAVE_PATH)\n",
    "#end for\n",
    "\n",
    "\"\"\"\n",
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(training_loss, label='Adam')\n",
    "plt.title(\"LSTM Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./figures/training_loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(validation_loss, label='Adam')\n",
    "plt.title(\"LSTM Model Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('./figures/validation_loss.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config['DEFAULT'] = {'ServerAliveInterval': '45',\n",
    "                     'Compression': 'yes',\n",
    "                     'CompressionLevel': '9'}\n",
    "config['bitbucket.org'] = {}\n",
    "config['bitbucket.org']['User'] = 'hg'\n",
    "config['topsecret.server.com'] = {}\n",
    "topsecret = config['topsecret.server.com']\n",
    "topsecret['Port'] = '50022'     # mutates the parser\n",
    "topsecret['ForwardX11'] = 'no'  # same here\n",
    "config['DEFAULT']['ForwardX11'] = 'yes'\n",
    "with open('example.ini', 'w') as configfile:\n",
    "  config.write(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
