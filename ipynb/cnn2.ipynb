{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled data...\n",
      "elapsed time: 60.41 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ranking_metrics import compute_mrr, precision_at_k, compute_map\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "DATA_FILE_NAME = config.get('paths', 'extracted_data_file_name')\n",
    "TRAIN_TEST_FILE_NAME = config.get('paths', 'train_test_file_name')\n",
    "SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE')) \n",
    "\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "\n",
    "data_filename = SAVE_PATH + DATA_FILE_NAME\n",
    "train_test_filename = SAVE_PATH + TRAIN_TEST_FILE_NAME\n",
    "\n",
    "print \"loading pickled data...\"\n",
    "tic = time()\n",
    "with open(data_filename) as f:  \n",
    "    train_text_df, train_idx_df, dev_idx_df, test_idx_df, embeddings, word_to_idx = pickle.load(f)\n",
    "f.close()\n",
    "with open(train_test_filename) as f:\n",
    "    train_data, val_data, test_data = pickle.load(f)\n",
    "f.close()\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 100, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 100, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "training...\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9621  0.9415  0.9465  0.9624  0.9668  0.9639  0.9668  0.9589  0.9559  0.9505\n",
      " 0.9795  0.9415  0.9465  0.9624  0.9668  0.9639  0.9668  0.9589  0.9559  0.9505\n",
      " 0.9492  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9450  0.9214  0.9060  0.9234  0.9141  0.9122  0.9036  0.9037  0.9300  0.9222\n",
      " 0.9385  0.9280  0.9089  0.9163  0.8936  0.9048  0.9171  0.9190  0.9182  0.9267\n",
      " 0.9184  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9550  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9734  0.9513  0.9327  0.9553  0.9564  0.9539  0.9418  0.9520  0.9506  0.9425\n",
      " 0.9298  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9526  0.9602  0.9469  0.9478  0.9519  0.9542  0.9520  0.9577  0.9339  0.9463\n",
      " 0.9429  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9519  0.9384  0.9434  0.9327  0.9385  0.9429  0.9342  0.9425  0.9410  0.9505\n",
      " 0.9584  0.9500  0.9612  0.9611  0.9482  0.9625  0.9559  0.9557  0.9478  0.9642\n",
      " 0.9646  0.9573  0.9532  0.9587  0.9523  0.9564  0.9610  0.9597  0.9410  0.9359\n",
      " 0.9379  0.9496  0.9457  0.9468  0.9227  0.9410  0.9430  0.9284  0.9321  0.9416\n",
      " 0.9379  0.9397  0.9258  0.9305  0.9425  0.9448  0.9338  0.9347  0.9397  0.9270\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.9562  0.9606  0.9415  0.9681  0.9416  0.9378  0.9563  0.9517  0.9471  0.9539\n",
      " 0.9562  0.9606  0.9415  0.9681  0.9416  0.9378  0.9563  0.9517  0.9471  0.9539\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9220  0.9134  0.9136  0.9219  0.9117  0.9181  0.9167  0.9036  0.9223  0.9208\n",
      " 0.9150  0.9094  0.9370  0.9190  0.8934  0.9065  0.9210  0.9047  0.8859  0.9068\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9412  0.9500  0.9426  0.9332  0.9508  0.9546  0.9439  0.9569  0.9492  0.9411\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9597  0.9616  0.9673  0.9531  0.9585  0.9560  0.9294  0.9619  0.9534  0.9592\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9411  0.9386  0.9505  0.9434  0.9343  0.9441  0.9471  0.9421  0.9485  0.9490\n",
      " 0.9516  0.9519  0.9531  0.9464  0.9522  0.9589  0.9144  0.9495  0.9548  0.9593\n",
      " 0.9568  0.9528  0.9452  0.9583  0.9551  0.9581  0.9534  0.9503  0.9538  0.9519\n",
      " 0.9512  0.9393  0.9420  0.9374  0.9333  0.9463  0.9476  0.9400  0.9398  0.9392\n",
      " 0.9395  0.9361  0.9352  0.9398  0.9359  0.9437  0.9416  0.9356  0.9339  0.9242\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.9669  0.9739  0.9299  0.9616  0.9648  0.9645  0.9610  0.9658  0.9418  0.9636\n",
      " 0.9669  0.9739  0.9299  0.9616  0.9648  0.9645  0.9610  0.9658  0.9418  0.9636\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9290  0.9090  0.9244  0.9128  0.9226  0.9226  0.9191  0.9252  0.9143  0.9082\n",
      " 0.8860  0.9170  0.9123  0.9166  0.9329  0.9201  0.9219  0.9115  0.9212  0.9367\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9523  0.9571  0.9520  0.9426  0.9443  0.9328  0.9371  0.9544  0.9580  0.9570\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9469  0.9543  0.9631  0.9465  0.9667  0.9434  0.9587  0.9374  0.9542  0.9575\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9345  0.9422  0.9524  0.9193  0.9357  0.9269  0.9370  0.9386  0.9280  0.9498\n",
      " 0.9445  0.9671  0.9404  0.9449  0.9486  0.9637  0.9604  0.9581  0.9457  0.9454\n",
      " 0.9623  0.9607  0.9528  0.9613  0.9474  0.9560  0.9563  0.9506  0.9582  0.9523\n",
      " 0.9364  0.9425  0.9451  0.9373  0.9251  0.9423  0.9475  0.9457  0.9396  0.9560\n",
      " 0.9461  0.9415  0.9377  0.9387  0.9402  0.9493  0.9520  0.9423  0.9341  0.9469\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.9414  0.9627  0.9594  0.9573  0.9397  0.9589  0.9662  0.9604  0.9471  0.9688\n",
      " 0.9414  0.9627  0.9594  0.9573  0.9397  0.9589  0.9662  0.9604  0.9471  0.9688\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9127  0.9181  0.9174  0.9245  0.9094  0.9178  0.9167  0.9132  0.9137  0.9199\n",
      " 0.9288  0.9352  0.9132  0.9082  0.9068  0.9059  0.9184  0.9211  0.9286  0.9235\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9570  0.9573  0.9277  0.9564  0.9551  0.9409  0.9462  0.9523  0.9521  0.9571\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9625  0.9531  0.9486  0.9634  0.9539  0.9428  0.9278  0.9582  0.9498  0.9568\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9388  0.9452  0.9294  0.9439  0.9339  0.9479  0.9291  0.9333  0.9482  0.9386\n",
      " 0.9466  0.9529  0.9635  0.9628  0.9527  0.9543  0.9565  0.9507  0.9466  0.9635\n",
      " 0.9556  0.9566  0.9391  0.9545  0.9564  0.9525  0.9630  0.9541  0.9601  0.9588\n",
      " 0.9350  0.9381  0.9405  0.9422  0.9376  0.9382  0.9388  0.9407  0.9347  0.9453\n",
      " 0.9430  0.9429  0.9468  0.9412  0.9287  0.9180  0.9439  0.9445  0.9407  0.9283\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.9619\n",
      " 0.9619\n",
      " 0.9388\n",
      " 0.9233\n",
      " 0.9251\n",
      " 0.9388\n",
      " 0.9388\n",
      " 0.9449\n",
      " 0.9388\n",
      " 0.9555\n",
      " 0.9388\n",
      " 0.9417\n",
      " 0.9466\n",
      " 0.9528\n",
      " 0.9388\n",
      " 0.9423\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:37<00:37, 37.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9644  0.9547  0.9553  0.9572  0.9592  0.9618  0.9531  0.9423  0.9494  0.9435\n",
      " 0.9706  0.9449  0.9570  0.9666  0.9511  0.9459  0.9333  0.9597  0.9605  0.9443\n",
      " 0.9658  0.9509  0.9694  0.9273  0.9601  0.9690  0.9519  0.9574  0.9511  0.9267\n",
      " 0.9609  0.9277  0.9319  0.9339  0.9347  0.9304  0.9360  0.9184  0.9245  0.9399\n",
      " 0.9649  0.9509  0.9694  0.9273  0.9601  0.9690  0.9519  0.9574  0.9511  0.9267\n",
      " 0.9627  0.9392  0.9357  0.8998  0.9544  0.9572  0.9392  0.9523  0.9482  0.9592\n",
      " 0.9424  0.9199  0.9165  0.9091  0.8859  0.9167  0.9272  0.8871  0.8876  0.9131\n",
      " 0.9632  0.9639  0.9535  0.9545  0.9240  0.9209  0.9489  0.9662  0.9523  0.9447\n",
      " 0.9550  0.9327  0.9333  0.9332  0.9359  0.9161  0.9304  0.9249  0.9268  0.9278\n",
      " 0.9667  0.9298  0.9382  0.9363  0.9481  0.9176  0.9325  0.9436  0.9256  0.9448\n",
      " 0.9498  0.9266  0.9318  0.9207  0.9237  0.9280  0.9217  0.9314  0.9290  0.9404\n",
      " 0.9622  0.9327  0.9333  0.9332  0.9359  0.9161  0.9304  0.9249  0.9268  0.9278\n",
      " 0.9287  0.9199  0.9165  0.9091  0.8859  0.9167  0.9272  0.8871  0.8876  0.9131\n",
      " 0.9559  0.9509  0.9694  0.9273  0.9601  0.9690  0.9519  0.9574  0.9511  0.9267\n",
      " 0.9356  0.9199  0.9165  0.9091  0.8859  0.9167  0.9272  0.8871  0.8876  0.9131\n",
      " 0.9331  0.9199  0.9165  0.9091  0.8859  0.9167  0.9272  0.8871  0.8876  0.9131\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.9484  0.9572  0.9460  0.9340  0.9555  0.9582  0.9614  0.9380  0.9468  0.9559\n",
      " 0.9374  0.9666  0.9618  0.9473  0.9393  0.9444  0.9538  0.9409  0.9635  0.9381\n",
      " 0.9256  0.9426  0.9601  0.9587  0.9437  0.9554  0.9404  0.9567  0.9517  0.9608\n",
      " 0.9336  0.9306  0.9370  0.9419  0.9396  0.9228  0.9344  0.9302  0.9324  0.9350\n",
      " 0.9256  0.9426  0.9601  0.9587  0.9437  0.9554  0.9404  0.9567  0.9517  0.9608\n",
      " 0.9381  0.9457  0.9304  0.9456  0.9511  0.9447  0.9414  0.9424  0.9472  0.9508\n",
      " 0.9209  0.9092  0.9084  0.9069  0.9115  0.9188  0.9238  0.9024  0.9029  0.9018\n",
      " 0.9537  0.9391  0.9521  0.9316  0.9489  0.9552  0.9420  0.9555  0.9583  0.9472\n",
      " 0.9248  0.9275  0.9320  0.9329  0.9322  0.9274  0.9261  0.9206  0.9140  0.9321\n",
      " 0.9378  0.9463  0.9356  0.9317  0.9387  0.9345  0.8972  0.9398  0.9451  0.9477\n",
      " 0.9275  0.9235  0.9423  0.9295  0.9199  0.9374  0.9367  0.9299  0.9415  0.9403\n",
      " 0.9248  0.9275  0.9320  0.9329  0.9322  0.9274  0.9261  0.9206  0.9140  0.9321\n",
      " 0.9209  0.9092  0.9084  0.9069  0.9115  0.9188  0.9238  0.9024  0.9029  0.9018\n",
      " 0.9256  0.9426  0.9601  0.9587  0.9437  0.9554  0.9404  0.9567  0.9517  0.9608\n",
      " 0.9209  0.9092  0.9084  0.9069  0.9115  0.9188  0.9238  0.9024  0.9029  0.9018\n",
      " 0.9209  0.9092  0.9084  0.9069  0.9115  0.9188  0.9238  0.9024  0.9029  0.9018\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.9680  0.9612  0.9412  0.9656  0.9469  0.9566  0.9496  0.9483  0.9363  0.9581\n",
      " 0.9393  0.9489  0.9583  0.9534  0.9636  0.9509  0.9520  0.9454  0.9265  0.9668\n",
      " 0.9565  0.9460  0.8961  0.9579  0.9533  0.9555  0.9473  0.9346  0.9644  0.9566\n",
      " 0.9364  0.9410  0.9360  0.9361  0.9355  0.9367  0.9422  0.9355  0.9366  0.9403\n",
      " 0.9565  0.9460  0.8961  0.9579  0.9533  0.9555  0.9473  0.9346  0.9644  0.9566\n",
      " 0.9531  0.9657  0.9398  0.9550  0.9538  0.9582  0.9491  0.9472  0.9539  0.9462\n",
      " 0.9072  0.9256  0.9063  0.9030  0.8795  0.9172  0.9127  0.9095  0.9081  0.9240\n",
      " 0.9647  0.9469  0.9456  0.9494  0.9266  0.9473  0.9539  0.9392  0.9563  0.9389\n",
      " 0.9143  0.9327  0.9340  0.9300  0.9278  0.9259  0.9387  0.9291  0.9252  0.9296\n",
      " 0.9299  0.9127  0.9478  0.9269  0.9165  0.9396  0.9430  0.9342  0.9317  0.9324\n",
      " 0.9225  0.9274  0.9408  0.9056  0.9215  0.9077  0.9231  0.9243  0.9137  0.9400\n",
      " 0.9143  0.9327  0.9340  0.9300  0.9278  0.9259  0.9387  0.9291  0.9252  0.9296\n",
      " 0.9072  0.9256  0.9063  0.9030  0.8795  0.9172  0.9127  0.9095  0.9081  0.9240\n",
      " 0.9565  0.9460  0.8961  0.9579  0.9533  0.9555  0.9473  0.9346  0.9644  0.9566\n",
      " 0.9072  0.9256  0.9063  0.9030  0.8795  0.9172  0.9127  0.9095  0.9081  0.9240\n",
      " 0.9072  0.9256  0.9063  0.9030  0.8795  0.9172  0.9127  0.9095  0.9081  0.9240\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.9576  0.9531  0.9526  0.9478  0.9569  0.9517  0.9378  0.9465  0.9569  0.9597\n",
      " 0.9492  0.9480  0.9537  0.9483  0.9538  0.9342  0.9556  0.9559  0.9534  0.9620\n",
      " 0.9352  0.9480  0.9553  0.9598  0.9521  0.9583  0.9341  0.9604  0.9550  0.9453\n",
      " 0.9346  0.9236  0.9221  0.9275  0.9452  0.9408  0.9285  0.9377  0.9266  0.9233\n",
      " 0.9352  0.9480  0.9553  0.9598  0.9521  0.9583  0.9341  0.9604  0.9550  0.9453\n",
      " 0.9490  0.9561  0.9448  0.9590  0.9378  0.9415  0.9530  0.9357  0.9503  0.9268\n",
      " 0.8909  0.9068  0.9113  0.9089  0.9084  0.8957  0.9082  0.9083  0.9002  0.9093\n",
      " 0.9365  0.9381  0.9451  0.9486  0.9397  0.9597  0.9500  0.9496  0.9345  0.9524\n",
      " 0.9251  0.9376  0.9290  0.9316  0.9266  0.9275  0.9338  0.9293  0.9449  0.9266\n",
      " 0.9284  0.9447  0.9428  0.9370  0.9367  0.9256  0.9409  0.9340  0.9473  0.9244\n",
      " 0.9261  0.9331  0.9158  0.9323  0.9224  0.9398  0.9172  0.9202  0.9387  0.9243\n",
      " 0.9251  0.9376  0.9290  0.9316  0.9266  0.9275  0.9338  0.9293  0.9449  0.9266\n",
      " 0.8909  0.9068  0.9113  0.9089  0.9084  0.8957  0.9082  0.9083  0.9002  0.9093\n",
      " 0.9352  0.9480  0.9553  0.9598  0.9521  0.9583  0.9341  0.9604  0.9550  0.9453\n",
      " 0.8909  0.9068  0.9113  0.9089  0.9084  0.8957  0.9082  0.9083  0.9002  0.9093\n",
      " 0.8909  0.9068  0.9113  0.9089  0.9084  0.8957  0.9082  0.9083  0.9002  0.9093\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.9579\n",
      " 0.9311\n",
      " 0.9504\n",
      " 0.9234\n",
      " 0.9504\n",
      " 0.9340\n",
      " 0.9014\n",
      " 0.9202\n",
      " 0.9306\n",
      " 0.9377\n",
      " 0.9285\n",
      " 0.9306\n",
      " 0.9014\n",
      " 0.9504\n",
      " 0.9014\n",
      " 0.9014\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:13<00:00, 37.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, training loss: 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9753  0.9106  0.9152  0.9035  0.9068  0.9112  0.9056  0.9184  0.9153  0.9276\n",
      " 0.9761  0.9480  0.9440  0.9456  0.9522  0.9525  0.9408  0.9294  0.9360  0.9291\n",
      " 0.9806  0.9109  0.9148  0.9389  0.9505  0.9635  0.9482  0.9385  0.9290  0.9304\n",
      " 0.9396  0.8829  0.8780  0.8651  0.8415  0.8872  0.9058  0.8394  0.8378  0.8785\n",
      " 0.9546  0.8829  0.8780  0.8651  0.8415  0.8872  0.9058  0.8394  0.8378  0.8785\n",
      " 0.9892  0.9109  0.9148  0.9389  0.9505  0.9635  0.9482  0.9385  0.9290  0.9304\n",
      " 0.9651  0.9011  0.9006  0.9014  0.9081  0.8801  0.9016  0.8942  0.9012  0.9014\n",
      " 0.9531  0.9478  0.9524  0.9415  0.9412  0.9150  0.9380  0.9259  0.9383  0.9395\n",
      " 0.9734  0.9273  0.9237  0.8811  0.9438  0.9461  0.9266  0.9418  0.9369  0.9507\n",
      " 0.9577  0.9106  0.9152  0.9035  0.9068  0.9112  0.9056  0.9184  0.9153  0.9276\n",
      " 0.9670  0.9430  0.9644  0.9096  0.9503  0.9653  0.9427  0.9496  0.9393  0.9141\n",
      " 0.9779  0.9360  0.9517  0.9633  0.9443  0.9365  0.9256  0.9538  0.9562  0.9375\n",
      " 0.9837  0.9434  0.9283  0.9284  0.9394  0.9315  0.9324  0.9443  0.9093  0.9260\n",
      " 0.9750  0.9430  0.9644  0.9096  0.9503  0.9653  0.9427  0.9496  0.9393  0.9141\n",
      " 0.9822  0.9326  0.9083  0.9375  0.9393  0.9358  0.9176  0.9323  0.9313  0.9207\n",
      " 0.9713  0.9011  0.9006  0.9014  0.9081  0.8801  0.9016  0.8942  0.9012  0.9014\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.9125  0.9017  0.9307  0.9127  0.9002  0.9273  0.9242  0.9156  0.9323  0.9295\n",
      " 0.9433  0.9470  0.9342  0.9235  0.9475  0.9508  0.9521  0.9261  0.9349  0.9449\n",
      " 0.9282  0.9384  0.9164  0.9536  0.9033  0.9106  0.9371  0.9228  0.9172  0.9230\n",
      " 0.8872  0.8800  0.8674  0.8721  0.8864  0.8873  0.8947  0.8590  0.8635  0.8578\n",
      " 0.8872  0.8800  0.8674  0.8721  0.8864  0.8873  0.8947  0.8590  0.8635  0.8578\n",
      " 0.9282  0.9384  0.9164  0.9536  0.9033  0.9106  0.9371  0.9228  0.9172  0.9230\n",
      " 0.8873  0.9010  0.9053  0.9000  0.9025  0.8949  0.9016  0.8933  0.8778  0.8998\n",
      " 0.9488  0.8334  0.9438  0.9285  0.9423  0.9370  0.9413  0.9320  0.9276  0.9254\n",
      " 0.9243  0.9323  0.9144  0.9337  0.9390  0.9343  0.9267  0.9300  0.9369  0.9369\n",
      " 0.9125  0.9017  0.9307  0.9127  0.9002  0.9273  0.9242  0.9156  0.9323  0.9295\n",
      " 0.9101  0.9316  0.9519  0.9508  0.9329  0.9464  0.9265  0.9471  0.9411  0.9530\n",
      " 0.9267  0.9634  0.9579  0.9410  0.9327  0.9380  0.9479  0.9320  0.9594  0.9286\n",
      " 0.9413  0.9435  0.9481  0.9364  0.9428  0.9391  0.9105  0.9439  0.9378  0.9389\n",
      " 0.9101  0.9316  0.9519  0.9508  0.9329  0.9464  0.9265  0.9471  0.9411  0.9530\n",
      " 0.9152  0.9267  0.9217  0.9110  0.9349  0.9372  0.9190  0.9375  0.9289  0.9155\n",
      " 0.8873  0.9010  0.9053  0.9000  0.9025  0.8949  0.9016  0.8933  0.8778  0.8998\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.9058  0.9083  0.9301  0.8876  0.9041  0.8864  0.9054  0.9046  0.8961  0.9281\n",
      " 0.9665  0.9525  0.9276  0.9595  0.9367  0.9489  0.9383  0.9409  0.9268  0.9496\n",
      " 0.9555  0.9691  0.8939  0.9501  0.9474  0.9517  0.9361  0.9568  0.9079  0.9482\n",
      " 0.8707  0.9026  0.8625  0.8634  0.8278  0.8887  0.8765  0.8664  0.8715  0.8894\n",
      " 0.8707  0.9026  0.8625  0.8634  0.8278  0.8887  0.8765  0.8664  0.8715  0.8894\n",
      " 0.9555  0.9691  0.8939  0.9501  0.9474  0.9517  0.9361  0.9568  0.9079  0.9482\n",
      " 0.8763  0.9011  0.9004  0.8986  0.8925  0.8959  0.9084  0.9025  0.8885  0.8975\n",
      " 0.9413  0.9373  0.9300  0.9414  0.9391  0.9410  0.9380  0.9359  0.9422  0.9232\n",
      " 0.9472  0.9609  0.9276  0.9440  0.9448  0.9471  0.9391  0.9367  0.9427  0.9323\n",
      " 0.9058  0.9083  0.9301  0.8876  0.9041  0.8864  0.9054  0.9046  0.8961  0.9281\n",
      " 0.9470  0.9353  0.8792  0.9483  0.9403  0.9469  0.9378  0.9210  0.9580  0.9464\n",
      " 0.9310  0.9411  0.9528  0.9481  0.9606  0.9438  0.9461  0.9383  0.9127  0.9652\n",
      " 0.9211  0.9329  0.9511  0.9249  0.9540  0.9196  0.9421  0.9137  0.9320  0.9374\n",
      " 0.9470  0.9353  0.8792  0.9483  0.9403  0.9469  0.9378  0.9210  0.9580  0.9464\n",
      " 0.9290  0.9393  0.9349  0.9228  0.9234  0.9131  0.9154  0.9310  0.9382  0.9401\n",
      " 0.8763  0.9011  0.9004  0.8986  0.8925  0.8959  0.9084  0.9025  0.8885  0.8975\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.9079  0.9198  0.8973  0.9192  0.9054  0.9284  0.9025  0.9040  0.9279  0.9079\n",
      " 0.9505  0.9415  0.9412  0.9357  0.9476  0.9382  0.9298  0.9372  0.9463  0.9520\n",
      " 0.9033  0.9377  0.9393  0.9295  0.9065  0.9318  0.9549  0.9406  0.9273  0.9537\n",
      " 0.8415  0.8687  0.8776  0.8704  0.8718  0.8482  0.8745  0.8683  0.8626  0.8648\n",
      " 0.8415  0.8687  0.8776  0.8704  0.8718  0.8482  0.8745  0.8683  0.8626  0.8648\n",
      " 0.9033  0.9377  0.9393  0.9295  0.9065  0.9318  0.9549  0.9406  0.9273  0.9537\n",
      " 0.8948  0.9114  0.8965  0.8981  0.8913  0.8958  0.9012  0.8979  0.9305  0.8971\n",
      " 0.9369  0.9302  0.9440  0.9430  0.9356  0.9279  0.9397  0.9354  0.9432  0.9251\n",
      " 0.9367  0.9470  0.9343  0.9507  0.9255  0.9280  0.9416  0.9221  0.9384  0.9123\n",
      " 0.9079  0.9198  0.8973  0.9192  0.9054  0.9284  0.9025  0.9040  0.9279  0.9079\n",
      " 0.9231  0.9340  0.9478  0.9519  0.9399  0.9515  0.9156  0.9509  0.9442  0.9315\n",
      " 0.9434  0.9404  0.9478  0.9430  0.9473  0.9266  0.9485  0.9514  0.9484  0.9578\n",
      " 0.9434  0.9345  0.9289  0.9502  0.9322  0.9239  0.9089  0.9380  0.9239  0.9415\n",
      " 0.9231  0.9340  0.9478  0.9519  0.9399  0.9515  0.9156  0.9509  0.9442  0.9315\n",
      " 0.9385  0.9424  0.9006  0.9406  0.9373  0.9152  0.9258  0.9366  0.9316  0.9399\n",
      " 0.8948  0.9114  0.8965  0.8981  0.8913  0.8958  0.9012  0.8979  0.9305  0.8971\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.9136\n",
      " 0.9484\n",
      " 0.9361\n",
      " 0.8625\n",
      " 0.8625\n",
      " 0.9361\n",
      " 0.8986\n",
      " 0.9383\n",
      " 0.9194\n",
      " 0.9136\n",
      " 0.9375\n",
      " 0.9168\n",
      " 0.9401\n",
      " 0.9375\n",
      " 0.9249\n",
      " 0.8986\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:19<00:19, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9261  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9088  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9655  0.8672  0.8211  0.8671  0.8600  0.8401  0.8106  0.8294  0.8848  0.8639\n",
      " 0.9574  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9726  0.9504  0.9377  0.9430  0.8952  0.8873  0.9286  0.9610  0.9364  0.9222\n",
      " 0.9737  0.8985  0.9046  0.9081  0.9270  0.8798  0.9053  0.9193  0.8923  0.9198\n",
      " 0.9501  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9416  0.9295  0.9533  0.9478  0.9449  0.9203  0.9239  0.9550  0.9397  0.9531\n",
      " 0.9372  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9802  0.9322  0.9444  0.9312  0.9195  0.9372  0.9301  0.9264  0.9173  0.9435\n",
      " 0.9520  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9806  0.9355  0.9259  0.9326  0.9288  0.9305  0.9459  0.9420  0.9049  0.9037\n",
      " 0.9728  0.9303  0.9548  0.8818  0.9339  0.9577  0.9275  0.9373  0.9198  0.8942\n",
      " 0.9272  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9158  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      " 0.9291  0.8426  0.8330  0.8161  0.7908  0.8516  0.8795  0.7842  0.7822  0.8403\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.8708  0.8446  0.8459  0.8741  0.8288  0.8524  0.8579  0.8106  0.8607  0.8562\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.9422  0.9141  0.9323  0.9053  0.9315  0.9422  0.9219  0.9381  0.9485  0.9375\n",
      " 0.9101  0.9277  0.9074  0.9006  0.9128  0.9079  0.8578  0.9113  0.9206  0.9245\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.9482  0.9475  0.9245  0.9507  0.9281  0.9446  0.9313  0.9572  0.9457  0.9455\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.9302  0.9260  0.9254  0.9303  0.9234  0.9332  0.8822  0.9178  0.9272  0.9458\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.9354  0.9340  0.9147  0.9383  0.9340  0.9396  0.9316  0.9224  0.9319  0.9257\n",
      " 0.8869  0.9137  0.9392  0.9371  0.9151  0.9325  0.9040  0.9312  0.9238  0.9397\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      " 0.8499  0.8423  0.8236  0.8297  0.8492  0.8485  0.8584  0.8101  0.8152  0.8096\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.8855  0.8402  0.8753  0.8400  0.8534  0.8530  0.8640  0.8726  0.8455  0.8368\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.9568  0.9334  0.9209  0.9277  0.8990  0.9307  0.9383  0.9186  0.9463  0.9108\n",
      " 0.8997  0.8745  0.9267  0.8945  0.8771  0.9117  0.9165  0.9067  0.9036  0.9049\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.9432  0.9537  0.9272  0.9453  0.9375  0.9495  0.9280  0.9476  0.9494  0.9494\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.9127  0.9420  0.9056  0.9178  0.9248  0.9405  0.9414  0.9378  0.9138  0.9155\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.9470  0.9413  0.9193  0.9399  0.9074  0.9318  0.9347  0.9255  0.9451  0.9296\n",
      " 0.9320  0.9181  0.8518  0.9326  0.9206  0.9325  0.9233  0.9003  0.9478  0.9306\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      " 0.8265  0.8702  0.8157  0.8171  0.7700  0.8523  0.8342  0.8172  0.8300  0.8522\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.8519  0.8610  0.8599  0.8602  0.8366  0.8589  0.8416  0.8358  0.8524  0.8631\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.9095  0.9179  0.9278  0.9292  0.9165  0.9516  0.9288  0.9301  0.9100  0.9373\n",
      " 0.8954  0.9214  0.9169  0.9104  0.9078  0.8911  0.9157  0.9081  0.9267  0.8900\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.9466  0.9435  0.9474  0.9442  0.9316  0.9458  0.9209  0.9538  0.8959  0.9511\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.9211  0.9303  0.9436  0.9518  0.9302  0.9318  0.9337  0.9210  0.9166  0.9460\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.9368  0.9391  0.9111  0.9333  0.9313  0.9221  0.9475  0.9331  0.9445  0.9430\n",
      " 0.9047  0.9136  0.9343  0.9398  0.9207  0.9396  0.8884  0.9357  0.9262  0.9108\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      " 0.7848  0.8287  0.8375  0.8270  0.8309  0.7938  0.8350  0.8251  0.8178  0.8155\n",
      "\n",
      "Columns 40 to 40 \n",
      " 0.8150\n",
      " 0.8150\n",
      " 0.8603\n",
      " 0.8150\n",
      " 0.8845\n",
      " 0.9128\n",
      " 0.8150\n",
      " 0.9538\n",
      " 0.8150\n",
      " 0.9201\n",
      " 0.8150\n",
      " 0.9221\n",
      " 0.9172\n",
      " 0.8150\n",
      " 0.8150\n",
      " 0.8150\n",
      "[torch.FloatTensor of size 16x41]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:36<00:00, 18.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    2, training loss: 0.6498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint \"loading pre-trained model...\"\\nmodel = torch.load(SAVE_PATH)\\nif use_gpu:\\n    print \"found CUDA GPU...\"\\n    model = model.cuda()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training parameters\n",
    "num_epochs = 2 #16\n",
    "batch_size = 16 \n",
    "\n",
    "#model parameters\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 100  #TODO: tune\n",
    "kernel_sizes = range(2,6)\n",
    "learning_rate = 1e-3 \n",
    "weight_decay = 1e-5\n",
    "\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\n",
    "print model\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "learning_rate_schedule = [] \n",
    "training_loss, validation_loss, test_loss = [], [], []\n",
    "\n",
    "print \"training...\"\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "        \n",
    "    for batch in tqdm(train_data_loader):\n",
    "    \n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])\n",
    "        similar_title = Variable(batch['similar_title'])\n",
    "        similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "        random_title_list = []\n",
    "        random_body_list = []\n",
    "        for ridx in range(NUM_NEGATIVE): #number of random negative examples\n",
    "            random_title_name = 'random_title_' + str(ridx)\n",
    "            random_body_name = 'random_body_' + str(ridx)\n",
    "            random_title_list.append(Variable(batch[random_title_name]))\n",
    "            random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "        if use_gpu:\n",
    "            query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "            similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "            random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "            random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cnn_query_title = model(query_title)\n",
    "        cnn_query_body = model(query_body)\n",
    "        cnn_query = (cnn_query_title + cnn_query_body)/2.0\n",
    "\n",
    "        cnn_similar_title = model(similar_title)\n",
    "        cnn_similar_body = model(similar_body)\n",
    "        cnn_similar = (cnn_similar_title + cnn_similar_body)/2.0\n",
    "\n",
    "        cnn_random_list = []\n",
    "        for ridx in range(len(random_title_list)):\n",
    "            cnn_random_title = model(random_title_list[ridx])\n",
    "            cnn_random_body = model(random_body_list[ridx])\n",
    "            cnn_random = (cnn_random_title + cnn_random_body)/2.0\n",
    "            cnn_random_list.append(cnn_random)\n",
    "        #end for\n",
    "           \n",
    "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score_pos = cosine_similarity(cnn_query, cnn_similar)\n",
    "\n",
    "        score_list = []\n",
    "        score_list.append(score_pos)\n",
    "        for ridx in range(len(cnn_random_list)):\n",
    "            score_neg = cosine_similarity(cnn_query, cnn_random_list[ridx])\n",
    "            score_list.append(score_neg)\n",
    "\n",
    "        X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "        print X_scores\n",
    "        y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "        if use_gpu:\n",
    "            y_targets = y_targets.cuda()\n",
    "        loss = criterion(X_scores, y_targets) #y_target=0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_train_loss += loss.cpu().data[0]        \n",
    "        \n",
    "    #end for\n",
    "\n",
    "    training_loss.append(running_train_loss)\n",
    "    learning_rate_schedule.append(scheduler.get_lr())\n",
    "    print \"epoch: %4d, training loss: %.4f\" %(epoch+1, running_train_loss)\n",
    "    \n",
    "    torch.save(model, SAVE_PATH + SAVE_NAME)\n",
    "\n",
    "    #early stopping\n",
    "    patience = 4\n",
    "    min_delta = 0.1\n",
    "    if epoch == 0:\n",
    "        patience_cnt = 0\n",
    "    elif epoch > 0 and training_loss[epoch-1] - training_loss[epoch] > min_delta:\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "\n",
    "    if patience_cnt > patience:\n",
    "        print \"early stopping...\"\n",
    "        break\n",
    "#end for\n",
    "\"\"\"\n",
    "print \"loading pre-trained model...\"\n",
    "model = torch.load(SAVE_PATH)\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring test questions...\n",
      "here!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:03<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9475\n",
      " 0.9597\n",
      " 0.9268\n",
      " 0.9597\n",
      " 0.9587\n",
      " 0.9488\n",
      " 0.9318\n",
      " 0.9811\n",
      " 0.9560\n",
      " 0.9441\n",
      " 0.9574\n",
      " 0.9556\n",
      " 0.9496\n",
      " 0.9635\n",
      " 0.9411\n",
      " 0.9426\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9371\n",
      " 0.9567\n",
      " 0.9806\n",
      " 0.9457\n",
      " 0.9532\n",
      " 0.9634\n",
      " 0.9439\n",
      " 0.9590\n",
      " 0.9528\n",
      " 0.9332\n",
      " 0.9440\n",
      " 0.9572\n",
      " 0.9582\n",
      " 0.9401\n",
      " 0.9571\n",
      " 0.9437\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9271\n",
      " 0.9363\n",
      " 0.9626\n",
      " 0.9423\n",
      " 0.9533\n",
      " 0.9524\n",
      " 0.9372\n",
      " 0.9616\n",
      " 0.9573\n",
      " 0.9460\n",
      " 0.9368\n",
      " 0.9520\n",
      " 0.9548\n",
      " 0.9388\n",
      " 0.9510\n",
      " 0.9510\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9515\n",
      " 0.9599\n",
      " 0.9756\n",
      " 0.9404\n",
      " 0.9525\n",
      " 0.9634\n",
      " 0.9482\n",
      " 0.9472\n",
      " 0.9603\n",
      " 0.9467\n",
      " 0.9396\n",
      " 0.9501\n",
      " 0.9553\n",
      " 0.9477\n",
      " 0.9517\n",
      " 0.9413\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.8974\n",
      " 0.9568\n",
      " 0.9772\n",
      " 0.9383\n",
      " 0.9521\n",
      " 0.9432\n",
      " 0.9407\n",
      " 0.9353\n",
      " 0.9553\n",
      " 0.9406\n",
      " 0.9404\n",
      " 0.9656\n",
      " 0.9476\n",
      " 0.9641\n",
      " 0.9164\n",
      " 0.9398\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9502\n",
      " 0.9463\n",
      " 0.9793\n",
      " 0.9382\n",
      " 0.9587\n",
      " 0.9639\n",
      " 0.9320\n",
      " 0.9661\n",
      " 0.9487\n",
      " 0.9021\n",
      " 0.9418\n",
      " 0.9559\n",
      " 0.9533\n",
      " 0.9430\n",
      " 0.9349\n",
      " 0.9315\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9300\n",
      " 0.9578\n",
      " 0.9793\n",
      " 0.9571\n",
      " 0.9540\n",
      " 0.9622\n",
      " 0.9271\n",
      " 0.9627\n",
      " 0.9552\n",
      " 0.8934\n",
      " 0.9415\n",
      " 0.9564\n",
      " 0.9468\n",
      " 0.9386\n",
      " 0.9398\n",
      " 0.9497\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9443\n",
      " 0.9581\n",
      " 0.9651\n",
      " 0.9402\n",
      " 0.9590\n",
      " 0.9600\n",
      " 0.9298\n",
      " 0.9590\n",
      " 0.9369\n",
      " 0.9566\n",
      " 0.9431\n",
      " 0.9608\n",
      " 0.9489\n",
      " 0.9564\n",
      " 0.9271\n",
      " 0.9547\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9179\n",
      " 0.9589\n",
      " 0.9519\n",
      " 0.9562\n",
      " 0.8764\n",
      " 0.9559\n",
      " 0.9425\n",
      " 0.9279\n",
      " 0.9486\n",
      " 0.9406\n",
      " 0.9249\n",
      " 0.9554\n",
      " 0.9547\n",
      " 0.9507\n",
      " 0.9597\n",
      " 0.9421\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9425\n",
      " 0.9633\n",
      " 0.9644\n",
      " 0.9522\n",
      " 0.9450\n",
      " 0.9599\n",
      " 0.9467\n",
      " 0.9510\n",
      " 0.9457\n",
      " 0.9371\n",
      " 0.9431\n",
      " 0.9613\n",
      " 0.9317\n",
      " 0.9603\n",
      " 0.9466\n",
      " 0.9506\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9376\n",
      " 0.9474\n",
      " 0.9750\n",
      " 0.9558\n",
      " 0.9429\n",
      " 0.9483\n",
      " 0.9289\n",
      " 0.9478\n",
      " 0.9356\n",
      " 0.9470\n",
      " 0.9487\n",
      " 0.9671\n",
      " 0.9509\n",
      " 0.9584\n",
      " 0.9464\n",
      " 0.9457\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9446\n",
      " 0.9635\n",
      " 0.9821\n",
      " 0.9456\n",
      " 0.9632\n",
      " 0.9580\n",
      " 0.9128\n",
      " 0.9653\n",
      " 0.9365\n",
      " 0.9314\n",
      " 0.9251\n",
      " 0.9589\n",
      " 0.9559\n",
      " 0.9574\n",
      " 0.9399\n",
      " 0.9466\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9416\n",
      " 0.9567\n",
      " 0.9684\n",
      " 0.9402\n",
      " 0.9507\n",
      " 0.9537\n",
      " 0.9389\n",
      " 0.9590\n",
      " 0.9539\n",
      " 0.9461\n",
      " 0.9445\n",
      " 0.9555\n",
      " 0.9456\n",
      " 0.9542\n",
      " 0.9430\n",
      " 0.9303\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9360\n",
      " 0.9665\n",
      " 0.9731\n",
      " 0.9323\n",
      " 0.9471\n",
      " 0.9608\n",
      " 0.9442\n",
      " 0.9439\n",
      " 0.9429\n",
      " 0.9535\n",
      " 0.8912\n",
      " 0.9569\n",
      " 0.9513\n",
      " 0.9625\n",
      " 0.9575\n",
      " 0.9476\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9485\n",
      " 0.9485\n",
      " 0.9686\n",
      " 0.9315\n",
      " 0.9528\n",
      " 0.9627\n",
      " 0.9455\n",
      " 0.9118\n",
      " 0.9604\n",
      " 0.9507\n",
      " 0.9009\n",
      " 0.9788\n",
      " 0.9449\n",
      " 0.9534\n",
      " 0.9502\n",
      " 0.9370\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9079\n",
      " 0.9462\n",
      " 0.9610\n",
      " 0.9571\n",
      " 0.9560\n",
      " 0.9558\n",
      " 0.9441\n",
      " 0.9603\n",
      " 0.9362\n",
      " 0.9448\n",
      " 0.9279\n",
      " 0.9490\n",
      " 0.9463\n",
      " 0.9529\n",
      " 0.9405\n",
      " 0.9360\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9378\n",
      " 0.9574\n",
      " 0.9705\n",
      " 0.9465\n",
      " 0.9517\n",
      " 0.9590\n",
      " 0.9336\n",
      " 0.9530\n",
      " 0.9239\n",
      " 0.9466\n",
      " 0.9388\n",
      " 0.9486\n",
      " 0.9499\n",
      " 0.9467\n",
      " 0.9537\n",
      " 0.9409\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9477\n",
      " 0.9620\n",
      " 0.9724\n",
      " 0.9507\n",
      " 0.9542\n",
      " 0.9638\n",
      " 0.9491\n",
      " 0.9607\n",
      " 0.9565\n",
      " 0.9249\n",
      " 0.9328\n",
      " 0.9595\n",
      " 0.9489\n",
      " 0.9298\n",
      " 0.9351\n",
      " 0.9512\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9470\n",
      " 0.9429\n",
      " 0.9597\n",
      " 0.9553\n",
      " 0.9542\n",
      " 0.9511\n",
      " 0.9469\n",
      " 0.9540\n",
      " 0.9442\n",
      " 0.9457\n",
      " 0.9357\n",
      " 0.9536\n",
      " 0.9483\n",
      " 0.9489\n",
      " 0.9515\n",
      " 0.9524\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "Variable containing:\n",
      " 0.9503\n",
      " 0.9416\n",
      " 0.9753\n",
      " 0.9537\n",
      " 0.9562\n",
      " 0.9479\n",
      " 0.9505\n",
      " 0.9480\n",
      " 0.9209\n",
      " 0.9275\n",
      " 0.9179\n",
      " 0.9487\n",
      " 0.9349\n",
      " 0.9370\n",
      " 0.9350\n",
      " 0.9495\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "haha!\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9515  0.9475  0.9371  0.9271  0.9515  0.8974  0.9502  0.9300  0.9443  0.9179\n",
      " 0.9581  0.9597  0.9567  0.9363  0.9599  0.9568  0.9463  0.9578  0.9581  0.9589\n",
      " 0.9268  0.9268  0.9806  0.9626  0.9756  0.9772  0.9793  0.9793  0.9651  0.9519\n",
      " 0.9323  0.9597  0.9457  0.9423  0.9404  0.9383  0.9382  0.9571  0.9402  0.9562\n",
      " 0.9587  0.9587  0.9532  0.9533  0.9525  0.9521  0.9587  0.9540  0.9590  0.8764\n",
      " 0.9488  0.9488  0.9634  0.9524  0.9634  0.9432  0.9639  0.9622  0.9600  0.9559\n",
      " 0.9271  0.9318  0.9439  0.9372  0.9482  0.9407  0.9320  0.9271  0.9298  0.9425\n",
      " 0.9472  0.9811  0.9590  0.9616  0.9472  0.9353  0.9661  0.9627  0.9590  0.9279\n",
      " 0.9560  0.9560  0.9528  0.9573  0.9603  0.9553  0.9487  0.9552  0.9369  0.9486\n",
      " 0.9441  0.9441  0.9332  0.9460  0.9467  0.9406  0.9021  0.8934  0.9566  0.9406\n",
      " 0.9574  0.9574  0.9440  0.9368  0.9396  0.9404  0.9418  0.9415  0.9431  0.9249\n",
      " 0.9556  0.9556  0.9572  0.9520  0.9501  0.9656  0.9559  0.9564  0.9608  0.9554\n",
      " 0.9553  0.9496  0.9582  0.9548  0.9553  0.9476  0.9533  0.9468  0.9489  0.9547\n",
      " 0.9635  0.9635  0.9401  0.9388  0.9477  0.9641  0.9430  0.9386  0.9564  0.9507\n",
      " 0.9571  0.9411  0.9571  0.9510  0.9517  0.9164  0.9349  0.9398  0.9271  0.9597\n",
      " 0.9426  0.9426  0.9437  0.9510  0.9413  0.9398  0.9315  0.9497  0.9547  0.9421\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.9425  0.9376  0.9446  0.9416  0.9360  0.9485  0.9079  0.9378  0.9477  0.9470\n",
      " 0.9633  0.9474  0.9635  0.9567  0.9665  0.9485  0.9462  0.9574  0.9620  0.9429\n",
      " 0.9644  0.9750  0.9821  0.9684  0.9731  0.9686  0.9610  0.9705  0.9724  0.9597\n",
      " 0.9522  0.9558  0.9456  0.9402  0.9323  0.9315  0.9571  0.9465  0.9507  0.9553\n",
      " 0.9450  0.9429  0.9632  0.9507  0.9471  0.9528  0.9560  0.9517  0.9542  0.9542\n",
      " 0.9599  0.9483  0.9580  0.9537  0.9608  0.9627  0.9558  0.9590  0.9638  0.9511\n",
      " 0.9467  0.9289  0.9128  0.9389  0.9442  0.9455  0.9441  0.9336  0.9491  0.9469\n",
      " 0.9510  0.9478  0.9653  0.9590  0.9439  0.9118  0.9603  0.9530  0.9607  0.9540\n",
      " 0.9457  0.9356  0.9365  0.9539  0.9429  0.9604  0.9362  0.9239  0.9565  0.9442\n",
      " 0.9371  0.9470  0.9314  0.9461  0.9535  0.9507  0.9448  0.9466  0.9249  0.9457\n",
      " 0.9431  0.9487  0.9251  0.9445  0.8912  0.9009  0.9279  0.9388  0.9328  0.9357\n",
      " 0.9613  0.9671  0.9589  0.9555  0.9569  0.9788  0.9490  0.9486  0.9595  0.9536\n",
      " 0.9317  0.9509  0.9559  0.9456  0.9513  0.9449  0.9463  0.9499  0.9489  0.9483\n",
      " 0.9603  0.9584  0.9574  0.9542  0.9625  0.9534  0.9529  0.9467  0.9298  0.9489\n",
      " 0.9466  0.9464  0.9399  0.9430  0.9575  0.9502  0.9405  0.9537  0.9351  0.9515\n",
      " 0.9506  0.9457  0.9466  0.9303  0.9476  0.9370  0.9360  0.9409  0.9512  0.9524\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.9503\n",
      " 0.9416\n",
      " 0.9753\n",
      " 0.9537\n",
      " 0.9562\n",
      " 0.9479\n",
      " 0.9505\n",
      " 0.9480\n",
      " 0.9209\n",
      " 0.9275\n",
      " 0.9179\n",
      " 0.9487\n",
      " 0.9349\n",
      " 0.9370\n",
      " 0.9350\n",
      " 0.9495\n",
      "[torch.FloatTensor of size 16x21]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print \"scoring test questions...\"\n",
    "running_test_loss = 0.0\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = 4, \n",
    "    drop_last = True)\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    print \"here!\"\n",
    "    query_idx = batch['query_idx']\n",
    "    query_title = Variable(batch['query_title'])\n",
    "    query_body = Variable(batch['query_body'])\n",
    "    similar_title = Variable(batch['similar_title'])\n",
    "    similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "    random_title_list = []\n",
    "    random_body_list = []\n",
    "    for ridx in range(20): #number of retrieved (bm25) examples\n",
    "        random_title_name = 'random_title_' + str(ridx)\n",
    "        random_body_name = 'random_body_' + str(ridx)\n",
    "        random_title_list.append(Variable(batch[random_title_name]))\n",
    "        random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "    if use_gpu:\n",
    "        query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "        similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "        random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "        random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "    \n",
    "    cnn_query_title = model(query_title)\n",
    "    cnn_query_body = model(query_body)\n",
    "    cnn_query = (cnn_query_title + cnn_query_body)/2.0\n",
    "\n",
    "    cnn_similar_title = model(similar_title)\n",
    "    cnn_similar_body = model(similar_body)\n",
    "    cnn_similar = (cnn_similar_title + cnn_similar_body)/2.0\n",
    "\n",
    "    cnn_random_list = []\n",
    "    for ridx in range(len(random_title_list)):\n",
    "        cnn_random_title = model(random_title_list[ridx])\n",
    "        cnn_random_body = model(random_body_list[ridx])\n",
    "        cnn_random = (cnn_random_title + cnn_random_body)/2.0\n",
    "        cnn_random_list.append(cnn_random)\n",
    "    #end for\n",
    "           \n",
    "    cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    score_pos = cosine_similarity(cnn_query, cnn_similar)\n",
    "\n",
    "    score_list = []\n",
    "    score_list.append(score_pos)\n",
    "    for ridx in range(len(cnn_random_list)):\n",
    "        score_neg = cosine_similarity(cnn_query, cnn_random_list[ridx])\n",
    "        score_list.append(score_neg)\n",
    "    \n",
    "    print \"haha!\"\n",
    "    X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "    print X_scores\n",
    "    y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "    if use_gpu:\n",
    "        y_targets = y_targets.cuda()\n",
    "    loss = criterion(X_scores, y_targets) #y_target=0\n",
    "    running_test_loss += loss.cpu().data[0]        \n",
    "    \n",
    "    #save scores to data-frame\n",
    "    cnn_query_idx = query_idx.numpy()\n",
    "    cnn_retrieved_scores = X_scores.data.numpy()[:,1:] #skip positive score\n",
    "    for row, qidx in enumerate(cnn_query_idx):\n",
    "        test_idx_df.loc[test_idx_df['query_id'] == qidx, 'cnn_score'] = \" \".join(cnn_retrieved_scores[row,:].astype('str'))\n",
    "#end for        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.947493553162' '0.937081992626' '0.92705309391' '0.951512098312'\n",
      "  '0.897426068783' '0.950182437897' '0.930031895638' '0.944299459457'\n",
      "  '0.917933762074' '0.942549645901' '0.937607169151' '0.944585978985'\n",
      "  '0.941568851471' '0.936048328876' '0.948451638222' '0.9079413414'\n",
      "  '0.937844455242' '0.94773453474' '0.94703400135' '0.950279891491']\n",
      " ['0.959692239761' '0.956662356853' '0.93625587225' '0.959926307201'\n",
      "  '0.956772506237' '0.94633603096' '0.957779526711' '0.958075761795'\n",
      "  '0.958883345127' '0.963328659534' '0.947406113148' '0.963506400585'\n",
      "  '0.95674341917' '0.966477632523' '0.948509156704' '0.946211159229'\n",
      "  '0.95744228363' '0.96197271347' '0.942865312099' '0.941648602486']\n",
      " ['0.926836252213' '0.980607330799' '0.962563574314' '0.975600838661'\n",
      "  '0.977192044258' '0.979271113873' '0.979290664196' '0.965135753155'\n",
      "  '0.951937675476' '0.96437650919' '0.975035309792' '0.982052326202'\n",
      "  '0.968396604061' '0.973123669624' '0.968552052975' '0.960977196693'\n",
      "  '0.97052615881' '0.972414910793' '0.959660291672' '0.975342452526']\n",
      " ['0.95971673727' '0.945660352707' '0.942273378372' '0.940364539623'\n",
      "  '0.938344538212' '0.938176751137' '0.957120358944' '0.940229713917'\n",
      "  '0.956210911274' '0.952158510685' '0.955820143223' '0.945565640926'\n",
      "  '0.940209209919' '0.932306289673' '0.931453943253' '0.957112908363'\n",
      "  '0.946507513523' '0.950693666935' '0.955252170563' '0.953689694405']\n",
      " ['0.958671867847' '0.953169703484' '0.953296363354' '0.952480733395'\n",
      "  '0.952091157436' '0.95867562294' '0.954048871994' '0.959022462368'\n",
      "  '0.876413881779' '0.944998800755' '0.942889094353' '0.963204801083'\n",
      "  '0.950694978237' '0.947054028511' '0.952833294868' '0.955958247185'\n",
      "  '0.951707363129' '0.95422744751' '0.954238653183' '0.956190466881']\n",
      " ['0.948846399784' '0.963385283947' '0.952396035194' '0.963428258896'\n",
      "  '0.94324362278' '0.963914513588' '0.962211668491' '0.959985375404'\n",
      "  '0.955898463726' '0.959857106209' '0.948281168938' '0.958033025265'\n",
      "  '0.953735888004' '0.960770010948' '0.962744891644' '0.955758631229'\n",
      "  '0.959026396275' '0.963800430298' '0.951077520847' '0.947903692722']\n",
      " ['0.931800663471' '0.943912148476' '0.937188744545' '0.948227584362'\n",
      "  '0.940686762333' '0.931966245174' '0.927055060863' '0.92981505394'\n",
      "  '0.942522168159' '0.946651875973' '0.928887784481' '0.912799477577'\n",
      "  '0.938852846622' '0.944238722324' '0.945535540581' '0.944131791592'\n",
      "  '0.933624863625' '0.949067771435' '0.946900725365' '0.950458288193']\n",
      " ['0.981117963791' '0.958981871605' '0.961637377739' '0.947158813477'\n",
      "  '0.935284256935' '0.966074049473' '0.962685346603' '0.958960771561'\n",
      "  '0.927924513817' '0.950992763042' '0.947779238224' '0.965345978737'\n",
      "  '0.958962798119' '0.943866670132' '0.911771714687' '0.960253179073'\n",
      "  '0.953027963638' '0.96070098877' '0.954007446766' '0.947981834412']\n",
      " ['0.955968081951' '0.952777385712' '0.957263588905' '0.960295557976'\n",
      "  '0.955290555954' '0.948747634888' '0.955223023891' '0.936856091022'\n",
      "  '0.948621869087' '0.94571852684' '0.935572862625' '0.936480939388'\n",
      "  '0.953909039497' '0.94290459156' '0.960394799709' '0.936163961887'\n",
      "  '0.923856258392' '0.956526696682' '0.944244265556' '0.920918405056']\n",
      " ['0.944069504738' '0.933190405369' '0.945961415768' '0.946651697159'\n",
      "  '0.940559089184' '0.902064204216' '0.893410861492' '0.956649422646'\n",
      "  '0.940552651882' '0.937119245529' '0.94698369503' '0.93136715889'\n",
      "  '0.946075081825' '0.953460037708' '0.950661957264' '0.944802820683'\n",
      "  '0.946616649628' '0.924871563911' '0.94574368' '0.927480280399']]\n"
     ]
    }
   ],
   "source": [
    "print cnn_retrieved_scores[0:10,:].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51865 48116 271591 509697 457104 456649 451242 486395 341375 450253 502203 343014 100874 448228 390059 499089 287958 94637 239160 496852\n",
      "53.8664 53.633076 53.01136 52.28167 51.24043 49.793518 49.464836 49.392956 47.692867 47.28724 47.100048 46.77263 46.53052 46.177017 46.04682 45.246006 45.207558 44.60032 44.574905 44.140083\n"
     ]
    }
   ],
   "source": [
    "print test_idx_df.loc[6,'random_id']\n",
    "print test_idx_df.loc[6,'bm25_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ranking metrics...\n",
      "cnn MRR (test):  0.646306818182\n",
      "cnn P@1 (test):  0.5\n",
      "cnn P@5 (test):  0.4625\n",
      "cnn map (test):  0.532974881509\n"
     ]
    }
   ],
   "source": [
    "#save scored data frame\n",
    "#test_idx_df.to_csv(SAVE_PATH + '/test_idx_df_scored_cnn.csv', header=True)\n",
    "\n",
    "print \"computing ranking metrics...\"\n",
    "cnn_mrr_test = compute_mrr(test_idx_df, score_name='cnn_score')\n",
    "print \"cnn MRR (test): \", np.mean(cnn_mrr_test)\n",
    "\n",
    "cnn_pr1_test = precision_at_k(test_idx_df, K=1, score_name='cnn_score')\n",
    "print \"cnn P@1 (test): \", np.mean(cnn_pr1_test)\n",
    "\n",
    "cnn_pr5_test = precision_at_k(test_idx_df, K=5, score_name='cnn_score')\n",
    "print \"cnn P@5 (test): \", np.mean(cnn_pr5_test)\n",
    "\n",
    "cnn_map_test = compute_map(test_idx_df, score_name='cnn_score')\n",
    "print \"cnn map (test): \", np.mean(cnn_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(training_loss, label='Adam')\n",
    "plt.title(\"CNN Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_training_loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate_schedule, label='learning rate')\n",
    "plt.title(\"CNN learning rate schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_learning_rate_schedule.png')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(validation_loss, label='Adam')\n",
    "plt.title(\"CNN Model Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_validation_loss.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
