{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named meter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7c312dd411e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmeter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUCMeter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named meter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ConfigParser\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from meter import AUCMeter\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_SOURCE = '../data/askubuntu/'\n",
    "DATA_PATH_TARGET = '../data/android/'\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open(r'../src/config.ini'))\n",
    "SAVE_PATH = config.get('paths', 'save_path')\n",
    "RNN_SAVE_NAME = config.get('rnn_params', 'save_name')\n",
    "CNN_SAVE_NAME = config.get('cnn_params', 'save_name')\n",
    "EMBEDDINGS_FILE = config.get('paths', 'embeddings_path')\n",
    "MAX_TITLE_LEN = int(config.get('data_params', 'MAX_TITLE_LEN'))\n",
    "MAX_BODY_LEN = int(config.get('data_params', 'MAX_BODY_LEN'))\n",
    "TRAIN_SAMPLE_SIZE = int(config.get('data_params', 'TRAIN_SAMPLE_SIZE'))\n",
    "NUM_NEGATIVE = int(config.get('data_params', 'NUM_NEGATIVE'))\n",
    "#TODO: do we keep the same title and body len for android dataset?\n",
    "\n",
    "def lambda_schedule(epoch):\n",
    "    gamma = 0.001 \n",
    "    lambda_epoch = (2.0 / (1 + np.exp(-gamma * epoch))) - 1.0\n",
    "    return Variable(torch.FloatTensor([lambda_epoch]), requires_grad=False)\n",
    "\n",
    "def get_embeddings():\n",
    "    lines = []\n",
    "    with open(EMBEDDINGS_FILE, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "    \n",
    "    embedding_tensor = []\n",
    "    word_to_idx = {}\n",
    "    \n",
    "    for idx, l in enumerate(lines):\n",
    "        word, emb = l.split()[0], l.split()[1:]\n",
    "        vector = [float(x) for x in emb]\n",
    "        if idx == 0: #reserved\n",
    "            embedding_tensor.append(np.zeros(len(vector)))\n",
    "        embedding_tensor.append(vector)\n",
    "        word_to_idx[word] = idx+1\n",
    "    #end for\n",
    "    embedding_tensor = np.array(embedding_tensor, dtype=np.float32)    \n",
    "    return embedding_tensor, word_to_idx\n",
    "        \n",
    "def get_tensor_idx(text, word_to_idx, max_len):\n",
    "    null_idx = 0  #idx if word is not in the embeddings dictionary\n",
    "    text_idx = [word_to_idx[x] if x in word_to_idx else null_idx for x in text][:max_len]\n",
    "    if len(text_idx) < max_len:\n",
    "        text_idx.extend([null_idx for _ in range(max_len - len(text_idx))])    \n",
    "    x = torch.LongTensor(text_idx)  #64-bit integer\n",
    "    return x\n",
    "\n",
    "def generate_train_data(data_frame, train_text_df, word_to_idx, tokenizer, num_samples, num_negative, type='source'):\n",
    "\n",
    "    if num_samples == -1:\n",
    "        num_samples = data_frame.shape[0]\n",
    "\n",
    "    if (type == 'source'):\n",
    "        domain_label = 0\n",
    "    elif (type == 'target'):\n",
    "        domain_label = 1\n",
    "    else:\n",
    "        print \"incorrect domain type: enter either source or target!\"\n",
    "        return [] \n",
    "\n",
    "    train_dataset = []\n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        query_id = data_frame.loc[idx, 'query_id']\n",
    "        similar_id_list = map(int, data_frame.loc[idx, 'similar_id'].split(' '))\n",
    "        random_id_list = map(int, data_frame.loc[idx, 'random_id'].split(' '))\n",
    "    \n",
    "        #query title and body tensor ids\n",
    "        query_title = train_text_df[train_text_df['id'] == query_id].title.tolist() \n",
    "        query_body = train_text_df[train_text_df['id'] == query_id].body.tolist()\n",
    "        query_title_tokens = tokenizer.tokenize(query_title[0])[:MAX_TITLE_LEN]\n",
    "        query_body_tokens = tokenizer.tokenize(query_body[0])[:MAX_BODY_LEN]\n",
    "        query_title_tensor_idx = get_tensor_idx(query_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "        query_body_tensor_idx = get_tensor_idx(query_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "\n",
    "        for similar_id in similar_id_list:\n",
    "            sample = {}  #reset sample dictionary here\n",
    "            sample['query_idx'] = query_id\n",
    "            sample['domain_label'] = domain_label\n",
    "            sample['query_title'] = query_title_tensor_idx\n",
    "            sample['query_body'] = query_body_tensor_idx\n",
    "\n",
    "            similar_title = train_text_df[train_text_df['id'] == similar_id].title.tolist() \n",
    "            similar_body = train_text_df[train_text_df['id'] == similar_id].body.tolist()\n",
    "            similar_title_tokens = tokenizer.tokenize(similar_title[0])[:MAX_TITLE_LEN]\n",
    "            similar_body_tokens = tokenizer.tokenize(similar_body[0])[:MAX_BODY_LEN]\n",
    "            similar_title_tensor_idx = get_tensor_idx(similar_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "            similar_body_tensor_idx = get_tensor_idx(similar_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "            sample['similar_title'] = similar_title_tensor_idx\n",
    "            sample['similar_body'] = similar_body_tensor_idx\n",
    "\n",
    "            for ridx, random_id in enumerate(random_id_list[:num_negative]):\n",
    "                random_title_name = 'random_title_' + str(ridx)\n",
    "                random_body_name = 'random_body_' + str(ridx)\n",
    "        \n",
    "                random_title = train_text_df[train_text_df['id'] == random_id].title.tolist() \n",
    "                random_body = train_text_df[train_text_df['id'] == random_id].body.tolist()\n",
    "\n",
    "                if (len(random_title) > 0 and len(random_body) > 0):\n",
    "                    random_title_tokens = tokenizer.tokenize(random_title[0])[:MAX_TITLE_LEN]\n",
    "                    random_body_tokens = tokenizer.tokenize(random_body[0])[:MAX_BODY_LEN]\n",
    "                    random_title_tensor_idx = get_tensor_idx(random_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "                    random_body_tensor_idx = get_tensor_idx(random_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "                    sample[random_title_name] = random_title_tensor_idx\n",
    "                    sample[random_body_name] = random_body_tensor_idx\n",
    "                else:\n",
    "                    #generate a vector of all zeros (need 100 negative examples for each batch)\n",
    "                    sample[random_title_name] = torch.zeros(MAX_TITLE_LEN).type(torch.LongTensor) \n",
    "                    sample[random_body_name] = torch.zeros(MAX_BODY_LEN).type(torch.LongTensor)\n",
    "                #end if\n",
    "            #end for\n",
    "            train_dataset.append(sample)\n",
    "        #end for\n",
    "    #end for\n",
    "    return train_dataset \n",
    "\n",
    "def generate_test_data(data_frame, train_text_df, word_to_idx, tokenizer):\n",
    "\n",
    "    target_dataset = []\n",
    "    #for idx in tqdm(range(data_frame.shape[0])):\n",
    "    for idx in tqdm(range(1000)):\n",
    "        q1_id = data_frame.loc[idx, 'id_1']\n",
    "        q2_id = data_frame.loc[idx, 'id_2']\n",
    "\n",
    "        #q1 title and body tensor ids\n",
    "        q1_title = train_text_df[train_text_df['id'] == q1_id].title.tolist() \n",
    "        q1_body = train_text_df[train_text_df['id'] == q1_id].body.tolist()\n",
    "        q1_title_tokens = tokenizer.tokenize(q1_title[0])[:MAX_TITLE_LEN]\n",
    "        q1_body_tokens = tokenizer.tokenize(q1_body[0])[:MAX_BODY_LEN]\n",
    "        q1_title_tensor_idx = get_tensor_idx(q1_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "        q1_body_tensor_idx = get_tensor_idx(q1_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "\n",
    "        #q2 title and body tensor ids\n",
    "        q2_title = train_text_df[train_text_df['id'] == q2_id].title.tolist() \n",
    "        q2_body = train_text_df[train_text_df['id'] == q2_id].body.tolist()\n",
    "        q2_title_tokens = tokenizer.tokenize(q2_title[0])[:MAX_TITLE_LEN]\n",
    "        q2_body_tokens = tokenizer.tokenize(q2_body[0])[:MAX_BODY_LEN]\n",
    "        q2_title_tensor_idx = get_tensor_idx(q2_title_tokens, word_to_idx, MAX_TITLE_LEN) \n",
    "        q2_body_tensor_idx = get_tensor_idx(q2_body_tokens, word_to_idx, MAX_BODY_LEN)\n",
    "\n",
    "        sample = {}  #reset sample dictionary here\n",
    "        sample['q1_idx'] = q1_id\n",
    "        sample['q1_title'] = q1_title_tensor_idx\n",
    "        sample['q1_body'] = q1_body_tensor_idx\n",
    "\n",
    "        sample['q2_idx'] = q2_id\n",
    "        sample['q2_title'] = q2_title_tensor_idx\n",
    "        sample['q2_body'] = q2_body_tensor_idx\n",
    "\n",
    "        sample['domain_label'] = 1  #target domain\n",
    "        target_dataset.append(sample)\n",
    "    #end for\n",
    "    return target_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading source data...\n",
      "elapsed time: 14.41 sec\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "print \"loading source data...\"\n",
    "tic = time()\n",
    "source_text_file = DATA_PATH_SOURCE + '/texts_raw_fixed.txt'\n",
    "source_text_df = pd.read_table(source_text_file, sep='\\t', header=None)\n",
    "source_text_df.columns = ['id', 'title', 'body']\n",
    "source_text_df = source_text_df.dropna()\n",
    "source_text_df['title'] = source_text_df['title'].apply(lambda words: ' '.join(filter(lambda x: x not in stop, words.split())))\n",
    "source_text_df['body'] = source_text_df['body'].apply(lambda words: ' '.join(filter(lambda x: x not in stop, words.split())))\n",
    "source_text_df['title_len'] = source_text_df['title'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "source_text_df['body_len'] = source_text_df['body'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "\n",
    "source_idx_file = DATA_PATH_SOURCE + '/train_random.txt' \n",
    "source_idx_df = pd.read_table(source_idx_file, sep='\\t', header=None)\n",
    "source_idx_df.columns = ['query_id', 'similar_id', 'random_id']\n",
    "source_idx_df = source_idx_df.dropna()\n",
    "source_idx_df = source_idx_df.reset_index()\n",
    "\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading target data...\n",
      "elapsed time: 5.32 sec\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "print \"loading target data...\"\n",
    "tic = time()\n",
    "target_text_file = DATA_PATH_TARGET + '/corpus.txt'\n",
    "target_text_df = pd.read_table(target_text_file, sep='\\t', header=None)\n",
    "target_text_df.columns = ['id', 'title', 'body']\n",
    "target_text_df = target_text_df.dropna()\n",
    "target_text_df['title'] = target_text_df['title'].apply(lambda words: ' '.join(filter(lambda x: x not in stop, words.split())))\n",
    "target_text_df['body'] = target_text_df['body'].apply(lambda words: ' '.join(filter(lambda x: x not in stop, words.split())))\n",
    "target_text_df['title_len'] = target_text_df['title'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "target_text_df['body_len'] = target_text_df['body'].apply(lambda words: len(tokenizer.tokenize(str(words))))\n",
    "\n",
    "target_pos_file = DATA_PATH_TARGET + '/test.pos.txt'\n",
    "target_pos_df = pd.read_table(target_pos_file, sep=' ', header=None)\n",
    "target_pos_df.columns = ['id_1', 'id_2']\n",
    "\n",
    "target_neg_file = DATA_PATH_TARGET + '/test.neg.txt'\n",
    "target_neg_df = pd.read_table(target_neg_file, sep=' ', header=None)\n",
    "target_neg_df.columns = ['id_1', 'id_2']\n",
    "\n",
    "#generate target_idx_df with format similar to source\n",
    "target_idx_df = pd.DataFrame()\n",
    "for row_idx in range(target_pos_df.shape[0]):\n",
    "    query_id = target_pos_df.loc[row_idx, 'id_1']\n",
    "    similar_id = target_pos_df.loc[row_idx, 'id_2']\n",
    "    random_id_list = target_neg_df[target_neg_df['id_1']==query_id]['id_2'].astype('str').tolist()\n",
    "\n",
    "    target_idx_df.loc[row_idx, 'query_id'] = query_id \n",
    "    target_idx_df.loc[row_idx, 'similar_id'] = similar_id \n",
    "    target_idx_df.loc[row_idx, 'random_id'] = \" \".join(random_id_list)\n",
    "#end for    \n",
    "target_idx_df[['query_id', 'similar_id']] = target_idx_df[['query_id', 'similar_id']].astype(int)\n",
    "target_idx_df['similar_id'] = target_idx_df['similar_id'].astype(str)\n",
    "\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:01, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (embeddings):  100406\n",
      "elapsed time: 9.93 sec\n",
      "generating training data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  4.00it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 13.68it/s]\n",
      "  4%|▎         | 36/1000 [00:00<00:02, 350.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.21 sec\n",
      "generating test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 327.33it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 316.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 6.22 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print \"loading embeddings...\"\n",
    "tic = time()\n",
    "embeddings, word_to_idx = get_embeddings()\n",
    "print \"vocab size (embeddings): \", len(word_to_idx)\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "\n",
    "print \"generating training data ...\"\n",
    "tic = time()\n",
    "source_train_data = generate_train_data(source_idx_df, source_text_df, word_to_idx, tokenizer, TRAIN_SAMPLE_SIZE, NUM_NEGATIVE, type='source')\n",
    "target_train_data = generate_train_data(target_idx_df, target_text_df, word_to_idx, tokenizer, TRAIN_SAMPLE_SIZE, NUM_NEGATIVE, type='target')\n",
    "\n",
    "train_data_combined = source_train_data + target_train_data  #merge source and target data\n",
    "shuffle(train_data_combined) #permute randomly in-place \n",
    "\n",
    "num_source_domain = len(source_train_data)\n",
    "num_target_domain = len(target_train_data)\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "\n",
    "print \"generating test data...\"\n",
    "tic = time()\n",
    "target_test_pos_data = generate_test_data(target_pos_df, target_text_df, word_to_idx, tokenizer)\n",
    "target_test_neg_data = generate_test_data(target_neg_df, target_text_df, word_to_idx, tokenizer)\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185\n",
      "118500\n"
     ]
    }
   ],
   "source": [
    "print target_pos_df.shape[0]\n",
    "print target_neg_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating question encoder CNN model...\n",
      "CNN (\n",
      "  (embed): Embedding(100406, 200)\n",
      "  (convs1): ModuleList (\n",
      "    (0): Conv2d(1, 200, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 200, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 200, kernel_size=(4, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 200, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "instantiating domain classifier model...\n",
      "DNN (\n",
      "  (fc1): Linear (800 -> 128)\n",
      "  (fc2): Linear (128 -> 32)\n",
      "  (fc3): Linear (32 -> 8)\n",
      "  (fc4): Linear (8 -> 2)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print \"instantiating question encoder CNN model...\"\n",
    "#training parameters\n",
    "num_epochs = 2 #16 \n",
    "batch_size = 8 #32 \n",
    "\n",
    "#CNN parameters\n",
    "kernel_num = 200\n",
    "kernel_sizes = range(2,6)\n",
    "weight_decay = 1e-5 \n",
    "learning_rate = 1e-3 \n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = embeddings.shape[1] #200\n",
    "\n",
    "#CNN architecture\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "    \n",
    "print model\n",
    "\n",
    "print \"instantiating domain classifier model...\"\n",
    "#DNN parameters\n",
    "hidden_size = kernel_num * len(kernel_sizes) #CNN output's dim \n",
    "\n",
    "dnn_input_dim = hidden_size\n",
    "dnn_output_dim = NUM_CLASSES\n",
    "\n",
    "#domain classifier architecture\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim  \n",
    " \n",
    "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        self.fc4 = nn.Linear(8, self.output_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "domain_clf = DNN(dnn_input_dim, dnn_output_dim)\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    domain_clf = domain_clf.cuda()\n",
    "\n",
    "print domain_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  [ 1.55555558  2.79999995]\n"
     ]
    }
   ],
   "source": [
    "#define loss and optimizer\n",
    "class_weights = np.array([num_source_domain, num_target_domain], dtype=np.float32)\n",
    "class_weights = sum(class_weights) / class_weights\n",
    "class_weights_tensor = torch.from_numpy(class_weights)\n",
    "if use_gpu:\n",
    "    class_weights_tensor = class_weights_tensor.cuda()\n",
    "print \"class weights: \", class_weights\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "criterion_gen = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True) #margin=0.3\n",
    "criterion_dis = nn.NLLLoss(weight=class_weights_tensor, size_average=True)\n",
    "\n",
    "optimizer_gen = torch.optim.Adam(model_parameters, lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer_dis = torch.optim.Adam(domain_clf.parameters(), lr= -1 * learning_rate, weight_decay=weight_decay) #NOTE: negative learning rate for adversarial training\n",
    "\n",
    "scheduler_gen = StepLR(optimizer_gen, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "scheduler_dis = StepLR(optimizer_dis, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "lambda_list = []\n",
    "training_loss_tot, training_loss_gen, training_loss_dis  = [], [], []\n",
    "\n",
    "grad_norm_df = pd.DataFrame()\n",
    "weight_norm_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.55555558,  2.79999995], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"training...\"\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_train_loss_tot = 0.0\n",
    "    running_train_loss_gen = 0.0\n",
    "    running_train_loss_dis = 0.0\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data_combined, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    domain_clf.train()\n",
    "\n",
    "    scheduler_gen.step() \n",
    "    scheduler_dis.step()\n",
    "        \n",
    "    for bidx, batch in enumerate(tqdm(train_data_loader)):\n",
    "      \n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])\n",
    "        similar_title = Variable(batch['similar_title'])\n",
    "        similar_body = Variable(batch['similar_body'])\n",
    "        \n",
    "        domain_label_flat = batch['domain_label'].numpy().ravel()\n",
    "        domain_label = Variable(torch.from_numpy(domain_label_flat))\n",
    "\n",
    "        random_title_list = []\n",
    "        random_body_list = []\n",
    "        for ridx in range(NUM_NEGATIVE): #100, number of random (negative) examples \n",
    "            random_title_name = 'random_title_' + str(ridx)\n",
    "            random_body_name = 'random_body_' + str(ridx)\n",
    "            random_title_list.append(Variable(batch[random_title_name]))\n",
    "            random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "        if use_gpu:\n",
    "            domain_label = domain_label.cuda()\n",
    "            query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "            similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "            random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "            random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "        \n",
    "        optimizer_gen.zero_grad() \n",
    "        optimizer_dis.zero_grad() \n",
    "\n",
    "        #question encoder\n",
    "\n",
    "        #query title\n",
    "        cnn_query_title = model(query_title)\n",
    "        #query body\n",
    "        cnn_query_body = model(query_body)\n",
    "        cnn_query = (cnn_query_title + cnn_query_body)/2.0\n",
    "\n",
    "        #similar title\n",
    "        cnn_similar_title = model(similar_title)\n",
    "        #similar body\n",
    "        cnn_similar_body = model(similar_body)\n",
    "        cnn_similar = (cnn_similar_title + cnn_similar_body)/2.0\n",
    "\n",
    "        cnn_random_list = []\n",
    "        for ridx in range(len(random_title_list)):\n",
    "            #random title\n",
    "            cnn_random_title = model(random_title_list[ridx])\n",
    "            #random body\n",
    "            cnn_random_body = model(random_body_list[ridx])\n",
    "\n",
    "            cnn_random = (cnn_random_title + cnn_random_body)/2.0\n",
    "            cnn_random_list.append(cnn_random)\n",
    "        #end for\n",
    "           \n",
    "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score_pos = cosine_similarity(cnn_query, cnn_similar)\n",
    "\n",
    "        score_list = []\n",
    "        score_list.append(score_pos)\n",
    "        for ridx in range(len(cnn_random_list)):\n",
    "            score_neg = cosine_similarity(cnn_query, cnn_random_list[ridx])\n",
    "            score_list.append(score_neg)\n",
    "\n",
    "        X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "        y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "        if use_gpu:\n",
    "            y_targets = y_targets.cuda()\n",
    "        loss_gen = criterion_gen(X_scores, y_targets) #y_target=0\n",
    "\n",
    "\n",
    "        #domain classifier\n",
    "        y_clf_query = domain_clf(cnn_query)\n",
    "        y_clf_similar = domain_clf(cnn_similar)\n",
    "\n",
    "        y_clf_random_list = []\n",
    "        for ridx in range(len(cnn_random_list)):\n",
    "            cnn_random = cnn_random_list[ridx]\n",
    "            y_clf_random = domain_clf(cnn_random)\n",
    "            y_clf_random_list.append(y_clf_random)\n",
    "        #end for\n",
    "\n",
    "        loss_dis_query = criterion_dis(y_clf_query, domain_label)\n",
    "        loss_dis_similar = criterion_dis(y_clf_similar, domain_label)\n",
    "\n",
    "        loss_dis_random_list = []\n",
    "        for ridx in range(len(y_clf_random_list)):\n",
    "            y_clf_random = y_clf_random_list[ridx]\n",
    "            loss_dis_random = criterion_dis(y_clf_random, domain_label)\n",
    "            loss_dis_random_list.append(loss_dis_random)\n",
    "        #end for\n",
    "        loss_dis = loss_dis_query + loss_dis_similar + sum(loss_dis_random_list)\n",
    "\n",
    "        #compute total loss\n",
    "        lambda_k = lambda_schedule(epoch)\n",
    "        if use_gpu:\n",
    "            lambda_k = lambda_k.cuda()\n",
    "\n",
    "        loss_tot = loss_gen - 1e-4 * loss_dis  #NOTE: keep lambda_k=1e-4 fixed\n",
    "\n",
    "        loss_tot.backward()   #call backward() once\n",
    "        optimizer_gen.step()  #min loss_tot: min loss_gen, max loss_dis\n",
    "        optimizer_dis.step()  #min loss_dis: update domain clf params with negative learning rate\n",
    "                \n",
    "        running_train_loss_tot += loss_tot.cpu().data[0]        \n",
    "        running_train_loss_gen += loss_gen.cpu().data[0]        \n",
    "        running_train_loss_dis += loss_dis.cpu().data[0]        \n",
    "        \n",
    "        #gradient and weight norms \n",
    "        df_idx = bidx + epoch * batch_size\n",
    "        grad_norm_df.loc[df_idx, 'df_idx'] = df_idx\n",
    "        weight_norm_df.loc[df_idx, 'df_idx'] = df_idx\n",
    "        \n",
    "        grad_norm_df.loc[df_idx,'w1_grad_l2'] = torch.norm(domain_clf.fc1.weight.grad, 2).cpu().data[0] \n",
    "        weight_norm_df.loc[df_idx,'w1_data_l2'] = torch.norm(domain_clf.fc1.weight, 2).cpu().data[0]\n",
    "\n",
    "        grad_norm_df.loc[df_idx,'w2_grad_l2'] = torch.norm(domain_clf.fc2.weight.grad, 2).cpu().data[0] \n",
    "        weight_norm_df.loc[df_idx,'w2_data_l2'] = torch.norm(domain_clf.fc2.weight, 2).cpu().data[0] \n",
    "\n",
    "        grad_norm_df.loc[df_idx,'w3_grad_l2'] = torch.norm(domain_clf.fc3.weight.grad, 2).cpu().data[0] \n",
    "        weight_norm_df.loc[df_idx,'w3_data_l2'] = torch.norm(domain_clf.fc3.weight, 2).cpu().data[0] \n",
    "        \n",
    "        grad_norm_df.loc[df_idx,'w4_grad_l2'] = torch.norm(domain_clf.fc4.weight.grad, 2).cpu().data[0] \n",
    "        weight_norm_df.loc[df_idx,'w4_data_l2'] = torch.norm(domain_clf.fc4.weight, 2).cpu().data[0] \n",
    "        \n",
    "    #end for\n",
    "    lambda_list.append(lambda_k.cpu().data[0])\n",
    "    training_loss_tot.append(running_train_loss_tot)\n",
    "    training_loss_gen.append(running_train_loss_gen)\n",
    "    training_loss_dis.append(running_train_loss_dis)\n",
    "    print \"epoch: %4d, training loss: %.4f\" %(epoch+1, running_train_loss_tot)\n",
    "    \n",
    "    torch.save(model, SAVE_PATH + '/adversarial_domain_transfer.pt')\n",
    "#end for\n",
    "\"\"\"\n",
    "print \"loading pre-trained model...\"\n",
    "model = torch.load(SAVE_PATH + '/adversarial_domain_transfer.pt')\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\"\"\"\n",
    "\n",
    "print \"scoring similarity between target questions...\"\n",
    "y_true, y_pred_cnn = [], []\n",
    "auc_meter = AUCMeter()\n",
    "\n",
    "test_data_loader_pos = torch.utils.data.DataLoader(\n",
    "    target_test_pos_data, \n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = 4, \n",
    "    drop_last = True)\n",
    "        \n",
    "for batch in tqdm(test_data_loader_pos):\n",
    "\n",
    "    q1_idx = batch['q1_idx']\n",
    "    q1_title = Variable(batch['q1_title'])\n",
    "    q1_body = Variable(batch['q1_body'])\n",
    "\n",
    "    q2_idx = batch['q2_idx']\n",
    "    q2_title = Variable(batch['q2_title'])\n",
    "    q2_body = Variable(batch['q2_body'])\n",
    "\n",
    "    if use_gpu:\n",
    "        q1_title, q1_body = q1_title.cuda(), q1_body.cuda()\n",
    "        q2_title, q2_body = q2_title.cuda(), q2_body.cuda()\n",
    "\n",
    "    #q1 title\n",
    "    cnn_q1_title = model(q1_title)\n",
    "    #q1 body\n",
    "    cnn_q1_body = model(q1_body)\n",
    "\n",
    "    cnn_q1 = (cnn_q1_title + cnn_q1_body)/2.0\n",
    "\n",
    "    #q2 title\n",
    "    cnn_q2_title = model(q2_title)\n",
    "    #q2 body\n",
    "    cnn_q2_body = model(q2_body)\n",
    "    cnn_q2 = (cnn_q2_title + cnn_q2_body)/2.0\n",
    "\n",
    "    cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    score_pos = cosine_similarity(cnn_q1, cnn_q2)\n",
    "\n",
    "    score_pos_numpy = score_pos.cpu().data.numpy() #TODO: check (some scores are negative)\n",
    "\n",
    "    y_true.extend(np.ones(batch_size)) #true label (similar)\n",
    "    y_pred_cnn.extend(score_pos_numpy.tolist())\n",
    "    auc_meter.add(score_pos_numpy, np.ones(batch_size))\n",
    "#end for        \n",
    "\n",
    "test_data_loader_neg = torch.utils.data.DataLoader(\n",
    "    target_test_neg_data, \n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = 4, \n",
    "    drop_last = True)\n",
    "        \n",
    "for batch in tqdm(test_data_loader_neg):\n",
    "    q1_idx = batch['q1_idx']\n",
    "    q1_title = Variable(batch['q1_title'])\n",
    "    q1_body = Variable(batch['q1_body'])\n",
    "\n",
    "    q2_idx = batch['q2_idx']\n",
    "    q2_title = Variable(batch['q2_title'])\n",
    "    q2_body = Variable(batch['q2_body'])\n",
    "\n",
    "    if use_gpu:\n",
    "        q1_title, q1_body = q1_title.cuda(), q1_body.cuda()\n",
    "        q2_title, q2_body = q2_title.cuda(), q2_body.cuda()\n",
    "\n",
    "    #q1 title\n",
    "    cnn_q1_title = model(q1_title)\n",
    "    #q1 body\n",
    "    cnn_q1_body = model(q1_body)\n",
    "\n",
    "    cnn_q1 = (cnn_q1_title + cnn_q1_body)/2.0\n",
    "\n",
    "    #q2 title\n",
    "    cnn_q2_title = model(q2_title)\n",
    "    #q2 body\n",
    "    cnn_q2_body = model(q2_body)\n",
    "\n",
    "    cnn_q2 = (cnn_q2_title + cnn_q2_body)/2.0\n",
    "\n",
    "    cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    score_neg = cosine_similarity(cnn_q1, cnn_q2)\n",
    "\n",
    "    score_neg_numpy = score_neg.cpu().data.numpy() #TODO: check (some scores are negative)\n",
    "\n",
    "    y_true.extend(np.zeros(batch_size)) #true label (not similar)\n",
    "    y_pred_cnn.extend(score_neg_numpy.tolist())\n",
    "    auc_meter.add(score_neg_numpy, np.ones(batch_size))\n",
    "#end for        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve:  0.9027025\n",
      "ROC AUC(0.05):  0.027003\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'auc_meter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-737350b6d0dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mroc_auc_0p05fpr_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"ROC AUC(0.05) meter: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_0p05fpr_meter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_meter' is not defined"
     ]
    }
   ],
   "source": [
    "roc_auc = roc_auc_score(y_true, y_pred_cnn)\n",
    "print \"area under ROC curve: \", roc_auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_cnn)\n",
    "\n",
    "idx_fpr_thresh = np.where(fpr < 0.05)[0]\n",
    "roc_auc_0p05fpr = auc(fpr[idx_fpr_thresh], tpr[idx_fpr_thresh])\n",
    "print \"ROC AUC(0.05): \", roc_auc_0p05fpr\n",
    "\n",
    "\n",
    "roc_auc_0p05fpr_meter = auc_meter.value(0.05)\n",
    "print \"ROC AUC(0.05) meter: \", roc_auc_0p05fpr_meter\n",
    "\n",
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, c='b', lw=2.0, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], c='k', lw=2.0, linestyle='--')\n",
    "plt.title('CNN Adversarial Domain Transfer')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('../figures/cnn_domain_transfer_adversarial_lstm.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambda_list)\n",
    "plt.title('CNN lambda schedule')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('lambda')\n",
    "plt.savefig('../figures/cnn_domain_transfer_adversarial_lambda.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_loss_tot, label='total loss')\n",
    "plt.plot(training_loss_gen, label='generator loss')\n",
    "plt.plot(training_loss_dis, label='discriminator loss')\n",
    "plt.title(\"CNN Adversarial Domain Transfer Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_domain_transfer_adversarial_loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(grad_norm_df['w1_grad_l2'], c='r', alpha=0.8, label='w1 grad l2')\n",
    "plt.plot(grad_norm_df['w2_grad_l2'], c='b', alpha=0.8, label='w2 grad l2')\n",
    "plt.plot(grad_norm_df['w3_grad_l2'], c='g', alpha=0.8, label='w3 grad l2')\n",
    "plt.plot(grad_norm_df['w4_grad_l2'], c='k', alpha=0.8, label='w4 grad l2')\n",
    "plt.title('cnn domain classifier gradient norm')\n",
    "plt.xlabel('num batches')\n",
    "plt.ylabel('l2 norm')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_domain_transfer_adversarial_gradient_norm.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(weight_norm_df['w1_data_l2'], c='r', alpha=0.8, label='w1 data l2')\n",
    "plt.plot(weight_norm_df['w2_data_l2'], c='b', alpha=0.8, label='w2 data l2')\n",
    "plt.plot(weight_norm_df['w3_data_l2'], c='g', alpha=0.8, label='w3 data l2')\n",
    "plt.plot(weight_norm_df['w4_data_l2'], c='k', alpha=0.8, label='w4 data l2')\n",
    "plt.title('cnn domain classifier weight norm')\n",
    "plt.xlabel('num batches')\n",
    "plt.ylabel('l2 norm')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_domain_transfer_adversarial_weight_norm.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"fitting tf-idf vectorizer on source data...\"\n",
    "tic = time()\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer.tokenize, analyzer='word', ngram_range=(1,1))\n",
    "tfidf.fit(source_text_df['title'].tolist() + source_text_df['body'].tolist())\n",
    "toc = time()\n",
    "print \"elapsed time: %.2f sec\" %(toc - tic)\n",
    "vocab = tfidf.vocabulary_\n",
    "print \"source vocab size: \", len(vocab)\n",
    "\n",
    "print \"computing tf-idf vectors for target data...\"\n",
    "target_tfidf_dict = {}\n",
    "for row_idx in tqdm(range(target_text_df.shape[0])):\n",
    "    target_id = target_text_df.loc[row_idx, 'id']\n",
    "    target_title_body = target_text_df.loc[row_idx,'title'] + ' ' + target_text_df.loc[row_idx,'body']\n",
    "    target_tfidf = tfidf.transform([target_title_body])\n",
    "    target_tfidf_dict[target_id] = target_tfidf\n",
    "#end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"scoring similarity between target questions...\"\n",
    "y_true, y_pred = [], []\n",
    "for row_idx in tqdm(range(target_pos_df.shape[0])):\n",
    "    y_true.append(1) #true label (similar)\n",
    "    \n",
    "    q1_idx = target_pos_df.loc[row_idx,'id']\n",
    "    q2_idx = target_pos_df.loc[row_idx,'pos']\n",
    "\n",
    "    score = cosine_similarity(target_tfidf_dict[q1_idx], target_tfidf_dict[q2_idx])\n",
    "    y_pred.append(score[0][0])\n",
    "#end for\n",
    "\n",
    "for row_idx in tqdm(range(target_neg_df.shape[0])):\n",
    "    y_true.append(0) #true label (not similar)\n",
    "    \n",
    "    q1_idx = target_neg_df.loc[row_idx,'id']\n",
    "    q2_idx = target_neg_df.loc[row_idx,'neg']\n",
    "\n",
    "    score = cosine_similarity(target_tfidf_dict[q1_idx], target_tfidf_dict[q2_idx])\n",
    "    y_pred.append(score[0][0])\n",
    "#end for\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print \"area under ROC curve: \", roc_auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "\n",
    "idx_fpr_thresh = np.where(fpr < 0.05)[0]\n",
    "roc_auc_0p05fpr = auc(fpr[idx_fpr_thresh], tpr[idx_fpr_thresh])\n",
    "print \"ROC AUC(0.05): \", roc_auc_0p05fpr\n",
    "\n",
    "y_df = pd.DataFrame()\n",
    "y_df['y_pred'] = y_pred\n",
    "y_df['y_true'] = y_true\n",
    "bins = np.linspace(min(y_pred)-0.1, max(y_pred)+0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, c='b', lw=2.0, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], c='k', lw=2.0, linestyle='--')\n",
    "plt.title('TF-IDF Domain Transfer')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(y_df[y_df['y_true']==1]['y_pred'], bins, kde=True, norm_hist=True, color='b', label='pos class')\n",
    "sns.distplot(y_df[y_df['y_true']==0]['y_pred'], bins, kde=True, norm_hist=True, color='r', label='neg class')\n",
    "plt.xlim([0,1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('normalized histogram')\n",
    "plt.title('pos and neg class separation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "num_epochs = 2 #16\n",
    "batch_size = 16 \n",
    "\n",
    "#model parameters\n",
    "embed_num = len(word_to_idx)\n",
    "embed_dim = len(embeddings[0])\n",
    "kernel_num = 200  #TODO: tune\n",
    "kernel_sizes = range(2,6)\n",
    "learning_rate = 1e-3 \n",
    "weight_decay = 1e-5\n",
    "\n",
    "class  CNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, kernel_num, kernel_sizes):\n",
    "        super(CNN,self).__init__()\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        Ci = 1            #input channel\n",
    "        Co = kernel_num   #depth\n",
    "        Ks = kernel_sizes #height of each filter\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight.data = torch.from_numpy(embeddings)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x\n",
    "\n",
    "model = CNN(embed_num, embed_dim, kernel_num, kernel_sizes)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss and optimizer\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "model.train()\n",
    "scheduler.step()\n",
    "    \n",
    "for batch in tqdm(train_data_loader):\n",
    "    query_title = Variable(batch['query_title'])\n",
    "    query_body = Variable(batch['query_body'])\n",
    "    cnn_query_title = model(query_title)\n",
    "    cnn_query_body = model(query_body)\n",
    "    print query_title.data.shape\n",
    "    print query_body.data.shape\n",
    "    print cnn_query_title.data.shape\n",
    "    print cnn_query_body.data.shape\n",
    "    print \"---\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss and optimizer\n",
    "criterion = nn.MultiMarginLoss(p=1, margin=0.4, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.5) #half learning rate every 4 epochs\n",
    "\n",
    "learning_rate_schedule = [] \n",
    "training_loss, validation_loss, test_loss = [], [], []\n",
    "\n",
    "print \"training...\"\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = 4, \n",
    "        drop_last = True)\n",
    "        \n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "        \n",
    "    for batch in tqdm(train_data_loader):\n",
    "    \n",
    "        query_title = Variable(batch['query_title'])\n",
    "        query_body = Variable(batch['query_body'])\n",
    "        similar_title = Variable(batch['similar_title'])\n",
    "        similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "        random_title_list = []\n",
    "        random_body_list = []\n",
    "        for ridx in range(NUM_NEGATIVE): #number of random negative examples\n",
    "            random_title_name = 'random_title_' + str(ridx)\n",
    "            random_body_name = 'random_body_' + str(ridx)\n",
    "            random_title_list.append(Variable(batch[random_title_name]))\n",
    "            random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "        if use_gpu:\n",
    "            query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "            similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "            random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "            random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cnn_query_title = model(query_title)\n",
    "        cnn_query_body = model(query_body)\n",
    "        cnn_query = (cnn_query_title + cnn_query_body)/2.0\n",
    "\n",
    "        cnn_similar_title = model(similar_title)\n",
    "        cnn_similar_body = model(similar_body)\n",
    "        cnn_similar = (cnn_similar_title + cnn_similar_body)/2.0\n",
    "\n",
    "        cnn_random_list = []\n",
    "        for ridx in range(len(random_title_list)):\n",
    "            cnn_random_title = model(random_title_list[ridx])\n",
    "            cnn_random_body = model(random_body_list[ridx])\n",
    "            cnn_random = (cnn_random_title + cnn_random_body)/2.0\n",
    "            cnn_random_list.append(cnn_random)\n",
    "        #end for\n",
    "           \n",
    "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        score_pos = cosine_similarity(cnn_query, cnn_similar)\n",
    "\n",
    "        score_list = []\n",
    "        score_list.append(score_pos)\n",
    "        for ridx in range(len(cnn_random_list)):\n",
    "            score_neg = cosine_similarity(cnn_query, cnn_random_list[ridx])\n",
    "            score_list.append(score_neg)\n",
    "\n",
    "        X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "        print X_scores\n",
    "        y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "        if use_gpu:\n",
    "            y_targets = y_targets.cuda()\n",
    "        loss = criterion(X_scores, y_targets) #y_target=0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_train_loss += loss.cpu().data[0]        \n",
    "        \n",
    "    #end for\n",
    "\n",
    "    training_loss.append(running_train_loss)\n",
    "    learning_rate_schedule.append(scheduler.get_lr())\n",
    "    print \"epoch: %4d, training loss: %.4f\" %(epoch+1, running_train_loss)\n",
    "    \n",
    "    torch.save(model, SAVE_PATH + SAVE_NAME)\n",
    "\n",
    "    #early stopping\n",
    "    patience = 4\n",
    "    min_delta = 0.1\n",
    "    if epoch == 0:\n",
    "        patience_cnt = 0\n",
    "    elif epoch > 0 and training_loss[epoch-1] - training_loss[epoch] > min_delta:\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "\n",
    "    if patience_cnt > patience:\n",
    "        print \"early stopping...\"\n",
    "        break\n",
    "#end for\n",
    "\"\"\"\n",
    "print \"loading pre-trained model...\"\n",
    "model = torch.load(SAVE_PATH)\n",
    "if use_gpu:\n",
    "    print \"found CUDA GPU...\"\n",
    "    model = model.cuda()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"scoring test questions...\"\n",
    "running_test_loss = 0.0\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = 4, \n",
    "    drop_last = True)\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_data_loader):\n",
    "    print \"here!\"\n",
    "    query_idx = batch['query_idx']\n",
    "    query_title = Variable(batch['query_title'])\n",
    "    query_body = Variable(batch['query_body'])\n",
    "    similar_title = Variable(batch['similar_title'])\n",
    "    similar_body = Variable(batch['similar_body'])\n",
    "\n",
    "    random_title_list = []\n",
    "    random_body_list = []\n",
    "    for ridx in range(20): #number of retrieved (bm25) examples\n",
    "        random_title_name = 'random_title_' + str(ridx)\n",
    "        random_body_name = 'random_body_' + str(ridx)\n",
    "        random_title_list.append(Variable(batch[random_title_name]))\n",
    "        random_body_list.append(Variable(batch[random_body_name]))\n",
    "\n",
    "    if use_gpu:\n",
    "        query_title, query_body = query_title.cuda(), query_body.cuda()\n",
    "        similar_title, similar_body = similar_title.cuda(), similar_body.cuda()\n",
    "        random_title_list = map(lambda item: item.cuda(), random_title_list)\n",
    "        random_body_list = map(lambda item: item.cuda(), random_body_list)\n",
    "    \n",
    "    cnn_query_title = model(query_title)\n",
    "    cnn_query_body = model(query_body)\n",
    "    cnn_query = (cnn_query_title + cnn_query_body)/2.0\n",
    "\n",
    "    cnn_similar_title = model(similar_title)\n",
    "    cnn_similar_body = model(similar_body)\n",
    "    cnn_similar = (cnn_similar_title + cnn_similar_body)/2.0\n",
    "\n",
    "    cnn_random_list = []\n",
    "    for ridx in range(len(random_title_list)):\n",
    "        cnn_random_title = model(random_title_list[ridx])\n",
    "        cnn_random_body = model(random_body_list[ridx])\n",
    "        cnn_random = (cnn_random_title + cnn_random_body)/2.0\n",
    "        cnn_random_list.append(cnn_random)\n",
    "    #end for\n",
    "           \n",
    "    cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    score_pos = cosine_similarity(cnn_query, cnn_similar)\n",
    "\n",
    "    score_list = []\n",
    "    score_list.append(score_pos)\n",
    "    for ridx in range(len(cnn_random_list)):\n",
    "        score_neg = cosine_similarity(cnn_query, cnn_random_list[ridx])\n",
    "        score_list.append(score_neg)\n",
    "    \n",
    "    print \"haha!\"\n",
    "    X_scores = torch.stack(score_list, 1) #[batch_size, K=101]\n",
    "    print X_scores\n",
    "    y_targets = Variable(torch.zeros(X_scores.size(0)).type(torch.LongTensor)) #[batch_size]\n",
    "    if use_gpu:\n",
    "        y_targets = y_targets.cuda()\n",
    "    loss = criterion(X_scores, y_targets) #y_target=0\n",
    "    running_test_loss += loss.cpu().data[0]        \n",
    "    \n",
    "    #save scores to data-frame\n",
    "    cnn_query_idx = query_idx.numpy()\n",
    "    cnn_retrieved_scores = X_scores.data.numpy()[:,1:] #skip positive score\n",
    "    for row, qidx in enumerate(cnn_query_idx):\n",
    "        test_idx_df.loc[test_idx_df['query_id'] == qidx, 'cnn_score'] = \" \".join(cnn_retrieved_scores[row,:].astype('str'))\n",
    "#end for        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cnn_retrieved_scores[0:10,:].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print test_idx_df.loc[6,'random_id']\n",
    "print test_idx_df.loc[6,'bm25_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scored data frame\n",
    "#test_idx_df.to_csv(SAVE_PATH + '/test_idx_df_scored_cnn.csv', header=True)\n",
    "\n",
    "print \"computing ranking metrics...\"\n",
    "cnn_mrr_test = compute_mrr(test_idx_df, score_name='cnn_score')\n",
    "print \"cnn MRR (test): \", np.mean(cnn_mrr_test)\n",
    "\n",
    "cnn_pr1_test = precision_at_k(test_idx_df, K=1, score_name='cnn_score')\n",
    "print \"cnn P@1 (test): \", np.mean(cnn_pr1_test)\n",
    "\n",
    "cnn_pr5_test = precision_at_k(test_idx_df, K=5, score_name='cnn_score')\n",
    "print \"cnn P@5 (test): \", np.mean(cnn_pr5_test)\n",
    "\n",
    "cnn_map_test = compute_map(test_idx_df, score_name='cnn_score')\n",
    "print \"cnn map (test): \", np.mean(cnn_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(training_loss, label='Adam')\n",
    "plt.title(\"CNN Model Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_training_loss.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate_schedule, label='learning rate')\n",
    "plt.title(\"CNN learning rate schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_learning_rate_schedule.png')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(validation_loss, label='Adam')\n",
    "plt.title(\"CNN Model Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_validation_loss.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
