% It is an example file showing how to use the 'sigkddExp.cls' 
% LaTeX2e document class file for submissions to sigkdd explorations.
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@acm.org
%
 
\documentclass{sigkddExp}

\usepackage{booktabs} 
\usepackage{multirow, array}
\usepackage{makecell}
\usepackage[table,x11names]{xcolor}
\usepackage{color}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{tabularx}

\begin{document}

%
% --- Author Metadata here ---
% -- Can be completely blank or contain 'commented' information like this...
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA} % If you happen to know the conference location etc.
%\CopyrightYear{2001} % Allows a non-default  copyright year  to be 'entered' - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows non-default copyright data to be 'entered' - IF NEED BE.
% --- End of author Metadata ---

\title{6.864, Fall 2017 - Neural Approaches to Question Retrieval}
\subtitle{Codebase: https://github.com/shashank-srikant/6.864\_term\_project}

% You need the command \numberofauthors to handle the "boxing"
% and alignment of the authors under the title, and to add
% a section for authors number 4 through n.
%
% Up to the first three authors are aligned under the title;
% use the \alignauthor commands below to handle those names
% and affiliations. Add names, affiliations, addresses for
% additional authors as the argument to \additionalauthors;
% these will be set for you without further effort on your
% part as the last section in the body of your article BEFORE
% References or any Appendices.

\numberofauthors{2}
%
% You can go ahead and credit authors number 4+ here;
% their names will appear in a section called
% "Additional Authors" just before the Appendices
% (if there are any) or Bibliography (if there
% aren't)

% Put no more than the first THREE authors in the \author command
%%You are free to format the authors in alternate ways if you have more 
%%than three authors.

\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Vadim Smolyakov*\\
       \affaddr{CSAIL, MIT}\\
       \email{vss@csail.mit.edu}
\alignauthor Shashank Srikant\titlenote{Author order decided by tossing a fair coin.}\\
       \affaddr{CSAIL, MIT}\\
       \email{shash@mit.edu}
}

\balancecolumns
\date{30 July 1999}
\maketitle
\begin{abstract}
Content on the internet grows at an exponential rate. Given this growth, finding relevant information accurately becomes a critical task for the NLP community to address. More so, with this rapid growth, curating labeled datasets to build models for the wide variety of content available on the internet has become extremely time and resource intensive. In this work, we explore whether neural models are able to successfully model content similarity and retrieval tasks, and whether they can transfer knowledge from one domain, where supervised labels are available, to a domain with no available labels. Specifically, we explore content similarity in online discussion forums, where we explore the following questions - a. how effectively can neural approaches model question-answer retrieval tasks, which is, given a question and answer pair present on an online discussion forum, how effectively can neural approaches find similar pairs on that forum. b. Given a model of question-answer similarity in one online discussion forum, how effective are neural approaches in transferring that knowledge to a new, loosely related domain. In this work, we explore a baseline approach of modeling question similarity tasks on the popular Stack Exchange online community \textit{AskUbuntu}. We show how neural architectures like LSTMs and CNNs outperform traditional approaches in information retrieval. Additionally, and importantly, we explore the problem of transferring these models to detect question-answer similarity on \textit{Android Stack Exchange},  a similar yet different online discussion community where members troubleshoot Android related problems. We show how neural domain adaptation techniques successfully beat baseline IR techniques and direct neural transfer techniques. We also discuss some limitations and challenges in using such architectures.
\end{abstract}

\begin{table*}[th]
\begin{tabular*}{\linewidth}{m{0.13\linewidth}m{0.12\linewidth}m{0.18\linewidth}m{0.18\linewidth}m{0.28\linewidth}}
\toprule
\toprule
\textbf{Scenario} & \textbf{Setting} & \textbf{Nature of Data} & \textbf{Learning Paradigm} & \textbf{Main Concepts}  \\
\toprule 
 \makecell{$\mathcal{D}_S = \mathcal{D}_T,$\\$ \mathcal{T}_S = \mathcal{T}_T$}
& Traditional Machine learning
& Labelled data in source domain(s) and unlabeled data in target domain
& Source and target domains are exactly the same
& Learn models on training set and test on future unseen data \\
\cmidrule{1-5}
 \multirow{2}{\linewidth}{\textbf{\makecell{$\mathcal{D}_S \neq \mathcal{D}_T$,\\ $\mathcal{T}_S = \mathcal{T}_T$}}} 
& \multirow{2}{\linewidth}{Transductive Transfer Learning}
& \multirow{2}{\linewidth}{Labelled data in source domain(s) and unlabeled data from $\mathcal{P}(X_S) \neq \mathcal{P}(X_T$)}
& \textbf{Single source domain adaptation}
%\cellcolor{blue!25}
& Learning common shared representation; instance weighing, parameter transfer
\\
\cmidrule{4-5}
 
&
&
& Multi-source adaptation
& Classifier combination; efficient combination of information from multiple sources; Feature representation \\
\cmidrule{1-5} 

\multirow{2}{\linewidth}{No conditions on $\mathcal{D}_S$, $\mathcal{D}_T$, 
but $\mathcal{T}_S \neq \mathcal{T}_T$ }
& \multirow{2}{\linewidth}{Inductive Transfer Learning}
& Unlabeled data in source domain(s) and labeled data in target domain
& Self-taught learning
& Extracts higher level representations from unlabeled auxiliary data to learn instance-to-label mapping with labeled target instances
\\
\cmidrule{3-5} 
& & Labeled data is available in all domains
& Multi-task learning
& Simultaneously learns multiple tasks within (or across) domain(s) by exploiting the common feature subspace shared across the tasks \\
\cmidrule{1-5}
\makecell{$\mathcal{D}_S\neq \mathcal{D}_T$\\$\mathcal{T}_S \neq \mathcal{T}_T$}
& Kim et al., 2015 \cite{kim2015new}
& Labeled data in source and target domains
& Transfer learning with disparate label set
& Disparate fine grained label sets across domains, however, same coarse grained labels set can be invoked across domains
\\
\bottomrule
\end{tabular*}
\caption {A brief summary of different transfer learning approaches. $\mathcal{D}$ is the domain and $\mathcal{T}$ is the task. The boldfaced cell highlights the approach explored in this work. This summary is based on the work detailed in the survey paper by Pan et. al.\protect\cite{pan2010survey}}
\label{tab:related_work}
\end{table*}

\section{Introduction}
The problem of text similarity, and specifically, similarity of short queries or answers found on the internet, have been central to the modern NLP community. With the explosion in content on the internet, a lack of robust tools to find similar content has the risk of creating further similar and redundant content, which only exacerbates the original problem.

Another relevant and pressing concern which such an explosion and variety of content has introduced is the increased cost of building predictive NLP, NLU models which cater to such content. The variety in content requires building a model from scratch, irrespective of how closely related the content may have been to a previously built model. For instance, if we were to model the reviews written for movies, we would have to reinvest effort and time in modeling reviews written for another domain, say, food or hotels. In spite there being conceptual, semantic similarities between the tasks of reviewing movies and reviewing food, models have to be created anew. And each such modeling exercise demands a large repository of curated, preferably labeled data, which most often is not feasible to put together. The pressing challenge such a variety of content has created is to be able to learn with minimal supervision, and from loosely related datasets.

In this work, we investigate two problems - one, to model question-similarity tasks using state of the art techniques in neural modeling and two, transfer those models to a loosely related, yet equally rich domain under the constraint of having no supervised information on that new domain. Specifically, we explore the problem of finding similar question-answer pairs on the popular \textit{Ask Ubuntu Stack Exchange}.\footnote{https://askubuntu.com} This discussion forum focuses on troubleshooting queries on Ubuntu, the popular open-source operating system. In addition to  learning models to find relevant question-answer pairs on this forum, we also use this domain to transfer knowledge onto modeling question-answer similarity in \textit{Android Stack Exchange}.\footnote{https://android.stackexchange.com} This is an online community, similar in setup and structure to \textit{Ask Ubuntu}, wherein short queries and corresponding answers on Android-related troubleshooting are present.

In this work, we investigate how state of the art neural architectures for NLP applications perform on the task of question similarity. Traditionally, text similarity tasks were solved using information retrieval techniques like building an indexer and applying string search over the indexed corpus. A lot of variations to this general strategy have been explored over the years. We study how more recent techniques like RNNs and CNNs, which have shown to successfully model a variety of NLP tasks, solve the question retrieval problem. For the domain adaptation task, we explore adversarial training techniques \cite{ganin2015unsupervised, lei2015semi} as well as the performance on different architectures like $\textsf{Doc2Vec}$ \cite{doc2vec1} and Siamese neural networks \cite{siamese}.

This study demonstrates the relevance of neural techniques in a critical NLP task like question-answer retrieval. We demonstrate how using neural approaches, one can outperform traditional information retrieval techniques while not having to invest heavily in engineering the right features to get to such performance. Additionally, we show the successful transfer of domain knowledge from one domain to another using state of the art neural transfer techniques.

This work is organized into the following sections - Section \ref{related} discusses related work in this field. Sections \ref{in_domain}, \ref{domain_adapt},  \ref{other_techniques} discuss various techniques to model in-domain and domain adaptation tasks for the given problem. Section \ref{experiments} discusses the experiment setup and the results. Section \ref{discussion} concludes our work.


\begin{figure*}[h]
\centering
\begin{tabular}{cc}
    \includegraphics[width=\columnwidth]{img/question_len_hist}
&   \includegraphics[width=\columnwidth]{img/similar}
\\
   \includegraphics[width=\columnwidth]{img/adversary}
&   \includegraphics[width=\columnwidth]{img/question_len_hist}
\end{tabular}
\label{fig:len_hist}
\caption{Summary of the \textit{Ask Ubuntu} dataset}
\end{figure*}

\section{Related work}
\label{related}
Given the growing popularity of community QA forums, question retrieval has emerged as an important area of research. Recent work has also gone beyond word-based methods to represent this task. For instance, Feng et. al. \cite{feng2015applying} learn word embeddings using category-based metadata information for questions. They define each question as a distribution which generates each word (embedding) independently, and subsequently use a Fisher kernel to assess question similarities. Dos Santos et. al. \cite{dos2015learning} propose an approach which combines a convolutional neural network (CNN) and a bag of words representation for comparing questions. In contrast to \cite{feng2015applying}, Lei et. al. \cite{lei2015semi} treat each question as a word sequence as opposed to a bag of words, and apply recurrent CNNs instead of traditional CNNs. We base our work on \cite{lei2015semi}. We apply different off-the-shelf deep learning models to encode a meaningful embedding and define similarity over these embeddings.

Domain adaptation has been an active area of research in NLP. Table \ref{tab:related_work}  summarizes different settings of transfer learning \cite{pan2010survey}. Our work explores techniques in unsupervised domain adaptation, where the source and target domains ($\mathcal{D}$) are different, but share a common task ($T$) to achieve. The summary helps in understanding where our approach lies in this spectrum of techniques.

We study in this work the scenario where the domains vary while the tasks remain the same. This is referred to as transductive transfer learning. This is the most extensively studied settings in the transfer learning literature and can be broadly categorized as single and multi-source adaptation. Single source adaptation\cite{chen2009extracting, ando2005framework, daume2009frustratingly} primarily aims at minimizing the divergence between the source and target domains either at instance or feature levels. The general idea being identifying a suitable low dimensional space where transformed source and target domains data follow similar distributions and hence, a standard supervised learning algorithm can be trained.

Multiple methods perform unsupervised domain adaptation by matching the feature distributions in the source and the target domains. Some approaches perform this by reweighing or selecting samples from the source domain  \cite{borgwardt2006integrating, huang2007correcting, gong2012geodesic}, while others seek an explicit feature space transformation that would map source distribution into the target ones \cite{pan2011domain, gopalan2011domain, baktashmotlagh2013unsupervised}. An important  aspect of the distribution matching approach is the way the (dis)similarity between distributions is measured. Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space \cite{borgwardt2006integrating, huang2007correcting}, whereas \cite{gong2012geodesic, fernando2013unsupervised} map the principal axes associated with each of the distributions. The approach explored in this work is based on \cite{ganin2015unsupervised} and also attempts to match feature space distributions. However, this is accomplished by modifying the feature representation itself rather than by reweighing or geometric transformation. Also, the method used to measure the disparity between distributions based on their separability is implicitly different and is done by a deep discriminatively-trained classifier in our work as.


\section{In-domain Question similarity}
\label{in_domain}
We first explore the task of question similarity by training a domain-specific classifier. The domain we have explored in this work involves the questions and answers asked on \textit{Ask Ubuntu}. The dataset for this task was curated by \cite{lei2015semi}. 

\begin{table}[h]
\begin{tabular}{l|r|r}
     
     & $\mathsf{source} (\mathcal{D_S})$
     & $\mathsf{target} (\mathcal{D_T})$
     \\
     \toprule
     Description
     & \textit{AskUbuntu}
     & \textit{Android SE}
     \\
     \# of sentences
     & 40
     & 50
     \\
     \# of stop words 
     & 50
     & 90
     \\
     mean word length - title
     & 299
     & 300
     \\
     mean word length - body
     & 299
     & 300
\end{tabular}
\caption{Summary statistics of our dataset}
\label{tab:dataset}
\end{table}

\subsection{Dataset}
We describe here the dataset that we base our work on.
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item The \textit{Ask Ubuntu} dataset consists of questions and answers related to problems people face in Ubuntu, the Linux operating system
\item Each query has two components - a title and main body. The average title length is {\color{red} XX} words and the average body length is {\color{red} YY words} long. Figure \ref{fig:len_hist} shows the distribution of word lengths
\item Each query is associated with a few ($<10$) positive (similar) questions and $100$ randomly drawn questions from the corpus which serve as negative (dissimilar) questions.
\item The dataset contained 110K such queries to train on.
\item Each query-question pair is also associated with its $\mathsf{BM25 scores}$ provided by Apache Lucene. This served as a baseline in our experiments.
\end{itemize}

\subsection{Model}
We model the task of in-domain question similarity as follows:
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
  \item We define a word-level deep neural network (DNN). The network takes a sequence of indices corresponding to word embeddings. We average the title and the body of each query, and use the averaged representation to compute similarity.
  \item We use word embeddings pre-trained on source domain and the Wikipedia.
  \item To train the classifier to find similar and dissimilar questions for a given query, we associate each query with two sets of texts - the set of similar questions as marked by users on \textit{Ask Ubuntu}, and 100 randomly sampled questions from the corpus which serve as dissimilar pairs. The user-defined questions are used only during training. Since user-marked similar questions are not comprehensive and serve only as a weak proxy, the system is tested on expert-marked similar questions.
  \item We use cosine similarity to rank similar questions by computing the cosine of the angle between the query encoding and its associated candidate question.
  \item We use a max-margin framework for learning parameters $\theta$ of our network. Specifically, in a context of a particular training example where $q_i$ is paired with $p^+_i$, we minimize the max-margin loss $L(\theta)$ defined as\\
  \begin{equation}
  \max_{p^- \in Q(q_i)} \{s(q_i,p^-;\theta) - s(q_i,p^+_i;\theta) + \delta(p, p^+_i)\}
  \end{equation}
where $\delta(.,.)$ denotes a non-negative margin. We set $\delta(p, p^+_i)$ to be a small constant when $p \neq p^+_i$ and 0 otherwise. The parameters $\theta$ can be optimized through sub-gradients ${\partial L}/{\partial \theta}$ aggregated over small batches of the training instances \cite{lei2015semi}.
  \item The neural network essentially serves as an encoder to generate a representation in a low-dimension space.
  \item We compare performance on two neural network architectures: Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) and a Convolutional Neural Network (CNN).
  \item Similarity is evaluated on standard information retrieval metrics like \textit{Precision@1}, \textit{Precision@5}, \textit{Mean Average Precision}, and \textit{Mean Reciprocal Rank}. The inputs to each of these metrics is a ranked list of scores. We pass in the cosine similarity scores obtained between the query and each of the positive and negative questions associated with that query, as our ranked list. The $\mathsf{BM25 scores}$ for each of these pairs is used as a baseline similarity score and their evaluations on the metrics mentioned above are considered the baseline performance indicators for this approach.
\end{itemize}

\subsection{CNN}
We describe the CNN architecture implemented in this work. We detail the inputs to the different layers of the network, which should help illustrate the architecture.
We define a CNN with the following layers -
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item \textbf{Input vector $\mathbf{X}$} of dimensions: ($\mathbf{B}$, $\mathbf{W}$) ($\mathbf{B}$: batch size, $\mathbf{W}$: max \# of words in sentence in batch)
\item \textbf{Embedding layer} Pass $\mathbf{X}$ through embeddings layer. Resulting dimension: ($\mathbf{B}$, $\mathbf{W}$, $\mathbf{D}$) ($\mathbf{D}$: \# of embeddings per word)
\item \textbf{Convolution, ReLU} Pass the above input through a convolution layer and then through ReLU. Resulting dimension: ($\mathbf{B}$, $\mathbf{K}$, $\mathbf{Y}$)($\mathbf{K}$: Depth of filter, i.e. \# of filters of given height, $\mathbf{Y}$: \# of values sliding the filter over the input)
\item \textbf{Pooling} Pass through mean pooling. Resulting dimension: ($\mathbf{B}$, $\mathbf{K}$)
\item The resulting encoded embedding of dimension ($\mathbf{B}$, $\mathbf{K}$) for $\mathbf{B}$ words in a given batch.
\end{itemize}

\subsection{LSTM}
{\color{red} TODO: describe the architecture, add a figure for both CNN and LSTM}
We describe the CNN architecture implemented in this work. We detail the inputs to the different layers of the network, which should help illustrate the architecture.
We define a CNN with the following layers -
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item \textbf{Input vector $\mathbf{X}$} Dimensions: ($\mathbf{B}$, $\mathbf{W}$) ($\mathbf{B}$: batch size, $\mathbf{W}$: max \# of words in sentence in batch)
\item \textbf{Embedding layer} Pass $\mathbf{X}$ through embeddings layer. Resulting dimension: ($\mathbf{B}$, $\mathbf{W}$, $\mathbf{D}$) ($\mathbf{D}$: \# of embeddings per word)
\item \textbf{Convolution, ReLU} Pass the above input through a convolution layer and then through ReLU. Resulting dimension: ($\mathbf{B}$, $\mathbf{K}$, $\mathbf{Y}$)($\mathbf{K}$: Depth of filter, i.e. \# of filters of given height, $\mathbf{Y}$: \# of values sliding the filter over the input)
\item \textbf{Pooling} Pass through mean pooling. Resulting dimension: ($\mathbf{B}$, $\mathbf{K}$)
\item The resulting encoded embedding of dimension ($\mathbf{B}$, $\mathbf{K}$) for $\mathbf{B}$ words in a given batch.
\end{itemize}

\section{Domain Adaptation (DA)}
\label{domain_adapt}
 We use adversarial training techniques \cite{ganin2015unsupervised} to adapt to a domain having no labeled data, having trained a model on a domain with labeled data. In this section and the next, we describe the dataset we worked on, the architecture for the adversarial model we use, and other models for DA we tried.


\subsection{Dataset}
We worked with two datasets, one as our $\mathsf{source}$ domain, wherein we had labeled data and another as our $\mathsf{target}$ domain, where we had no labeled data. The task on both these datasets was the same - to predict similarity between questions. The $\mathsf{source}$ domain in our experiments was the \textit{AskUbuntu} dataset, described in the previous section. The $\mathsf{target}$ domain was the \textit{Android Stack Exchange}, a question-answering site on Android-related issues. Table \ref{tab:dataset} describes the summary statistics of this dataset. For the $\mathsf{target}$ domain, we had {\color{red} XX} sentences to test. Each query was expert-annotated with similar and dissimilar queries they corresponded to. A TF-IDF representation was used as a baseline for evaluating domain adaptation.
\subsection{Model}
We model this task of unsupervised domain adaptation using a neural adversarial approach \cite{ganin2015unsupervised}. The core idea is as follows - 
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item We define an encoder which takes in data from the $\mathsf{source}$ domain and learns a representation which predicts the similarity for tasks in its own domain, given that we have labels for this domain. This is similar to the in-domain learning described in the previous section. We refer to this network as the \textit{encoder network} here on.
    \item While the primary goal of predicting $\mathcal{Y}_\mathsf{source}$ remains, we define an adversarial objective to guide the direction of the embeddings encoded by the encoder network in a way to make them abstract enough to be able to discriminate $\mathcal{Y}_\mathsf{target}$ as well.
    \item We do this by setting up another network to learn this adversarial objective. We refer to this network as \textit{adversarial network} here on. We set the goal of the adversarial network to predict whether a given data-point input to it originates from either the $\mathsf{source}$ or $\mathsf{target}$ dataset. The idea behind setting up this objective is that if the origin of the input data is being discriminated well by the adversarial network, then the features are still specific enough to discriminate between the two datasets. What we want is to however have an abstract enough feature space which, in spite of predicting the labels of the $\mathsf{source}$ well, is unable to distinguish between the $\mathsf{source}$ and $\mathsf{target}$ dataset.
    \item The objective of the adversarial network is set up in a way to propogate a negative weight to the encoder whenever it successfully discriminates between $\mathsf{source}$ and $\mathsf{target}$. This ensures the successful transfer of the encoded embeddings to the $\mathsf{target}$ domain. Figure \ref{fig:adversary} depicts this arrangement.
\end{itemize}


\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{img/adversary.png}
\label{fig:adversary}
\caption{Adversarial network for unsupervised domain adaptation investigated in this work. Image reference: Ganin et al., 2015 \protect\cite{ganin2015unsupervised}}
\end{figure}
 

\section{Exploring Domain Adaptation}
\label{other_techniques}
We explored two other techniques to carry out unsupervised domain transfer - $\textsf{Doc2Vec}$ \cite{doc2vec1} and Siamese recurrent networks \cite{siamese}. We describe each briefly here:

\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item $\mathbf{\mathsf{Doc2Vec}}$ Paragraph vectors, or $\mathsf{doc2vec}$, were proposed by Le and Mikolov (2014) as a simple extension to word2vec to extend the learning of embeddings from words to word sequences. $\mathsf{doc2vec}$ is agnostic to the  granularity of the word sequence â€” it can equally be a word n-gram, sentence, paragraph or document. In this paper, we use the term ``document embedding'' to refer to the embedding of title and body.

\item \textbf{Siamese networks} 
{\color{red} TODO}

\end{itemize}

%\begin{table*}[h]
%\parbox{.3\linewidth}{
%}
%\caption{Parameters of our encoder networks for in-domain classification}
%\label{tab:indomain}
%\parbox{.3\linewidth}{
    
%}
%\parbox{.3\linewidth}{
%\begin{table}
   
%}
%\end{table*}


\section{Experiments and Results}
\label{experiments}
We pose the following research questions, whose answers we investigate in this work
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item \textbf{E1:} How well do deep neural encoders solve the task of in-domain question retrieval. How does it compare to baselines and the state of the art? What are the limitations.
\item \textbf{E2:} How effective are deep adversarial neural methods in adapting from one domain to another to learn a common task. How does it compare to the baselines and state of the art? What are the limitations?
\item \textbf{E3:} How effective is $\mathsf{doc2vec}$ in adapting knowledge from one domain to another.
\item \textbf{E4:} How effective are Siamese networks in adapting knowledge from one domain to another.
\end{itemize}

\subsection{Experiment 1}
To answer $\mathbf{E1}$, we measure the performance of our neural encoders against a baseline of $\mathsf{BM25 scores}$ provided for each query in the test set of the corpus. These scores are pre-computed for each query-positive and query-negative pair. The ranked list of these scores per query is used as a baseline performance on the four metrics we choose (see Section \ref{in_domain} for details on the metrics).

\textbf{Implementation specifics:}
In our implementation, we ensure that stop words are removed from the corpus. This reduces the average sentence length by 30\%. Each input in a batch that we pass to our network consists of words in a query, one positive question corresponding to it and all its corresponding negative questions. We pad unseen words in the pre-trained embeddings with 0s. We also equate sentence length in a batch by padding with 0s. The 0-padding is taken into account while mean pooling the intermediate results in the network. We do not train the pre-trained embedding layer. In this work, to limit computational requirements, we consider only the first 10 words in the title and the first 100 words in the body of each sentence. Further, we score only 40 random negative questions per query instead of the full 100 provided in the dataset. All hyperparameters are tuned on the dev set. The number of trainable network parameters are constrained to a budget of 450K parameters so that the expressability of the models are comparable. Table {\color{red} XX} describes the various hyperparameters of the encoder and adversarial network. The model is trained on {\color{red} XX samples and tested on YY samples. All experiments were carried out on a XXX machine. Each model was run for {\color{red} XX epochs} to train. Each epoch took XX minutes, and we were able to train the network on the full training-set in under 180 minutes.}

\textbf{Results:}
Table \ref{tab:exp1} tabulates the performance of neural encoders on the task of question retrieval and similiarity. CNN and LSTM denote the performance of our two neural encoders while $\textsf{BM25}$ denotes the baseline scores gathered from Apache Lucene. Against each of our models, we also record the number of trainable parameters used. The performance is on the scale $0$-$1$, where higher the number, better the retrieval. We analyze and discuss these results in Section \ref{discussion}.


\begin{table}[h]
\begin{tabular}{l|r|l|r}
      \multicolumn{2}{c|}{\textbf{CNN}}
     & \multicolumn{2}{c}{\textbf{LSTM}}
     \\
     \toprule
     \textbf{Parameter}
     & \textbf{Value}
     & \textbf{Parameter}
     & \textbf{Value}
     \\
     \midrule
     output channels
     & 250
     & hidden size
     & 128 $\times$ 2
     \\
     loss margin
     & 0.4
     & loss margin
     & 0.4
     \\
     kernel size
     & [1-5] $\times$ 300
     & bidirectional
     & true
     \\
     pooling
     & average
     & pooling
     & average
\end{tabular}
\end{table}

\subsection{Experiment 2} 
To answer $\mathbf{E2}$, we measure the performance of domain adaptation on the \textit{Android Stack Exchange} dataset, having learnt a model on the \textit{Ask Ubuntu} dataset.

\textbf{Evaluation:} The evaluation metric for this task is changed to instead evaluate a binary classification task - is a query similar to a given candidate question present in the corpus or not. As a consequence, the robustness of the system is measured using a $AUC(0.05)$ score. This designers of this problem statement chose this different metric compared to those used in $\mathbf{E1}$. This makes comparing results between the two experiments hard.

\textbf{Baselines:} We establish multiple baselines - some stronger than the other. To begin with, we train a TF-IDF model on the $\textsf{source}$ domain and evaluate on the $\textsf{target}$ domain. As a slightly stronger baseline, we directly apply the model learnt in $\mathbf{E1}$ onto the $\textsf{target}$ domain.

\textbf{Implementation specifics:} We train the encoder network from scratch to train the two objectives detailed in Section \ref{domain_adapt}. We maintain the constraint of evaluating on 40 negative samples for the $\mathsf{source}$ label classifier but do not vary the validation/test set for the $\mathsf{target}$ domain. We maintain the same constraints on the length of the title and body as in $\mathbf{E1}$. Stop-words and word-casing are handled for the $\mathsf{target}$ domain. We evaluate the system on both, pre-trained embeddings used in $\mathbf{E1}$ and off-the-shelf $\mathsf{GloVe}$ embeddings \cite{pennington2014glove}. All hyperparameters are tuned on the dev set. The model is trained on {\color{red} XX samples and tested on YY samples. All experiments were carried out on a XXX machine. Each model was run for {\color{red} XX epochs} to train. Each epoch took XX minutes, and we were able to train the network on the full training-set in under 180 minutes.}

\textbf{Results:} Table \ref{tab:exp1} tabulates the performance of neural encoders on the task of domain adaptation for question retrieval. Models with the suffix $\mathsf{xx-direct}$ in column 1, Table \ref{tab:exp1} correspond to the direct-transfer baseline models. The adversarial models are described with the suffix $\mathsf{xx-adv}$. The evaluation metric is AUC(0.05), which is reported on a scale of $0$-$1$. An AUC(0.05) score of $1$ implies that {\color{red}the model has a false positive rate of $<$0.05\%}. Results are reported for both, the dev and test sets. We analyze and discuss these results in Section \ref{discussion}.

\begin{table}[h]
    \begin{tabular}{l|r|l|r}
          \multicolumn{2}{c|}{\textbf{CNN}}
         & \multicolumn{2}{c}{\textbf{LSTM}}
         \\
         \toprule
         \textbf{Parameter}
         & \textbf{Value}
         & \textbf{Parameter}
         & \textbf{Value}
         \\
         \midrule
         \# of sentences
         & 40
         & \# of sentences
         & 50
         \\
         \# of stop words 
         & 50
         & \# of sentences
         & 90
         \\
         mean word length - title
         & 299
         & \# of sentences
         & 300
         \\
         mean word length - body
         & 299
         & \# of sentences
         & 300
         \\
         Batch size
         & 299
         & \# of sentences
         & 300
         \\
         Number of epochs
         & 299
         & \# of sentences
         & 300
         \\
         mean word length - body
         & 299
         & \# of sentences
         & 300
    \end{tabular}
    \caption{Parameters of the encoder and adversarial\protect\\ networks for domain adaptation}
    \label{tab:param_indomain}
\end{table}



\subsection{Experiment 3}
{\color{red} TODO}
To answer $\mathbf{E1}$, we measure the performance of our neural encoders against a baseline of $\mathsf{BM25 scores}$ provided for each query in the test set of the corpus. These scores are pre-computed for each query-positive and query-negative pair. The ranked list of these scores per query is used as a baseline performance on the four metrics we choose (see Section \ref{in_domain} for details on the metrics).

\textbf{Implementation specifics:}
In our implementation, we ensure that stop words are removed from the corpus. This reduces the average sentence length by 30\%. Each input in a batch that we pass to our network consists of words in a query, one positive question corresponding to it and all its corresponding negative questions. We pad unseen words in the pre-trained embeddings with 0s. We also equate sentence length in a batch by padding with 0s. The 0-padding is taken into account while mean pooling the intermediate results in the network. We do not train the pre-trained embedding layer. In this work, to limit computational requirements, we consider only the first 10 words in the title and the first 100 words in the body of each sentence. Further, we score only 40 random negative questions per query instead of the full 100 provided in the dataset. All hyperparameters are tuned on the dev set. The number of trainable network parameters are constrained to a budget of 450K parameters so that the expressability of the models are comparable. Table {\color{red} XX} describes the various hyperparameters of the encoder and adversarial network. The model is trained on {\color{red} XX samples and tested on YY samples. All experiments were carried out on a XXX machine. Each model was run for {\color{red} XX epochs} to train. Each epoch took XX minutes, and we were able to train the network on the full training-set in under 180 minutes.}
\begin{table}[h]
 \begin{tabular}{l|r|l|r}
          \multicolumn{2}{c|}{\textbf{Siamese}}
         & \multicolumn{2}{c}{\textbf{\textsf{doc2vec}}}
         \\
         \toprule
         \textbf{Parameter}
         & \textbf{Value}
         & \textbf{Parameter}
         & \textbf{Value}
         \\
         \midrule
         \# of sentences
         & 40
         & \# of sentences
         & 50
         \\
         \# of stop words 
         & 50
         & \# of sentences
         & 90
         \\
         mean word length - title
         & 299
         & \# of sentences
         & 300
         \\
         mean word length - body
         & 299
         & \# of sentences
         & 300
    \end{tabular}
    \caption{Parameters of $\mathsf{doc2vec}$ and the Siamese network architectures}
    \label{tab:indomain}
\end{table}

\subsection{Experiment 4}
{\color{red} TODO}

\begin{table*}[h]
    \centering
    \begin{tabular*}{0.8\textwidth}{c | m{0.12\linewidth} | c c c c | c c c r}
         \toprule
         \toprule
         
         \multirow{2}{*}{\textbf{Model name}}
         & \multirow{2}{*}{\textbf{\shortstack{\# of param \\ in the n/w}}}
         & \multicolumn{4}{c|}{\textbf{Test}}
         & \multicolumn{4}{c}{\textbf{Dev}}
         \\
         
         &  
         & \textbf{MRR}
         & \textbf{P@1}
         & \textbf{P@5}
         & \textbf{MAP}
         
         & \textbf{MRR}
         & \textbf{P@1}
         & \textbf{P@5}
         & \textbf{MAP}
         \\
         \midrule
         \textbf{BM25 baseline}
         & NA
         & 0.68
         & 0.54
         & 0.42
         & 0.56
         & 0.66
         & 0.52
         & 0.42
         & 0.52
         \\
         \midrule
         \textbf{CNN}
         & 337290
         & 0.63
         & 0.48
         & 0.40
         & 0.43
         & 0.63
         & 0.49
         & 0.41
         & 0.51
         \\
         \midrule
         \textbf{LSTM}
         & 337290
         & 0.69
         & 0.54
         & 0.42
         & 0.56
         & 0.65
         & 0.48
         & 0.44
         & 0.54
         
    \end{tabular*}
    \caption{Results for Experiment 1 - In-domain learning using neural models}
    \label{tab:exp1}
 
 \vspace{7pt}
  
    \begin{tabular}{r | c | r | r}
    \toprule
    \toprule
    \textbf{Model name}
    & \textbf{Description}
    & \textbf{AUC(0.05) - Test}
    & \textbf{AUC(0.05) - Dev}
    \\
    \toprule
    \textbf{TF-IDF}
    & Baseline
    & 0.58
    & 0.58
    \\
    \cmidrule{1-4} 
    \textbf{CNN-direct}
    & \multirow{2}{*}{Direct transfer}
    & 0.62
    & 0.62
    \\
    \cmidrule{1-1} 
    \cmidrule{3-4} 
    \textbf{LSTM-direct}
    & 
    & 0.62
    & 0.63
    \\
    \midrule
    \textbf{CNN-adv}
    & \multirow{2}{*}{Adversarial domain transfer}
    & 0.70
    & 0.70
    \\
    \cmidrule{1-1} 
    \cmidrule{3-4}
    \textbf{LSTM-adv}
    &
    & 0.70
    & 0.70
    \\
    \midrule
    \textbf{Siamese networks}
    & \multirow{2}{*}{Other domain transfer methods}
    & 0.70
    & 0.70
    \\
    \cmidrule{1-1} 
    \cmidrule{3-4}
    \textbf{\textsf{Doc2Vec}}
    &
    & 0.70
    & 0.70
    \\
    \bottomrule
    \end{tabular}
    \caption{Results from Experiment 2 - Domain adaptation}
\end{table*}


\begin{figure*}[h]
\begin{tabular}{cccc}
    \includegraphics[width=0.5\columnwidth]{img/cnn_training_loss_toy}
&   \includegraphics[width=0.5\columnwidth]{img/cnn_domain_transfer_adversarial_gradient_norm}
&   \includegraphics[width=0.5\columnwidth]{img/domain_transfer_direct_lstm_hist}
&   \includegraphics[width=0.5\columnwidth]{img/cnn_domain_transfer_adversarial_lstm}
\end{tabular}
\label{fig:len_hist}
\caption{Results from Experiment 1}
\end{figure*}

\section{Discussion}
\label{discussion}
\subsection{Experiment 1}
{\color{red}TODO: add adversarial training objectives, discuss stability of changing lambda, discuss differences with siamese networks etc.}
We observe that both, the LSTM and the CNN networks, beat the \textsf{BM25} baseline by {\color{red}XX points} on an average on each of the metrics. We notice that this accuracy is achieved with a very simple model architecture, without any effort invested in engineering features. Such a performance by off-the-shelf models suggests that neural models are an apt choice for modeling such tasks.  Figures \ref{fig:len_hist, fig:len_hist} describe the loss function over different epochs for the CNN and LSTM respectively. We note here though that we found the results to be sensitive to different hyperparameter choices. With the right choice of hyperparameters, we could gain 10 points on an average on all the metrics. For instance, we found \textsf{tanh} to produce better results over using \textsf{ReLU} as an activation function for the CNNs. We found the learning rates of \textit{Adam}, the optimizer, when outside the range of {\color{red}XX, to have an adverse effect on the results}. We found that mean pooling and max pooling produced comparable results on both networks. We found the choice of the margin in the maximum-margin separator to specially have an effect on the performance. Moving it by 0.5 points above or below the margin set for the final models (see Table \ref{tab:hyperparam_exp1}) decreased the final accuracy by 0.10 units.

\subsection{Experiment 2}
The baseline is measured against an unsupervised (TF-IDF) model being learnt on the \textsf{source} domain. In comparison, we see the direct transfer model to perform slightly better by both, the CNN and LSTM. This is expected since these models learn , but relatively the 
%ACKNOWLEDGEMENTS are optional
\section{Acknowledgements}
\label{ack}
The authors thank the course staff of 6.864, Fall 2017, MIT. This was a fun project!

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
% SIGKDD: balancing columns messes up the footers: Sunita Sarawagi, Jan 2000.
% \balancecolumns

% That's all folks!
\end{document}
